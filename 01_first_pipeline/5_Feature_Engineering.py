# ç‰¹å¾å·¥ç¨‹å®æˆ˜ä»£ç 
from scipy.sparse import hstack, csr_matrix
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import jieba
import re

# è®¾ç½®å­—ä½“ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
plt.rcParams['axes.unicode_minus'] = False

# ä½¿ç”¨æ™ºèƒ½å®¢æœæ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹æ¼”ç¤º
sample_texts = [
    "æ€ä¹ˆé€€æ¬¾ï¼Ÿæ€¥æ€¥æ€¥",
    "æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™å‘è´§å‘¢",
    "æœ‰ä»€ä¹ˆä¼˜æƒ æ´»åŠ¨å—ï¼Ÿæƒ³äº†è§£ä¸€ä¸‹",
    "äº§å“è´¨é‡æœ‰é—®é¢˜ï¼Œè¦æ±‚é€€è´§ï¼",
    "å®¢æœç”µè¯å¤šå°‘ï¼Ÿè”ç³»ä¸ä¸Š",
    "èƒ½ä¸èƒ½æ¢è´§ï¼Ÿä¸æ»¡æ„è¿™ä¸ªé¢œè‰²",
    "ä¸ºä»€ä¹ˆè¿˜æ²¡æ”¶åˆ°è´§ï¼Ÿå·²ç»ä¸€å‘¨äº†",
    "è¿™ä¸ªäº§å“æ€ä¹ˆä½¿ç”¨ï¼Ÿè¯´æ˜ä¹¦çœ‹ä¸æ‡‚",
    "æˆ‘è¦æŠ•è¯‰ï¼æœåŠ¡æ€åº¦å¤ªå·®äº†",
    "æœ‰æ–°å“æ¨èå—ï¼Ÿæƒ³ä¹°ç‚¹ä¸œè¥¿"
]

intents = [
    "é€€æ¬¾å’¨è¯¢", "ç‰©æµæŸ¥è¯¢", "ä¼˜æƒ å’¨è¯¢", "å”®åæŠ•è¯‰", "è”ç³»æ–¹å¼",
    "æ¢è´§å’¨è¯¢", "ç‰©æµæŸ¥è¯¢", "ä½¿ç”¨å’¨è¯¢", "å”®åæŠ•è¯‰", "äº§å“å’¨è¯¢"
]

df = pd.DataFrame({
    'text': sample_texts,
    'intent': intents
})

print("ğŸ“Š === ç‰¹å¾å·¥ç¨‹æ•°æ®å‡†å¤‡ ===")
print(f"æ•°æ®é‡: {len(df)} æ¡")
print("æ ·æœ¬æ•°æ®:")
for idx, (i, row) in enumerate(df.iterrows()):
    print(f"    {idx+1}. '{row['text']}' -> {row['intent']}")

print("\nâš™ï¸ === ç¬¬1å±‚ï¼šåŸºç¡€ç»Ÿè®¡ç‰¹å¾ ===")

def extract_basic_features(input_text):
    """æå–åŸºç¡€ç»Ÿè®¡ç‰¹å¾"""
    features = {'text_length': len(input_text), 'word_count': len(jieba.lcut(input_text)),  # æ–‡æœ¬é•¿åº¦ç‰¹å¾
                'question_marks': input_text.count('ï¼Ÿ') + input_text.count('?'),  # æ ‡ç‚¹ç¬¦å·ç‰¹å¾
                'exclamation_marks': input_text.count('ï¼') + input_text.count('!'),
                'punctuation_count': len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š]', input_text))}

    # ç‰¹æ®Šè¯æ±‡ç‰¹å¾
    urgent_words = ['æ€¥', 'å¿«', 'é©¬ä¸Š', 'ç«‹å³', 'èµ¶ç´§']
    features['urgent_words'] = sum(1 for word in urgent_words if word in input_text)

    negative_words = ['ä¸', 'æ²¡', 'å·®', 'å', 'çƒ‚', 'ç³Ÿ']
    features['negative_words'] = sum(1 for word in negative_words if word in input_text)

    return features

# æå–åŸºç¡€ç‰¹å¾
basic_features_list = []
for text in df['text']:
    basic_features_list.append(extract_basic_features(text))

basic_features_df = pd.DataFrame(basic_features_list)
print("åŸºç¡€ç‰¹å¾ç¤ºä¾‹ï¼š")
print(basic_features_df.head())

print(f"\nåŸºç¡€ç‰¹å¾ç»Ÿè®¡:")
print(basic_features_df.describe())

print("\nâš™ï¸ === ç¬¬2å±‚ï¼šè¯è¢‹æ¨¡å‹ç‰¹å¾ ===")

# ä¸­æ–‡åˆ†è¯é¢„å¤„ç†
def preprocess_chinese(chinese_text):
    """ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†"""
    # åˆ†è¯
    words = jieba.lcut(chinese_text)
    # å»é™¤åœç”¨è¯å’Œæ ‡ç‚¹
    stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´',
                  'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è¿™'}
    words = [w for w in words if w not in stop_words and len(w.strip()) > 1 and not re.match(r'\W', w)]
    return ' '.join(words)

# å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†
processed_texts = [preprocess_chinese(text) for text in df['text']]
print("é¢„å¤„ç†åçš„æ–‡æœ¬ï¼š")
for i, (orig, proc) in enumerate(zip(df['text'], processed_texts)):
    print(f"{i + 1}. åŸæ–‡ï¼š{orig}")
    print(f"    å¤„ç†ï¼š{proc}")
    print()

# è¯è¢‹æ¨¡å‹ç‰¹å¾æå–
print("æ„å»ºè¯è¢‹æ¨¡å‹")
count_vectorizer = CountVectorizer(
    max_features=100,               # æœ€å¤šä¿ç•™100ä¸ªç‰¹å¾
    ngram_range=(1, 2)              # 1-gramå’Œ2-gram
)

bow_features = count_vectorizer.fit_transform(processed_texts)
feature_names = count_vectorizer.get_feature_names_out()

print(f"è¯è¢‹æ¨¡å‹ç‰¹å¾ç»´åº¦: {bow_features.shape}")
print(f"ç‰¹å¾è¯æ±‡ç¤ºä¾‹: {list(feature_names[:])}")

# å±•ç¤ºéƒ¨åˆ†ç‰¹å¾çŸ©é˜µ
bow_df = pd.DataFrame(bow_features.toarray()[:5, :10], columns=feature_names[:10])
print("è¯è¢‹ç‰¹å¾çŸ©é˜µç¤ºä¾‹:")
print(bow_df)

print("\nâš™ï¸ === ç¬¬3å±‚ï¼šTF-IDFç‰¹å¾ ===")

# TF-IDFç‰¹å¾æå–
tfidf_vectorizer = TfidfVectorizer(
    max_features=100,
    ngram_range=(1, 2),
    sublinear_tf=True       # ä½¿ç”¨æ¬¡çº¿æ€§ç¼©æ”¾
)

tfidf_features = tfidf_vectorizer.fit_transform(processed_texts)
tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()

print(f"TF-IDFç‰¹å¾ç»´åº¦ï¼š{tfidf_features.shape}")

# å±•ç¤ºTF-IDFç‰¹å¾é‡è¦æ€§
feature_importance = np.array(tfidf_features.sum(axis=0)).flatten()
importance_df = pd.DataFrame({
    'feature': tfidf_feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("TF-IDFç‰¹å¾é‡è¦æ€§Top10ï¼š")
print(importance_df.head(10))

print("\nâš™ï¸ === ç¬¬4å±‚ï¼šç»„åˆç‰¹å¾ ===")

# å°†ä¸åŒç±»å‹çš„ç‰¹å¾ç»„åˆ
# å°†åŸºç¡€ç‰¹å¾è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µæ ¼å¼
basic_features_sparse = csr_matrix(basic_features_df.values)

# ç»„åˆæ‰€æœ‰ç‰¹å¾
combined_features = hstack([
    basic_features_sparse,          # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    tfidf_features                  # TF-IDFç‰¹å¾
])

print(f"ç»„åˆç‰¹å¾ç»´åº¦: {combined_features.shape}")
print(f"  - åŸºç¡€ç‰¹å¾: {basic_features_sparse.shape[1]} ç»´")
print(f"  - TF-IDFç‰¹å¾: {tfidf_features.shape[1]} ç»´")

print("\nğŸ“Š === ç‰¹å¾å·¥ç¨‹æ•ˆæœå¯è§†åŒ– ===")

# å¯è§†åŒ–ä¸åŒç‰¹å¾çš„åˆ†å¸ƒ
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
axes[0, 0].hist(basic_features_df['text_length'], bins=5, alpha=0.7, color='skyblue')
axes[0, 0].set_title('æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')
axes[0, 0].set_xlabel('æ–‡æœ¬é•¿åº¦')
axes[0, 0].set_ylabel('é¢‘æ¬¡')

# 2. æ ‡ç‚¹ç¬¦å·ä½¿ç”¨
punct_data = basic_features_df[['question_marks', 'exclamation_marks']].sum()
axes[0, 1].bar(punct_data.index, punct_data.values, color=['orange', 'red'], alpha=0.7)
axes[0, 1].set_title('æ ‡ç‚¹ç¬¦å·ä½¿ç”¨æƒ…å†µ')
axes[0, 1].set_ylabel('æ€»æ•°')

# 3. ç‰¹å¾é‡è¦æ€§
top_features = importance_df.head(8)
axes[1, 0].barh(top_features['feature'], top_features['importance'], color='green', alpha=0.7)
axes[1, 0].set_title('TF-IDFç‰¹å¾é‡è¦æ€§')
axes[1, 0].set_xlabel('é‡è¦æ€§å¾—åˆ†')

# 4. ç‰¹å¾ç±»å‹åˆ†å¸ƒ
feature_types = ['åŸºç¡€ç‰¹å¾', 'TF-IDFç‰¹å¾']
feature_counts = [basic_features_sparse.shape[1], tfidf_features.shape[1]]
axes[1, 1].pie(feature_counts, labels=feature_types, autopct='%1.1f%%',
              colors=['lightblue', 'lightgreen'], startangle=90)
axes[1, 1].set_title('ç‰¹å¾ç±»å‹åˆ†å¸ƒ')

plt.tight_layout()
plt.show()

print("\nâœ… === ç‰¹å¾å·¥ç¨‹æ€»ç»“ ===")
print("ğŸ¯ å®Œæˆçš„ç‰¹å¾ç±»å‹:")
print("  âœ… åŸºç¡€ç»Ÿè®¡ç‰¹å¾: æ–‡æœ¬é•¿åº¦ã€æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šè¯æ±‡")
print("  âœ… è¯è¢‹æ¨¡å‹ç‰¹å¾: è¯é¢‘ç»Ÿè®¡")
print("  âœ… TF-IDFç‰¹å¾: è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡")
print("  âœ… N-gramç‰¹å¾: 1-gram + 2-gram")
print("  âœ… ç»„åˆç‰¹å¾: å¤šç§ç‰¹å¾èåˆ")

print(f"\nğŸ“Š æœ€ç»ˆç‰¹å¾çŸ©é˜µ:")
print(f"  - æ ·æœ¬æ•°: {combined_features.shape[0]} æ¡")
print(f"  - ç‰¹å¾ç»´åº¦: {combined_features.shape[1]} ç»´")
print(f"  - ç‰¹å¾å¯†åº¦: {combined_features.nnz / (combined_features.shape[0] * combined_features.shape[1]):.4f}")

print("\nğŸ’¡ ç‰¹å¾å·¥ç¨‹å»ºè®®:")
print("âœ… åŸºç¡€ç‰¹å¾èƒ½æ•æ‰æ–‡æœ¬çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
print("âœ… TF-IDFç‰¹å¾èƒ½è¯†åˆ«é‡è¦çš„è¯æ±‡ä¿¡æ¯")
print("âœ… N-gramç‰¹å¾èƒ½æ•æ‰è¯æ±‡æ­é…ä¿¡æ¯")
print("âœ… ç»„åˆç‰¹å¾æä¾›äº†æ›´å…¨é¢çš„æ–‡æœ¬è¡¨ç¤º")
print("âœ… ç‰¹å¾å·²å‡†å¤‡å¥½è¿›å…¥æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼")