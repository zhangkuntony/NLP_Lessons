# å»ºæ¨¡å®æˆ˜ä»£ç 
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split
from sklearn.metrics import confusion_matrix
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba
import time

# è®¾ç½®å­—ä½“ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
plt.rcParams['axes.unicode_minus'] = False

# å‡†å¤‡æ›´å¤§çš„æ¨¡æ‹Ÿæ•°æ®é›†ç”¨äºå»ºæ¨¡
np.random.seed(42)

# ç”Ÿæˆæ¨¡æ‹Ÿçš„æ™ºèƒ½å®¢æœæ•°æ®
def generate_sample_data(n_samples=500):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„æ™ºèƒ½å®¢æœæ•°æ®"""
    intents = ['é€€æ¬¾å’¨è¯¢', 'ç‰©æµæŸ¥è¯¢', 'ä¼˜æƒ å’¨è¯¢', 'å”®åæŠ•è¯‰', 'è”ç³»æ–¹å¼']

    # ä¸åŒæ„å›¾çš„æ¨¡æ¿
    templates = {
        'é€€æ¬¾å’¨è¯¢': ['æ€ä¹ˆé€€æ¬¾', 'é€€æ¬¾æµç¨‹', 'ç”³è¯·é€€æ¬¾', 'é’±ä»€ä¹ˆæ—¶å€™é€€å›æ¥', 'é€€æ¬¾è¦å¤šä¹…'],
        'ç‰©æµæŸ¥è¯¢': ['ä»€ä¹ˆæ—¶å€™å‘è´§', 'æŸ¥è¯¢ç‰©æµ', 'å¿«é€’åˆ°å“ªäº†', 'å¤šä¹…èƒ½æ”¶åˆ°è´§', 'ç‰©æµä¿¡æ¯'],
        'ä¼˜æƒ å’¨è¯¢': ['æœ‰ä»€ä¹ˆä¼˜æƒ ', 'æ‰“æŠ˜æ´»åŠ¨', 'ä¼˜æƒ åˆ¸æ€ä¹ˆç”¨', 'ä¿ƒé”€ä¿¡æ¯', 'ä¼šå‘˜ä»·æ ¼'],
        'å”®åæŠ•è¯‰': ['äº§å“æœ‰é—®é¢˜', 'è´¨é‡ä¸å¥½', 'è¦æŠ•è¯‰', 'æœåŠ¡æ€åº¦å·®', 'ä¸æ»¡æ„'],
        'è”ç³»æ–¹å¼': ['å®¢æœç”µè¯', 'è”ç³»æ–¹å¼', 'äººå·¥å®¢æœ', 'åœ¨çº¿å®¢æœ', 'å®¢æœQQ']
    }

    sample_texts = []
    sample_labels = []

    for _ in range(n_samples):
        intent = np.random.choice(intents)
        template = np.random.choice(templates[intent])

        # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
        variations = ['ï¼Ÿ', 'ï¼', 'å‘¢', 'å—', 'å•Š', 'ï¼Œæ€¥æ€¥æ€¥', 'ï¼Œè°¢è°¢']
        text = template + np.random.choice(variations)

        sample_texts.append(text)
        sample_labels.append(intent)

    return sample_texts, sample_labels

# ç”Ÿæˆè®­ç»ƒæ•°æ®
texts, labels = generate_sample_data(500)
df = pd.DataFrame({'text': texts, 'intent': labels})

print("ğŸ¤– === å»ºæ¨¡æ•°æ®å‡†å¤‡ ===")
print(f"æ•°æ®é‡: {len(df)} æ¡")
print("æ„å›¾åˆ†å¸ƒ:")
print(df['intent'].value_counts())

# æ–‡æœ¬é¢„å¤„ç†
def preprocess_text(text):
    """ç®€å•çš„æ–‡æœ¬é¢„å¤„ç†"""
    words = jieba.lcut(text)
    # å»é™¤åœç”¨è¯
    stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´',
                  'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è¿™'}
    words = [w for w in words if w not in stop_words and len(w.strip()) > 1]
    return ' '.join(words)

# é¢„å¤„ç†æ–‡æœ¬
processed_texts = [preprocess_text(text) for text in df['text']]

# ç‰¹å¾æå–
tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
x = tfidf.fit_transform(processed_texts)
y = df['intent']

# æ•°æ®åˆ†å‰²
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nè®­ç»ƒé›†: {x_train.shape[0]} æ¡")
print(f"æµ‹è¯•é›†: {x_test.shape[0]} æ¡")
print(f"ç‰¹å¾ç»´åº¦: {x_train.shape[1]} ç»´")

print("\nğŸ§  === æ¨¡å‹è®­ç»ƒå¯¹æ¯” ===")

# å®šä¹‰å¤šä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”
models = {
    'æœ´ç´ è´å¶æ–¯': MultinomialNB(),
    'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000),
    'éšæœºæ£®æ—': RandomForestClassifier(n_estimators=100, random_state=42),
    'æ”¯æŒå‘é‡æœº': SVC(kernel='linear', random_state=42),
    'Kè¿‘é‚»': KNeighborsClassifier(n_neighbors=5)
}

# å­˜å‚¨ç»“æœ
results = {}
training_times = {}

print("å¼€å§‹è®­ç»ƒå„ç§æ¨¡å‹...")
for name, model in models.items():
    print(f"\nè®­ç»ƒ {name}...")

    # è®­ç»ƒæ—¶é—´æµ‹é‡
    start_time = time.time()

    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, x_train, y_train, cv=5, scoring='accuracy')

    # è®­ç»ƒå®Œæ•´æ¨¡å‹
    model.fit(x_train, y_train)

    training_time = time.time() - start_time

    # é¢„æµ‹
    y_pred = model.predict(x_test)
    test_accuracy = np.mean(y_pred == y_test)

    # å­˜å‚¨ç»“æœ
    results[name] = {
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'test_accuracy': test_accuracy,
        'model': model
    }
    training_times[name] = training_time

    print(f"  äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
    print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")

print("\nğŸ“Š === æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===")

# åˆ›å»ºç»“æœDataFrame
results_df = pd.DataFrame({
    name: {
        'äº¤å‰éªŒè¯å‡†ç¡®ç‡': data['cv_mean'],
        'æµ‹è¯•é›†å‡†ç¡®ç‡': data['test_accuracy'],
        'è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰': training_times[name]
    }
    for name, data in results.items()
}).T

print("å„æ¨¡å‹æ€§èƒ½æ±‡æ€»:")
print(results_df.round(4))

# æ‰¾å‡ºæœ€ä½³æ¨¡å‹
best_model_name = max(results.keys(), key=lambda key: results[key]['test_accuracy'])
best_model = results[best_model_name]['model']

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {results[best_model_name]['test_accuracy']:.4f}")

print("\nğŸ“ˆ === æ€§èƒ½å¯è§†åŒ– ===")

# å¯è§†åŒ–æ¨¡å‹æ€§èƒ½å¯¹æ¯”
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. å‡†ç¡®ç‡å¯¹æ¯”
model_names = list(results.keys())
test_accs = [results[name]['test_accuracy'] for name in model_names]
cv_accs = [results[name]['cv_mean'] for name in model_names]

x = np.arange(len(model_names))
width = 0.35

axes[0, 0].bar(x - width / 2, cv_accs, width, label='äº¤å‰éªŒè¯', alpha=0.8, color='skyblue')
axes[0, 0].bar(x + width / 2, test_accs, width, label='æµ‹è¯•é›†', alpha=0.8, color='lightcoral')
axes[0, 0].set_xlabel('æ¨¡å‹')
axes[0, 0].set_ylabel('å‡†ç¡®ç‡')
axes[0, 0].set_title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(model_names, rotation=45)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. è®­ç»ƒæ—¶é—´å¯¹æ¯”
train_times = [training_times[name] for name in model_names]
axes[0, 1].bar(model_names, train_times, color='lightgreen', alpha=0.8)
axes[0, 1].set_xlabel('æ¨¡å‹')
axes[0, 1].set_ylabel('è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰')
axes[0, 1].set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯”')
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(True, alpha=0.3)

# 3. æœ€ä½³æ¨¡å‹çš„æ··æ·†çŸ©é˜µ
y_pred_best = best_model.predict(x_test)
cm = confusion_matrix(y_test, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=best_model.classes_,
            yticklabels=best_model.classes_,
            ax=axes[1, 0])
axes[1, 0].set_title(f'{best_model_name} æ··æ·†çŸ©é˜µ')
axes[1, 0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
axes[1, 0].set_ylabel('çœŸå®æ ‡ç­¾')

# 4. å‡†ç¡®ç‡ vs é€Ÿåº¦ æ•£ç‚¹å›¾
axes[1, 1].scatter(train_times, test_accs, s=100, alpha=0.7, c='purple')
for i, name in enumerate(model_names):
    axes[1, 1].annotate(name, (train_times[i], test_accs[i]),
                        xytext=(5, 5), textcoords='offset points',)
axes[1, 1].set_xlabel('è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰')
axes[1, 1].set_ylabel('æµ‹è¯•å‡†ç¡®ç‡')
axes[1, 1].set_title('å‡†ç¡®ç‡ vs è®­ç»ƒæ—¶é—´')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nğŸ”§ === è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹ ===")

# å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
if best_model_name == 'éšæœºæ£®æ—':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, None],
        'min_samples_split': [2, 5, 10],
    }
elif best_model_name == 'é€»è¾‘å›å½’':
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear']
    }
else:
    param_grid = {}

if param_grid:
    print(f"å¯¹ {best_model_name} è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")

    grid_search = GridSearchCV(
        models[best_model_name],
        param_grid,
        cv=3,
        scoring='accuracy',
        n_jobs=1
    )

    grid_search.fit(x_train, y_train)

    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.4f}")

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°è°ƒä¼˜åçš„æ¨¡å‹
    optimized_score = grid_search.score(x_test, y_test)
    print(f"è°ƒä¼˜åæµ‹è¯•å‡†ç¡®ç‡: {optimized_score:.4f}")

    # æ€§èƒ½æå‡
    improvement = optimized_score - results[best_model_name]['test_accuracy']
    print(f"æ€§èƒ½æå‡: {improvement:.4f}")

print("\nâœ… === å»ºæ¨¡æ€»ç»“ ===")
print("ğŸ¯ æ¨¡å‹è®­ç»ƒå®Œæˆæƒ…å†µ:")
print(f"  âœ… æµ‹è¯•äº† {len(models)} ç§ä¸åŒæ¨¡å‹")
print(f"  âœ… æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"  âœ… æœ€é«˜å‡†ç¡®ç‡: {max(test_accs):.4f}")
print(f"  âœ… å¹³å‡è®­ç»ƒæ—¶é—´: {np.mean(train_times):.2f}ç§’")

print("\nğŸ’¡ å»ºæ¨¡å»ºè®®:")
print("âœ… ç®€å•æ¨¡å‹(æœ´ç´ è´å¶æ–¯)è®­ç»ƒå¿«é€Ÿï¼Œé€‚åˆå¿«é€ŸåŸå‹")
print("âœ… å¤æ‚æ¨¡å‹(éšæœºæ£®æ—)æ•ˆæœæ›´å¥½ï¼Œä½†éœ€è¦æ›´å¤šè®¡ç®—èµ„æº")
print("âœ… çº¿æ€§æ¨¡å‹(é€»è¾‘å›å½’)å¹³è¡¡äº†æ•ˆæœå’Œé€Ÿåº¦")
print("âœ… å¯ä»¥æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹")
print("âœ… æ¨¡å‹å·²å‡†å¤‡å¥½è¿›å…¥è¯„ä¼°é˜¶æ®µï¼")
