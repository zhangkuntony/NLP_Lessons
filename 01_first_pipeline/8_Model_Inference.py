# æ¨ç†éƒ¨ç½²å®æˆ˜ä»£ç 
import pickle
import time
import json
from datetime import datetime
import numpy as np
import jieba
import matplotlib.pyplot as plt
from collections import Counter

# è®¾ç½®å­—ä½“ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸª === æ¨¡å‹æ¨ç†ç³»ç»Ÿæ„å»º ===")

def _preprocess_text(preprocess_text):
    """æ–‡æœ¬é¢„å¤„ç†"""
    if not preprocess_text or not isinstance(preprocess_text, str):
        return ""

    # åˆ†è¯
    words = jieba.lcut(preprocess_text.strip())

    # å»é™¤åœç”¨è¯
    stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°',
                  'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è¿™'}

    words = [w for w in words if w not in stop_words and len(w.strip()) > 1]
    return ' '.join(words)

# 1. æ¨¡æ‹Ÿä¿å­˜å’ŒåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
class IntentClassifier:
    """æ™ºèƒ½å®¢æœæ„å›¾åˆ†ç±»å™¨"""

    def __init__(self):
        self.model = None
        self.vectorizer = None
        self.label_encoder = None
        self.is_trained = False

        # æ€§èƒ½ç»Ÿè®¡
        self.total_requests = 0
        self.total_time = 0
        self.error_count = 0

        # ç¼“å­˜æœºåˆ¶
        self.cache = {}
        self.cache_hit = 0

    def save_model(self, model_path="intent_model.pkl"):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        model_data = {
            'model': self.model,
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder
        }

        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)              # type: ignore
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")

    def load_model(self, model_path="intent_model.pkl"):
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹"""
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)

            self.model = model_data['model']
            self.vectorizer = model_data['vectorizer']
            self.label_encoder = model_data['label_encoder']
            self.is_trained = True
            print(f"âœ… æ¨¡å‹å·²ä» {model_path} åŠ è½½")

        except FileNotFoundError:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹")
            self._create_mock_model()

    def _create_mock_model(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹ç”¨äºæ¼”ç¤º"""
        # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„è§„åˆ™æ¨¡å‹
        self.intent_keywords = {
            'é€€æ¬¾å’¨è¯¢': ['é€€æ¬¾', 'é€€é’±', 'ç”³è¯·é€€', 'æ€ä¹ˆé€€'],
            'ç‰©æµæŸ¥è¯¢': ['å‘è´§', 'ç‰©æµ', 'å¿«é€’', 'æ”¶åˆ°è´§', 'é…é€'],
            'ä¼˜æƒ å’¨è¯¢': ['ä¼˜æƒ ', 'æ‰“æŠ˜', 'æ´»åŠ¨', 'ä¿ƒé”€', 'æŠ˜æ‰£'],
            'å”®åæŠ•è¯‰': ['æŠ•è¯‰', 'è´¨é‡', 'é—®é¢˜', 'ä¸æ»¡æ„', 'å·®'],
            'è”ç³»æ–¹å¼': ['å®¢æœ', 'ç”µè¯', 'è”ç³»', 'äººå·¥']
        }
        self.is_trained = True
        print("âœ… æ¨¡æ‹Ÿæ¨¡å‹åˆ›å»ºå®Œæˆ")

    def _predict_by_rules(self, predict_text):
        """åŸºäºè§„åˆ™çš„ç®€å•é¢„æµ‹ï¼ˆæ¨¡æ‹Ÿæ¨¡å‹ï¼‰"""
        text_lower = predict_text.lower()

        for intent, keywords in self.intent_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    # æ¨¡æ‹Ÿç½®ä¿¡åº¦
                    confidence = np.random.uniform(0.8, 0.95)
                    return intent, confidence

        # é»˜è®¤é¢„æµ‹
        return 'å…¶ä»–', 0.5

    def predict(self, predict_text, use_cache=True):
        """é¢„æµ‹æ–‡æœ¬æ„å›¾"""
        start_time = time.time()
        self.total_requests += 1

        try:
            # è¾“å…¥éªŒè¯
            if not predict_text or not isinstance(predict_text, str):
                raise ValueError("è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

            # æ£€æŸ¥ç¼“å­˜
            if use_cache and predict_text in self.cache:
                self.cache_hit += 1
                predict_result = self.cache[predict_text]
                print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: '{predict_text}' â†’ {predict_result['intent']}")
                return predict_result

            # æ–‡æœ¬é¢„å¤„ç†
            processed_text = _preprocess_text(predict_text)

            # æ¨¡å‹é¢„æµ‹
            if self.is_trained:
                intent, confidence = self._predict_by_rules(processed_text)
            else:
                raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒ")

            # æ„å»ºç»“æœ
            predict_result = {
                'intent': intent,
                'confidence': float(confidence),
                'processed_text': processed_text,
                'timestamp': datetime.now().isoformat()
            }

            # ç¼“å­˜ç»“æœ
            if use_cache:
                self.cache[predict_text] = predict_result

            # è®°å½•æ€§èƒ½
            inference_time = time.time() - start_time
            self.total_time += inference_time

            return predict_result

        except Exception as e:
            self.error_count += 1
            print(f"âŒ é¢„æµ‹å‡ºé”™: {str(e)}")

            # è¿”å›é™çº§ç»“æœ
            return {
                'intent': 'å…¶ä»–',
                'confidence': 0.0,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

    def get_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if self.total_requests > 0:
            avg_time = self.total_time / self.total_requests
            cache_rate = self.cache_hit / self.total_requests
            error_rate = self.error_count / self.total_requests
        else:
            avg_time = cache_rate = error_rate = 0

        return {
            'total_requests': self.total_requests,
            'average_time_ms': avg_time * 1000,
            'cache_hit_rate': cache_rate,
            'error_rate': error_rate,
            'cache_size': len(self.cache)
        }

# 2. åˆ›å»ºæ¨ç†æœåŠ¡
print("\nğŸš€ === åˆ›å»ºæ¨ç†æœåŠ¡ ===")

# åˆå§‹åŒ–åˆ†ç±»å™¨
classifier = IntentClassifier()
classifier.load_model()                 # åŠ è½½æ¨¡å‹ï¼ˆä¼šä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ï¼‰

print("\nğŸ§ª === æ¨ç†åŠŸèƒ½æµ‹è¯• ===")

# æµ‹è¯•æ ·æœ¬
test_samples = [
    "æˆ‘è¦ç”³è¯·é€€æ¬¾",
    "è®¢å•ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿ",
    "æœ‰ä»€ä¹ˆä¼˜æƒ æ´»åŠ¨å—",
    "äº§å“è´¨é‡æœ‰é—®é¢˜è¦æŠ•è¯‰",
    "å®¢æœç”µè¯æ˜¯å¤šå°‘",
    "æˆ‘è¦ç”³è¯·é€€æ¬¾",                   # é‡å¤ï¼Œæµ‹è¯•ç¼“å­˜
    "æ€ä¹ˆé€€é’±å•Š",
    "è¿™ä¸ªå•†å“ä»€ä¹ˆæ—¶å€™èƒ½åˆ°è´§",
    "",                             # ç©ºè¾“å…¥ï¼Œæµ‹è¯•å¼‚å¸¸å¤„ç†
    "éšä¾¿è¯´ç‚¹ä»€ä¹ˆ"                    # æœªçŸ¥æ„å›¾
]

print("å¼€å§‹æ‰¹é‡æ¨ç†æµ‹è¯•...")
results = []

for i, text in enumerate(test_samples):
    print(f"\næµ‹è¯• {i + 1}: '{text}'")
    result = classifier.predict(text)
    results.append(result)

    if 'error' not in result:
        print(f"  é¢„æµ‹ç»“æœ: {result['intent']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
    else:
        print(f"  é”™è¯¯: {result['error']}")

print("\nğŸ“Š === æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š ===")

# è·å–æ€§èƒ½ç»Ÿè®¡
stats = classifier.get_stats()
print("ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ï¼š")
for metric, value in stats.items():
    if isinstance(value, float):
        print(f"    {metric}: {value:.4f}")
    else:
        print(f"    {metric}: {value}")

print("\nâš¡ === å¹¶å‘æ€§èƒ½æµ‹è¯• ===")

# æ¨¡æ‹Ÿå¹¶å‘è¯·æ±‚
def simulate_concurrent_requests(classifier_model, num_requests=100):
    """æ¨¡æ‹Ÿå¹¶å‘è¯·æ±‚æµ‹è¯•"""
    test_texts = ["é€€æ¬¾æ€ä¹ˆç”³è¯·", "æŸ¥è¯¢ç‰©æµ", "æœ‰ä¼˜æƒ å—", "è¦æŠ•è¯‰", "è”ç³»å®¢æœ"] * (num_requests // 5 + 1)

    start_time = time.time()

    for test_text in test_texts[:num_requests]:
        classifier_model.predict(test_text)

    total_request_time = time.time() - start_time
    request_qps = num_requests / total_request_time

    return request_qps, total_request_time

# è¿è¡Œå¹¶å‘æµ‹è¯•
print("æ¨¡æ‹Ÿ100ä¸ªå¹¶å‘è¯·æ±‚...")
qps, total_time = simulate_concurrent_requests(classifier, 100)

print(f"å¹¶å‘æ€§èƒ½æµ‹è¯•ç»“æœ:")
print(f"  æ€»è¯·æ±‚æ•°: 100")
print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
print(f"  QPS (æ¯ç§’è¯·æ±‚æ•°): {qps:.2f}")
print(f"  å¹³å‡å“åº”æ—¶é—´: {total_time/100*1000:.2f}ms")

print("\nğŸ“ˆ === æ¨ç†ç»“æœå¯è§†åŒ– ===")

# ç»Ÿè®¡æ„å›¾åˆ†å¸ƒ
# ç»Ÿè®¡é¢„æµ‹ç»“æœ
valid_results = [r for r in results if 'error' not in r]
intent_counts = Counter([r['intent'] for r in valid_results])

plt.figure(figsize=(12, 5))

# æ„å›¾åˆ†å¸ƒé¥¼å›¾
plt.subplot(1, 2, 1)
colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC']
plt.pie(intent_counts.values(), labels=list(intent_counts.keys()), autopct='%1.1f%%',
        colors=colors[:len(intent_counts)], startangle=90)
plt.title('é¢„æµ‹æ„å›¾åˆ†å¸ƒ')

# ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
plt.subplot(1, 2, 2)
confidences = [r['confidence'] for r in valid_results if r['confidence'] > 0]
plt.hist(confidences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
plt.xlabel('ç½®ä¿¡åº¦')
plt.ylabel('é¢‘æ¬¡')

plt.tight_layout()
plt.show()

print("\nğŸ¯ === APIæ¥å£ç¤ºä¾‹ ===")

# æ¨¡æ‹ŸAPIæ¥å£
def intent_api(api_text):
    """æ¨¡æ‹ŸAPIæ¥å£"""
    try:
        api_response_result = classifier.predict(api_text)

        # æ„å»ºAPIå“åº”æ ¼å¼
        api_response = {
            "code": 200,
            "message": "success",
            "data": {
                "intent": api_response_result['intent'],
                "confidence": api_response_result['confidence'],
                "timestamp": api_response_result['timestamp']
            }
        }

        if 'error' in api_response_result:
            api_response["code"] = 500
            api_response["message"] = api_response_result['error']

        return api_response

    except Exception as e:
        return {
            "code": 500,
            "message": f"Internal Server Error: {str(e)}",
            "data": None
        }

# æµ‹è¯•APIæ¥å£
print("APIæ¥å£è°ƒç”¨ç¤ºä¾‹ï¼š")
api_test_cases = [
    "æˆ‘æƒ³è¦é€€æ¬¾",
    "æŸ¥è¯¢è®¢å•çŠ¶æ€",
    "æœ‰ä»€ä¹ˆä¼˜æƒ "
]

for text in api_test_cases:
    response = intent_api(text)
    print(f"\nè¯·æ±‚: '{text}'")
    print(f"å“åº”: {json.dumps(response, ensure_ascii=False, indent=2)}")

print("\nâœ… === æ¨ç†ç³»ç»Ÿæ€»ç»“ ===")

final_stats = classifier.get_stats()

print("ğŸ¯ æ¨ç†ç³»ç»Ÿå®Œæˆæƒ…å†µ:")
print(f"  âœ… å¤„ç†è¯·æ±‚æ€»æ•°: {final_stats['total_requests']}")
print(f"  âœ… å¹³å‡å“åº”æ—¶é—´: {final_stats['average_time_ms']:.2f}ms")
print(f"  âœ… ç¼“å­˜å‘½ä¸­ç‡: {final_stats['cache_hit_rate']:.1%}")
print(f"  âœ… é”™è¯¯ç‡: {final_stats['error_rate']:.1%}")

print("\nğŸš€ éƒ¨ç½²å»ºè®®:")
print("âœ… æ¨¡å‹å·²å°è£…æˆå¯è°ƒç”¨çš„æœåŠ¡")
print("âœ… æ”¯æŒæ‰¹é‡æ¨ç†å’Œå®æ—¶å“åº”")
print("âœ… åŒ…å«ç¼“å­˜æœºåˆ¶å’Œå¼‚å¸¸å¤„ç†")
print("âœ… æä¾›æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡åŠŸèƒ½")
print("âœ… å¯ä»¥ç›´æ¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")

print("\nğŸ‰ === NLPå®Œæ•´æµç¨‹ç»“æŸ ===")
print("æ­å–œï¼ä½ å·²ç»å®Œæˆäº†ä»é—®é¢˜å®šä¹‰åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´NLPæµç¨‹ï¼")
print("ğŸ¯ ä¸‹ä¸€æ­¥å¯ä»¥è€ƒè™‘ï¼š")
print("  ğŸ“ˆ åœ¨çœŸå®æ•°æ®ä¸Šè®­ç»ƒæ›´å¼ºçš„æ¨¡å‹")
print("  ğŸš€ éƒ¨ç½²åˆ°äº‘ç«¯æœåŠ¡å™¨")
print("  ğŸ“Š å»ºç«‹å®Œæ•´çš„ç›‘æ§ä½“ç³»")
print("  ğŸ”„ å»ºç«‹æ¨¡å‹çš„æŒç»­ä¼˜åŒ–æµç¨‹")