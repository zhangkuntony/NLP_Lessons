# ğŸ”§ åŸºç¡€å·¥å…·
import os          # æ“ä½œç³»ç»Ÿæ¥å£
import random      # éšæœºæ•°ç”Ÿæˆå™¨
import re          # æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºæ–‡æœ¬æ¸…ç†

# ğŸ“Š æ•°æ®å¤„ç†å’Œåˆ†æçš„"ç‘å£«å†›åˆ€"
import numpy as np           # æ•°å€¼è®¡ç®—åº“
import pandas as pd          # æ•°æ®åˆ†æç¥å™¨

# ğŸ¨ è®©æ•°æ®"ç°å½¢"çš„å¯è§†åŒ–å·¥å…·
import matplotlib.pyplot as plt  # åŸºç¡€ç”»å›¾å·¥å…·
import seaborn as sns           # æ›´ç¾è§‚çš„ç»Ÿè®¡å›¾è¡¨

# ğŸ”¤ æ–‡æœ¬å¤„ç†ä¸“ä¸šå·¥å…·
import nltk                     # è‡ªç„¶è¯­è¨€å·¥å…·åŒ…
from paddlex.inference.models.common.tokenizer import vocab
from sklearn.feature_extraction.text import CountVectorizer  # è¯è¢‹æ¨¡å‹å·¥å…·

from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import RegexpTokenizer

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸ‰ å·¥å…·ç®±å‡†å¤‡å®Œæ¯•ï¼è®©æˆ‘ä»¬å¼€å§‹æ–‡æœ¬é­”æ³•ä¹‹æ—…å§ï¼")
#
# clean_data = pd.read_csv("Tweets.csv")
# print(clean_data.head())
# print(clean_data.info())
#
# sns.countplot(x="airline_sentiment", data=clean_data)
# # plt.title('èˆªç©ºæƒ…æ„Ÿåˆ†å¸ƒ')
# # plt.xlabel('airline_sentiment')
# # plt.ylabel('count')
# # plt.show()
#
# # First of all let's drop the columns which we don't required
#
# waste_col = [
#     "tweet_id",
#     "airline_sentiment_confidence",
#     "negativereason",
#     "negativereason_confidence",
#     "airline",
#     "airline_sentiment_gold",
#     "name",
#     "negativereason_gold",
#     "retweet_count",
#     "tweet_coord",
#     "tweet_created",
#     "tweet_location",
#     "user_timezone",
# ]
#
# data = clean_data.drop(waste_col, axis=1)
#
# print(data.head())
#
# def sentiment(x):
#     if x == "positive":
#         return 1
#     elif x == "negative":
#         return -1
#     else:
#         return 0
#
# nltk.download('stopwords')
#
# stopwords = stopwords.words('english')
# stemmer = SnowballStemmer('english')
# tokenizer = RegexpTokenizer(r'\w+')
# # As this dataset is fetched from twitter so it has lots of people tag in tweets
# # we will remove them
# tags = r"@\w*"
#
# def preprocess_text(sentence, stem=False):
#     sentence = [re.sub(tags, "", sentence)]
#     text = []
#     for word in sentence:
#         if word not in stopwords:
#             if stem:
#                 # å¯ç”¨è¯å¹²æå–ï¼Œä¾‹å¦‚running -> run
#                 text.append(stemmer.stem(word).lower())
#             else:
#                 text.append(word.lower())
#
#     return tokenizer.tokenize(" ".join(text))
#
# print(f"Orignal Text : {data.text[11]}")
# print()
# print(f"Preprocessed Text : {preprocess_text(data.text[11])}")
#
# data.text = data.text.map(preprocess_text)
# print(data.head())

# # ç¬¬ä¸€å…³ï¼šOne-Hot ç¼–ç 
# # this is an example vocabulary just to make concept clear
# sample_vocab = ["the", "cat", "sat", "on", "mat", "dog", "run", "green", "tree"]
# # data_vocab = set(sample_vocab)
#
# # vocabulary of words present in dataset
# data_vocab = []
# for text in data.text:
#     for word in text:
#         if word not in data_vocab:
#             data_vocab.append(word)
#
# # function to return one-hot representation of passed text
# def get_onehot_representation(text_to_onehot, vocab_for_onehot=None):
#     if vocab_for_onehot is None:
#         vocab_for_onehot = data_vocab
#     onehot_encoded = []
#     for word_to_onehot in text_to_onehot:
#         temp = [0] * len(vocab_for_onehot)
#         temp[vocab_for_onehot.index(word_to_onehot)] = 1
#         onehot_encoded.append(temp)
#     return onehot_encoded
#
# print('One Hot Representation for sentence "the cat sat on the mat" :')
# print(get_onehot_representation(["the", "cat", "sat", "on", "the", "cat"], sample_vocab))
#
# print(f"Length of Vocabulary : {len(data_vocab)}")
# print(f"Sample of Vocabulary : {data_vocab[302 : 312]}")
#
# sample_one_hot_rep = get_onehot_representation(data.text[7], data_vocab)
# print(f"Shapes of a single sentence : {np.array(sample_one_hot_rep).shape}")
#
# # å¥å­çš„ one-hot è¡¨ç¤º
#
# # data.loc[:, 'one_hot_rep'] = data.loc[:, 'text'].map(get_onehot_representation)
#
# # å¦‚æœæ‚¨è¿è¡Œæ­¤å•å…ƒï¼Œå®ƒå°†ç»™æ‚¨ä¸€ä¸ªå†…å­˜é”™è¯¯
#
# print(data.head())


# ç¬¬äºŒå…³ï¼šè¯è¢‹æ¨¡å‹ï¼ˆBOWï¼‰

from sklearn.feature_extraction.text import CountVectorizer

sample_bow = CountVectorizer()

sample_corpus = ["the cat sat", "the cat sat in the hat", "the cat with the hat"]

sample_bow.fit(sample_corpus)

def get_bow_representation(text):
    return sample_bow.transform(text)

print(f"Vocabulary mapping for given sample corpus : \n {sample_bow.vocabulary_}")
print(f"Sorted vocabulary (by index): \n{sorted(sample_bow.vocabulary_.items(), key=lambda x: x[1])}")
print("\nBag of word Representation of sentence 'the cat cat sat in the hat'")
print(get_bow_representation(["the cat cat sat in the hat"]).toarray())