# ğŸ”§ åŸºç¡€å·¥å…·
import os          # æ“ä½œç³»ç»Ÿæ¥å£
import random      # éšæœºæ•°ç”Ÿæˆå™¨
import re          # æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºæ–‡æœ¬æ¸…ç†

# ğŸ“Š æ•°æ®å¤„ç†å’Œåˆ†æçš„"ç‘å£«å†›åˆ€"
import numpy as np           # æ•°å€¼è®¡ç®—åº“
import pandas as pd          # æ•°æ®åˆ†æç¥å™¨

# ğŸ¨ è®©æ•°æ®"ç°å½¢"çš„å¯è§†åŒ–å·¥å…·
import matplotlib.pyplot as plt  # åŸºç¡€ç”»å›¾å·¥å…·
import seaborn as sns           # æ›´ç¾è§‚çš„ç»Ÿè®¡å›¾è¡¨

# ğŸ”¤ æ–‡æœ¬å¤„ç†ä¸“ä¸šå·¥å…·
import nltk                     # è‡ªç„¶è¯­è¨€å·¥å…·åŒ…
from fastai.metrics import perplexity
from paddlex.inference.models.common.tokenizer import vocab
from sklearn.feature_extraction.text import CountVectorizer  # è¯è¢‹æ¨¡å‹å·¥å…·
from gensim.models import Word2Vec  # Word2Vecæ¨¡å‹

from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import RegexpTokenizer

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸ‰ å·¥å…·ç®±å‡†å¤‡å®Œæ¯•ï¼è®©æˆ‘ä»¬å¼€å§‹æ–‡æœ¬é­”æ³•ä¹‹æ—…å§ï¼")

clean_data = pd.read_csv("Tweets.csv")
print(clean_data.head())
print(clean_data.info())

sns.countplot(x="airline_sentiment", data=clean_data)
plt.title('èˆªç©ºæƒ…æ„Ÿåˆ†å¸ƒ')
plt.xlabel('airline_sentiment')
plt.ylabel('count')
plt.show()

# First of all let's drop the columns which we don't required

waste_col = [
    "tweet_id",
    "airline_sentiment_confidence",
    "negativereason",
    "negativereason_confidence",
    "airline",
    "airline_sentiment_gold",
    "name",
    "negativereason_gold",
    "retweet_count",
    "tweet_coord",
    "tweet_created",
    "tweet_location",
    "user_timezone",
]

data = clean_data.drop(waste_col, axis=1)

print(data.head())

def sentiment(x):
    if x == "positive":
        return 1
    elif x == "negative":
        return -1
    else:
        return 0

nltk.download('stopwords')

stopwords = stopwords.words('english')
stemmer = SnowballStemmer('english')
tokenizer = RegexpTokenizer(r'\w+')
# As this dataset is fetched from twitter so it has lots of people tag in tweets
# we will remove them
tags = r"@\w*"

def preprocess_text(sentence, stem=False):
    sentence = [re.sub(tags, "", sentence)]
    text = []
    for word in sentence:
        if word not in stopwords:
            if stem:
                # å¯ç”¨è¯å¹²æå–ï¼Œä¾‹å¦‚running -> run
                text.append(stemmer.stem(word).lower())
            else:
                text.append(word.lower())

    return tokenizer.tokenize(" ".join(text))

print(f"Orignal Text : {data.text[11]}")
print()
print(f"Preprocessed Text : {preprocess_text(data.text[11])}")

data.text = data.text.map(preprocess_text)
print(data.head())

# ç¬¬ä¸€å…³ï¼šOne-Hot ç¼–ç 
# this is an example vocabulary just to make concept clear
sample_vocab = ["the", "cat", "sat", "on", "mat", "dog", "run", "green", "tree"]
# data_vocab = set(sample_vocab)

# vocabulary of words present in dataset
data_vocab = []
for text in data.text:
    for word in text:
        if word not in data_vocab:
            data_vocab.append(word)

# function to return one-hot representation of passed text
def get_onehot_representation(text_to_onehot, vocab_for_onehot=None):
    if vocab_for_onehot is None:
        vocab_for_onehot = data_vocab
    onehot_encoded = []
    for word_to_onehot in text_to_onehot:
        temp = [0] * len(vocab_for_onehot)
        temp[vocab_for_onehot.index(word_to_onehot)] = 1
        onehot_encoded.append(temp)
    return onehot_encoded

print('One Hot Representation for sentence "the cat sat on the mat" :')
print(get_onehot_representation(["the", "cat", "sat", "on", "the", "cat"], sample_vocab))

print(f"Length of Vocabulary : {len(data_vocab)}")
print(f"Sample of Vocabulary : {data_vocab[302 : 312]}")

sample_one_hot_rep = get_onehot_representation(data.text[7], data_vocab)
print(f"Shapes of a single sentence : {np.array(sample_one_hot_rep).shape}")

# å¥å­çš„ one-hot è¡¨ç¤º

# data.loc[:, 'one_hot_rep'] = data.loc[:, 'text'].map(get_onehot_representation)

# å¦‚æœæ‚¨è¿è¡Œæ­¤å•å…ƒï¼Œå®ƒå°†ç»™æ‚¨ä¸€ä¸ªå†…å­˜é”™è¯¯

print(data.head())


# ç¬¬äºŒå…³ï¼šè¯è¢‹æ¨¡å‹ï¼ˆBOWï¼‰

from sklearn.feature_extraction.text import CountVectorizer

sample_bow = CountVectorizer()

sample_corpus = ["the cat sat", "the cat sat in the hat", "the cat with the hat"]

sample_bow.fit(sample_corpus)

def get_bow_representation(text):
    return sample_bow.transform(text)

print(f"Vocabulary mapping for given sample corpus : \n {sample_bow.vocabulary_}")
print(f"Sorted vocabulary (by index): \n{sorted(sample_bow.vocabulary_.items(), key=lambda x: x[1])}")
print("\nBag of word Representation of sentence 'the cat cat sat in the hat'")
print(get_bow_representation(["the cat cat sat in the hat"]).toarray())

sample_bow = CountVectorizer(binary=True)

sample_corpus = ["the cat sat", "the cat sat in the hat", "the cat with the hat"]

sample_bow.fit(sample_corpus)

def get_bow_representation(text):
    return sample_bow.transform(text)

print(f"Vacabulary mapping for given sample corpus : \n {sample_bow.vocabulary_}")
print(
    "\nBag of word Representation of sentence 'the the the the cat cat sat in the hat'"
)
print(get_bow_representation(["the the the the cat cat sat in the hat"]).toarray())

# generate bag of word representation for given dataset

bow = CountVectorizer()
bow_rep = bow.fit_transform(data.loc[:, "text"].astype("str"))

# intrested one can see vocabulary of given corpus by uncommenting below code line
# bow.vocabulary_
print(f"Shape of Bag of word representaion matrix : {bow_rep.toarray().shape}")


# ç¬¬ä¸‰å…³ï¼šN-Gramsè¯è¢‹

# Bag of 1-gram (unigram)
from sklearn.feature_extraction.text import CountVectorizer

sample_boN = CountVectorizer(ngram_range=(1, 1))
sample_corpus = ["the cat sat", "the cat sat in the hat", "the cat with the hat"]
sample_boN.fit(sample_corpus)

def get_bo_n_representation(text):
    return sample_boN.transform(text)

print(f"Unigram Vocabulary mapping for given sample corpus : \n {sample_boN.vocabulary_}")
print("\nBag of 1-gram (unigram) Representation of sentence 'the cat cat sat in the hat'")
print(get_bo_n_representation(["the cat cat sat in the hat"]).toarray())

# Bag of 2-gram (bigram)
sample_boN = CountVectorizer(ngram_range=(2, 2))
sample_corpus = ["the cat sat", "the cat sat in the hat", "the cat with the hat"]
sample_boN.fit(sample_corpus)

def get_bo_n_representation(text):
    return sample_boN.transform(text)

print(f"Bigram Vocabulary mapping for given sample corpus : \n {sample_boN.vocabulary_}")
print("\nBag of 2-gram (bigram) Representation of sentence 'the cat cat sat in the hat'")
print(get_bo_n_representation(["the cat cat sat in the hat"]).toarray())


# Bag of 3-gram (trigram)
sample_boN = CountVectorizer(ngram_range=(3, 3))
sample_corpus = ["the cat sat", "the cat sat in the hat", "the cat with the hat"]
sample_boN.fit(sample_corpus)

def get_bo_n_representation(text):
    return sample_boN.transform(text)


print(f"Trigram Vocabulary mapping for given sample corpus : \n {sample_boN.vocabulary_}")
print("\nBag of 3-gram (trigram) Representation of sentence 'the cat cat sat in the hat'")
print(get_bo_n_representation(["the cat cat sat in the hat"]).toarray())


# ç¬¬å››å…³ï¼šTF-IDF

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
sample_corpus = ["the cat sat", "the cat sat in the hat", "the cat with the hat"]
tfidf_rep = tfidf.fit_transform(sample_corpus)
print(f"IDF Values for sample corpus : {tfidf.idf_}")

print("TF-IDF Representation for sentence 'the cat sat in the hat' :")
print(tfidf.transform(["the cat sat in the hat"]).toarray())


# ç¬¬äº”å…³ï¼šWord2vec

# åˆ›å»ºä¸€äº›ç¤ºä¾‹æ•°æ®æ¥è®­ç»ƒWord2Vecæ¨¡å‹
sample_sentences = [
    ['good', 'movie', 'great', 'acting'],
    ['bad', 'movie', 'terrible', 'acting'],
    ['computer', 'science', 'programming', 'python'],
    ['happy', 'feeling', 'good', 'great'],
    ['sad', 'feeling', 'bad', 'terrible'],
    ['love', 'romance', 'great', 'story'],
    ['hate', 'dislike', 'bad', 'terrible'],
    ['python', 'programming', 'computer', 'good'],
    ['excellent', 'great', 'amazing', 'good'],
    ['awful', 'terrible', 'horrible', 'bad'],
    ['technology', 'computer', 'science', 'innovative'],
    ['art', 'beautiful', 'creative', 'great']
]

# è®­ç»ƒWord2Vecæ¨¡å‹
print("ğŸš€ è®­ç»ƒWord2Vecæ¨¡å‹...")
Word2VecModel = Word2Vec(
    sentences=sample_sentences,
    vector_size=100,    # è¯å‘é‡ç»´åº¦
    window=5,           # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    min_count=1,        # æœ€å°è¯é¢‘
    workers=4,          # å¹¶è¡Œæ•°
    sg=1                # ä½¿ç”¨Skip-gramç®—æ³•
)

print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼è¯æ±‡è¡¨å¤§å°: {len(Word2VecModel.wv)}")

# æ–¹æ³•1: ä½¿ç”¨Plotlyè¿›è¡Œäº¤äº’å¼å¯è§†åŒ–
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from sklearn.manifold import TSNE

plotly_available = True

if plotly_available:
    def plot_embeddings_plotly(embeddings, words, title="äº¤äº’å¼è¯åµŒå…¥å¯è§†åŒ–"):
        """ä½¿ç”¨Plotlyåˆ›å»ºäº¤äº’å¼çš„embeddingå¯è§†åŒ–"""

        # å°†åˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„
        embeddings = np.array(embeddings)
        print(f"è¯å‘é‡çŸ©é˜µå½¢çŠ¶: {embeddings.shape}")

        # è°ƒæ•´perplexityå‚æ•°ï¼Œç¡®ä¿å°äºæ ·æœ¬æ•°
        perplexity = min(15, embeddings.shape[0] - 1)
        if perplexity < 1:
            perplexity = 1

        # ä½¿ç”¨t-SNEé™ä¸ºåˆ°2D
        tsne_2d = TSNE(n_components=2, random_state=42, perplexity=perplexity)
        embeddings_2d = tsne_2d.fit_transform(embeddings)

        # åˆ›å»ºDataFrameç”¨äºPlotly
        df = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1],
            'word': words,
            'cluster': ['cluster_' + str(i // 30) for i in range(len(words))]
        })

        # åˆ›å»ºäº¤äº’å¼æ•£ç‚¹å›¾
        fig = px.scatter(df, x="x", y="y", color="cluster",
                         hover_name='word', title=title,
                         width=800, height=600)

        # è‡ªå®šä¹‰æ‚¬åœä¿¡æ¯
        fig.update_traces(
            hovertemplate = '<b>%{hovertext}</b><br>' +
                          'X: %{x:.2f}<br>' +
                          'Y: %{y:.2f}<br>' +
                          '<extra></extra>',
            hovertext = df['word']
        )

        # ç¾åŒ–å›¾è¡¨
        fig.update_layout(
            title_font_size=16,
            xaxis_title='t-SNE ç»´åº¦ 1',
            yaxis_title='t-SNE ç»´åº¦ 2',
            showlegend=True,
            template="plotly_white"
        )

        return fig

    # åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œå¯è§†åŒ–
    if 'Word2VecModel' in locals():
        # é€‰æ‹©ä¸€äº›å…³é”®è¯
        sample_words = ["good", "bad", "great", "terrible", "computer", "science",
                       "python", "programming", "happy", "sad", "love", "hate"]

        # è·å–å¯¹åº”çš„è¯å‘é‡
        sample_embeddings = []
        available_words = []

        for word in sample_words:
            try:
                embedding = Word2VecModel.wv[word]
                sample_embeddings.append(embedding)
                available_words.append(word)
            except KeyError:
                print(f"è¯ '{word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")

        if sample_embeddings:
            # åˆ›å»ºäº¤äº’å¼å¯è§†åŒ–
            fig = plot_embeddings_plotly(sample_embeddings, available_words)
            fig.show()
        else:
            print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è¯æ±‡")

    else:
        print("Word2VecModel ä¸å¯ç”¨ï¼Œè·³è¿‡Plotlyå¯è§†åŒ–")
else:
    print("Plotlyä¸å¯ç”¨ï¼Œè·³è¿‡äº¤äº’å¼å¯è§†åŒ–")

# æ–¹æ³•2ï¼šä½¿ç”¨TensorBoard Embedding Projector (å®˜æ–¹æ–¹æ³•)
import os
import tensorflow as tf
from tensorboard.plugins.projector import ProjectorConfig


def create_tensorboard_embeddings(embeddings, labels, log_dir="./embedding_logs"):
    """
    åˆ›å»ºTensorBoard embedding projectorå¯è§†åŒ–

    Args:
        embeddings: è¯åµŒå…¥çŸ©é˜µ (n_words, embedding_dim)
        labels: è¯æ±‡åˆ—è¡¨
        log_dir: æ—¥å¿—ç›®å½•
    """

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(log_dir, exist_ok=True)

    # åˆ›å»ºmetadataæ–‡ä»¶ï¼ˆè¯æ±‡æ ‡ç­¾ï¼‰
    metadata_path = os.path.join(log_dir, "metadata.tsv")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        f.write("Word\n")  # åˆ—æ ‡é¢˜
        for label in labels:
            f.write(f"{label}\n")

    # ä¿å­˜è¯å‘é‡åˆ°æ–‡ä»¶
    embeddings_path = os.path.join(log_dir, "embeddings.tsv")
    with open(embeddings_path, 'w', encoding='utf-8') as f:
        for embedding in embeddings:
            f.write("\t".join(map(str, embedding)) + "\n")

    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config = {
        "embeddings": [
            {
                "tensorName": "word_embeddings",
                "tensorShape": list(embeddings.shape),
                "tensorPath": embeddings_path,
                "metadataPath": metadata_path
            }
        ]
    }

    import json
    config_path = os.path.join(log_dir, "projector_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2)

    print(f"TensorBoard embedding æ–‡ä»¶å·²ä¿å­˜åˆ°: {log_dir}")
    print("è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨TensorBoard:")
    print(f"tensorboard --logdir={log_dir}")
    print("ç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006 æŸ¥çœ‹äº¤äº’å¼embeddingå¯è§†åŒ–")


# å¦‚æœæœ‰è¯å‘é‡æ¨¡å‹ï¼Œåˆ›å»ºTensorBoardå¯è§†åŒ–
if 'Word2VecModel' in locals():
    try:
        # é€‰æ‹©å‰1000ä¸ªæœ€å¸¸ç”¨çš„è¯
        vocab_size = min(1000, len(Word2VecModel.wv.key_to_index))
        selected_words = list(Word2VecModel.wv.key_to_index.keys())[:vocab_size]
        selected_embeddings = np.array([Word2VecModel.wv[word] for word in selected_words])

        # åˆ›å»ºTensorBoardå¯è§†åŒ–
        create_tensorboard_embeddings(selected_embeddings, selected_words)

    except Exception as e:
        print(f"åˆ›å»ºTensorBoardå¯è§†åŒ–æ—¶å‡ºé”™: {e}")
else:
    print("Word2VecModel ä¸å¯ç”¨ï¼Œè·³è¿‡TensorBoardå¯è§†åŒ–")