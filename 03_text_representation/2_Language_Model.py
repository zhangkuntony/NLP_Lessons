# å¯¼å…¥å¿…è¦çš„åº“
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter, defaultdict
import re
import math
import random
from typing import List, Dict, Tuple, Set
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸ“š æ¬¢è¿æ¥åˆ°è¯­è¨€æ¨¡å‹è¯¾ç¨‹ï¼")


# ğŸ² æ¦‚ç‡è®ºæ¦‚å¿µçš„å¯è§†åŒ–æ¼”ç¤º

# 1. ğŸª™ æŠ›ç¡¬å¸å®éªŒ
def coin_flip_simulation():
    """æ¨¡æ‹ŸæŠ›ç¡¬å¸å®éªŒï¼Œå±•ç¤ºæ¦‚ç‡çš„åŸºæœ¬æ¦‚å¿µ"""
    print("ğŸª™ æŠ›ç¡¬å¸å®éªŒï¼šè§‚å¯Ÿæ¦‚ç‡å¦‚ä½•ç¨³å®š")
    print("=" * 40)

    # æ¨¡æ‹Ÿä¸åŒæ¬¡æ•°çš„æŠ›ç¡¬å¸
    import random

    flip_counts = [10, 50, 100, 500, 1000, 5000]
    head_ratios = []

    for n_flips in flip_counts:
        heads = sum(1 for _ in range(n_flips) if random.choice(['æ­£é¢', 'åé¢']) == 'æ­£é¢')
        ratio = heads / n_flips
        head_ratios.append(ratio)
        print(f"æŠ› {n_flips:4d} æ¬¡ï¼šæ­£é¢ {heads:4d} æ¬¡ï¼Œæ¯”ä¾‹ {ratio:.3f} ({ratio * 100:.1f}%)")

    # å¯è§†åŒ–
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # å·¦å›¾ï¼šéšç€å®éªŒæ¬¡æ•°å¢åŠ ï¼Œæ¦‚ç‡å¦‚ä½•ç¨³å®š
    ax1.plot(flip_counts, head_ratios, 'bo-', linewidth=2, markersize=8)
    ax1.axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='ç†è®ºæ¦‚ç‡ 50%')
    ax1.set_xlabel('æŠ›ç¡¬å¸æ¬¡æ•°')
    ax1.set_ylabel('æ­£é¢æœä¸Šçš„æ¯”ä¾‹')
    ax1.set_title('å¤§æ•°å®šå¾‹ï¼šæ¦‚ç‡å¦‚ä½•ç¨³å®šåˆ°ç†è®ºå€¼')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.set_xscale('log')

    # å³å›¾ï¼šæ¦‚ç‡åˆ†å¸ƒçš„ç›´è§‚å±•ç¤º
    outcomes = ['æ­£é¢', 'åé¢']
    probabilities = [0.5, 0.5]
    colors = ['gold', 'silver']

    bars = ax2.bar(outcomes, probabilities, color=colors, alpha=0.7, edgecolor='black', linewidth=2)
    ax2.set_ylabel('æ¦‚ç‡')
    ax2.set_title('æŠ›ç¡¬å¸çš„æ¦‚ç‡åˆ†å¸ƒ')
    ax2.set_ylim(0, 0.6)

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ¦‚ç‡å€¼
    for bar, prob in zip(bars, probabilities):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                 f'{prob * 100:.0f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.show()


# è¿è¡Œæ‰€æœ‰æ¼”ç¤º
print("ğŸ¯ å¼€å§‹æ¦‚ç‡è®ºåŸºç¡€æ¼”ç¤ºï¼")
coin_flip_simulation()


# 2. ğŸŒ§ï¸ æ¡ä»¶æ¦‚ç‡æ¼”ç¤ºï¼šå¤©æ°”é¢„æµ‹
def weather_prediction_demo():
    """æ¼”ç¤ºæ¡ä»¶æ¦‚ç‡åœ¨å¤©æ°”é¢„æµ‹ä¸­çš„åº”ç”¨"""
    print("\nğŸŒ§ï¸ æ¡ä»¶æ¦‚ç‡æ¼”ç¤ºï¼šçœ‹äº‘è¯†å¤©æ°”")
    print("=" * 40)

    # è®¾å®šä¸åŒäº‘ç±»å‹ä¸‹çš„ä¸‹é›¨æ¦‚ç‡
    cloud_types = ['ä¹Œäº‘', 'ç™½äº‘', 'æ™´ç©º']
    rain_probs = [0.8, 0.2, 0.05]
    cloud_colors = ['darkgray', 'lightgray', 'skyblue']

    # å¯è§†åŒ–æ¡ä»¶æ¦‚ç‡
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # å·¦å›¾ï¼šä¸åŒäº‘ç±»å‹çš„ä¸‹é›¨æ¦‚ç‡
    bars = ax1.bar(cloud_types, rain_probs, color=cloud_colors, alpha=0.7, edgecolor='black')
    ax1.set_ylabel('ä¸‹é›¨æ¦‚ç‡')
    ax1.set_title('æ¡ä»¶æ¦‚ç‡ï¼šä¸åŒäº‘ç±»å‹ä¸‹çš„ä¸‹é›¨æ¦‚ç‡')
    ax1.set_ylim(0, 1)

    # æ˜¾ç¤ºæ¦‚ç‡å€¼
    for bar, prob in zip(bars, rain_probs):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.02,
                 f'{prob * 100:.0f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')

    # å³å›¾ï¼šè´å¶æ–¯æ¨ç†ç¤ºä¾‹
    ax2.text(0.1, 0.9, 'ğŸ•µï¸ è´å¶æ–¯æ¨ç†ç¤ºä¾‹', fontsize=16, fontweight='bold')
    ax2.text(0.1, 0.8, 'é—®é¢˜ï¼šå¦‚æœä¸‹é›¨äº†ï¼Œæœ€å¯èƒ½æ˜¯ä»€ä¹ˆäº‘ï¼Ÿ', fontsize=12)

    # ç®€åŒ–çš„è´å¶æ–¯è®¡ç®—
    cloud_prior = [0.3, 0.5, 0.2]  # å„ç§äº‘çš„å…ˆéªŒæ¦‚ç‡

    ax2.text(0.1, 0.65, 'å…ˆéªŒæ¦‚ç‡ï¼ˆå¹³æ—¶å„ç§äº‘å‡ºç°çš„æ¦‚ç‡ï¼‰ï¼š', fontsize=11, fontweight='bold')
    for i, (cloud, prior) in enumerate(zip(cloud_types, cloud_prior)):
        ax2.text(0.15, 0.6 - i * 0.05, f'{cloud}: {prior * 100:.0f}%', fontsize=10)

    # è®¡ç®—åéªŒæ¦‚ç‡ï¼ˆä¸‹é›¨æ—¶æ˜¯å„ç§äº‘çš„æ¦‚ç‡ï¼‰
    evidence = sum(p_rain * p_cloud for p_rain, p_cloud in zip(rain_probs, cloud_prior))
    # evidence = 0.8Ã—0.3 + 0.2Ã—0.5 + 0.05Ã—0.2 = 0.35
    posteriors = [(p_rain * p_cloud) / evidence for p_rain, p_cloud in zip(rain_probs, cloud_prior)]
    # posteriors = [0.8Ã—0.3/0.35, 0.2Ã—0.5/0.35, 0.05Ã—0.2/0.35] = [0.686ï¼Œ 0.286ï¼Œ 0.029]

    ax2.text(0.1, 0.35, 'åéªŒæ¦‚ç‡ï¼ˆä¸‹é›¨æ—¶æ˜¯å„ç§äº‘çš„æ¦‚ç‡ï¼‰ï¼š', fontsize=11, fontweight='bold')
    for i, (cloud, posterior) in enumerate(zip(cloud_types, posteriors)):
        ax2.text(0.15, 0.3 - i * 0.05, f'{cloud}: {posterior * 100:.1f}%', fontsize=10)

    best_cloud = cloud_types[np.argmax(posteriors)]
    ax2.text(0.1, 0.1, f'ğŸ¯ ç»“è®ºï¼šä¸‹é›¨æ—¶æœ€å¯èƒ½æ˜¯{best_cloud}ï¼',
             fontsize=12, fontweight='bold', color='red',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.8))

    ax2.set_xlim(0, 1)
    ax2.set_ylim(0, 1)
    ax2.axis('off')

    plt.tight_layout()
    plt.show()

weather_prediction_demo()


# 3. ğŸ’¬ è¯­è¨€ä¸­çš„æ¦‚ç‡æ¼”ç¤º
def language_probability_demo():
    """æ¼”ç¤ºè¯­è¨€ä¸­çš„æ¦‚ç‡æ¦‚å¿µ"""
    print("\nğŸ’¬ è¯­è¨€ä¸­çš„æ¦‚ç‡ï¼š'æ¥è¯'æ¸¸æˆ")
    print("=" * 40)

    # æ¨¡æ‹Ÿç®€å•çš„è¯­è¨€æ¦‚ç‡
    context_words = {
        "æˆ‘ä»Šå¤©å¿ƒæƒ…": {"å¾ˆå¥½": 0.4, "ä¸é”™": 0.3, "ä¸€èˆ¬": 0.2, "ä¸å¥½": 0.1},
        "ä»Šå¤©å¤©æ°”": {"å¾ˆå¥½": 0.5, "ä¸é”™": 0.3, "ä¸€èˆ¬": 0.15, "å¾ˆå·®": 0.05},
        "è¿™é“èœ": {"å¾ˆé¦™": 0.4, "ä¸é”™": 0.35, "ä¸€èˆ¬": 0.2, "éš¾åƒ": 0.05}
    }

    # å¯è§†åŒ–è¯­è¨€æ¦‚ç‡åˆ†å¸ƒ
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    for i, (context, words_probs) in enumerate(context_words.items()):
        words = list(words_probs.keys())
        probs = list(words_probs.values())

        bars = axes[i].bar(words, probs, alpha=0.7,
                           color=['green', 'lightgreen', 'orange', 'red'])
        axes[i].set_title(f'"{context}..."\nä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒ')
        axes[i].set_ylabel('æ¦‚ç‡')
        axes[i].tick_params(axis='x', rotation=45)

        # æ˜¾ç¤ºæ¦‚ç‡å€¼
        for bar, prob in zip(bars, probs):
            height = bar.get_height()
            axes[i].text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                         f'{prob * 100:.0f}%', ha='center', va='bottom', fontsize=10)

    plt.tight_layout()
    plt.show()

    # äº’åŠ¨å¼æ¼”ç¤º
    print("\nğŸ® äº’åŠ¨æ¼”ç¤ºï¼š")
    for context, words_probs in context_words.items():
        print(f"\nå½“æˆ‘è¯´ï¼š'{context}...'")
        print("ä½ è§‰å¾—ä¸‹ä¸€ä¸ªè¯æœ€å¯èƒ½æ˜¯ä»€ä¹ˆï¼Ÿ")

        # æŒ‰æ¦‚ç‡æ’åº
        sorted_words = sorted(words_probs.items(), key=lambda x: x[1], reverse=True)
        for rank, (word, prob) in enumerate(sorted_words, 1):
            if rank == 1:
                print(f"  ğŸ¥‡ ç¬¬{rank}å: '{word}' ({prob * 100:.0f}% æ¦‚ç‡) â† æœ€å¯èƒ½ï¼")
            else:
                print(f"  ğŸ“ ç¬¬{rank}å: '{word}' ({prob * 100:.0f}% æ¦‚ç‡)")

language_probability_demo()

print("\nâœ¨ æ¦‚ç‡è®ºåŸºç¡€æ¼”ç¤ºå®Œæˆï¼")
print("ç°åœ¨ä½ åº”è¯¥å¯¹æ¦‚ç‡æœ‰äº†ç›´è§‚çš„ç†è§£ï¼š")
print("â€¢ ğŸ“Š æ¦‚ç‡å°±æ˜¯è¡¡é‡'å¯èƒ½æ€§'çš„æ•°å­—")
print("â€¢ ğŸ”— æ¡ä»¶æ¦‚ç‡å‘Šè¯‰æˆ‘ä»¬ä¸Šä¸‹æ–‡å¦‚ä½•å½±å“ç»“æœ")
print("â€¢ ğŸ§  è´å¶æ–¯æ€ç»´å¸®æˆ‘ä»¬æ ¹æ®æ–°ä¿¡æ¯æ›´æ–°åˆ¤æ–­")
print("â€¢ ğŸ’¬ è¯­è¨€æ¨¡å‹å°±æ˜¯åœ¨è®¡ç®—è¯è¯­å‡ºç°çš„æ¦‚ç‡ï¼")


# ğŸ¯ è®©æˆ‘ä»¬ç”¨å›¾ç‰‡æ¥çœ‹çœ‹"æ¥è¯"æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# å·¦å›¾ï¼šæ¨¡æ‹Ÿå¤§è„‘"æ¥è¯"çš„è¿‡ç¨‹
ax1.text(0.5, 0.95, 'ğŸ§  å¤§è„‘å¦‚ä½•"æ¥è¯"', ha='center', fontsize=16, fontweight='bold', color='darkblue')

# è¾“å…¥éƒ¨åˆ†
ax1.add_patch(plt.Rectangle((0.1, 0.7), 0.3, 0.15, facecolor='lightblue', edgecolor='blue', linewidth=2))
ax1.text(0.25, 0.775, 'è¾“å…¥:\n"æˆ‘çˆ±"', ha='center', va='center', fontsize=12, fontweight='bold')

# ç®­å¤´
ax1.annotate('', xy=(0.6, 0.775), xytext=(0.4, 0.775),
            arrowprops=dict(arrowstyle='->', lw=3, color='orange'))
ax1.text(0.5, 0.82, 'å¤§è„‘æ€è€ƒ', ha='center', fontsize=10, color='orange')

# é¢„æµ‹ç»“æœ
predictions = [
    ('ğŸ‡¨ğŸ‡³ ä¸­å›½', 60, 'red'),
    ('ğŸ“š å­¦ä¹ ', 30, 'blue'),
    ('ğŸ’» ç¼–ç¨‹', 10, 'green')
]

y_positions = [0.6, 0.4, 0.2]
for i, (word, prob, color) in enumerate(predictions):
    width = prob / 100 * 0.25  # æŒ‰æ¦‚ç‡è°ƒæ•´å®½åº¦
    ax1.add_patch(plt.Rectangle((0.6, y_positions[i]), width, 0.08,
                               facecolor=color, alpha=0.7, edgecolor=color))
    ax1.text(0.6 + width + 0.02, y_positions[i] + 0.04, f'{word} {prob}%',
             va='center', fontsize=11, fontweight='bold')

ax1.set_xlim(0, 1)
ax1.set_ylim(0, 1)
ax1.axis('off')

# å³å›¾ï¼šä¸‰ç§"è®°å¿†"æ–¹å¼çš„æ¯”è¾ƒ
ax2.text(0.5, 0.95, 'ğŸ¯ ä¸‰ç§"è®°å¿†"æ–¹å¼', ha='center', fontsize=16, fontweight='bold', color='darkblue')

memory_types = [
    ('ğŸ² éšæœºçŒœæµ‹', 'å®Œå…¨é è¿æ°”', 'lightcoral', 0.8),
    ('ğŸ“Š ç»Ÿè®¡è§„å¾‹', 'çœ‹å†å²ç»éªŒ', 'lightgreen', 0.55),
    ('ğŸ§  æ·±åº¦å­¦ä¹ ', 'æ¨¡æ‹Ÿå¤§è„‘æ€è€ƒ', 'lightblue', 0.3)
]

for name, desc, color, y in memory_types:
    # ç»˜åˆ¶æ–¹æ¡†
    ax2.add_patch(plt.Rectangle((0.1, y-0.08), 0.8, 0.15,
                               facecolor=color, alpha=0.7, edgecolor='black'))
    ax2.text(0.15, y, name, fontsize=12, fontweight='bold', va='center')
    ax2.text(0.15, y-0.04, desc, fontsize=10, va='center', style='italic')

# æ·»åŠ é‡ç‚¹æ ‡è®°
ax2.text(0.75, 0.55, 'â† ä»Šå¤©å­¦è¿™ä¸ªï¼', fontsize=12, fontweight='bold',
         color='red', bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.8))

ax2.set_xlim(0, 1)
ax2.set_ylim(0, 1)
ax2.axis('off')

plt.tight_layout()
plt.show()

print("ğŸ‰ å¤ªå¥½äº†ï¼ç°åœ¨ä½ çŸ¥é“ä»€ä¹ˆæ˜¯è¯­è¨€æ¨¡å‹äº†ï¼")
print("ğŸ’¡ ç®€å•è¯´ï¼šè¯­è¨€æ¨¡å‹å°±æ˜¯æ•™è®¡ç®—æœºå­¦ä¼š'æ¥è¯'çš„æ–¹æ³•ï¼")


# ğŸ¯ è®©æˆ‘ä»¬ä¸€èµ·æ¥åšä¸ª"æ•°æ•°æ¸¸æˆ"ï¼
def demonstrate_simple_counting():
    """ç”¨æœ€ç®€å•çš„æ–¹å¼æ¼”ç¤ºè®¡ç®—æœºå¦‚ä½•å­¦ä¹ """

    print("ğŸª æ¬¢è¿æ¥åˆ°'æ•°æ•°è®­ç»ƒè¥'ï¼")
    print("æˆ‘ä»¬è¦æ•™è®¡ç®—æœºå­¦ä¼šç»Ÿè®¡è¯è¯­çš„è§„å¾‹")
    print()

    # å‡†å¤‡ç®€å•çš„è®­ç»ƒæ•°æ®
    training_sentences = [
        "æˆ‘çˆ±ä¸­å›½",
        "æˆ‘çˆ±å­¦ä¹ ",
        "æˆ‘å–œæ¬¢ä¸­å›½",
        "ä»–çˆ±å­¦ä¹ ",
        "å¥¹çˆ±ä¸­å›½"
    ]

    print("ğŸ“š è®¡ç®—æœºçš„'æ•™æ'ï¼ˆè®­ç»ƒæ•°æ®ï¼‰ï¼š")
    for i, sentence in enumerate(training_sentences, 1):
        print(f"   ç¬¬{i}è¯¾: {sentence}")
    print()

    # æ‰‹å·¥ç»Ÿè®¡ï¼Œè®©è¿‡ç¨‹æ›´ç›´è§‚
    print("ğŸ” ç°åœ¨è®©æˆ‘ä»¬åƒè®¡ç®—æœºä¸€æ ·'æ•°æ•°'ï¼š")
    print()

    # ç»Ÿè®¡æ¯ä¸ªè¯
    all_words = []
    for sentence in training_sentences:
        words = list(sentence)  # æŠŠå¥å­æ‹†æˆå­—
        all_words.extend(words)

    # ç»Ÿè®¡å•ä¸ªå­—å‡ºç°çš„æ¬¡æ•°
    word_counts = {}
    for word in all_words:
        word_counts[word] = word_counts.get(word, 0) + 1

    print("ğŸ“Š ç¬¬ä¸€æ­¥ï¼šæ•°æ•°æ¯ä¸ªå­—å‡ºç°äº†å¤šå°‘æ¬¡")
    for word, count in sorted(word_counts.items()):
        print(f"   '{word}' å‡ºç°äº† {count} æ¬¡")
    print()

    # ç»Ÿè®¡è¯å¯¹ï¼ˆ2-gramï¼‰
    print("ğŸ‘¥ ç¬¬äºŒæ­¥ï¼šæ•°æ•°å“ªä¸¤ä¸ªå­—ç»å¸¸åœ¨ä¸€èµ·")

    word_pairs = {}
    for sentence in training_sentences:
        words = list(sentence)
        for i in range(len(words) - 1):
            pair = (words[i], words[i + 1])
            word_pairs[pair] = word_pairs.get(pair, 0) + 1

    for pair, count in sorted(word_pairs.items()):
        print(f"   '{pair[0]}' â†’ '{pair[1]}' å‡ºç°äº† {count} æ¬¡")
    print()

    # è®¡ç®—æ¦‚ç‡
    print("ğŸ§® ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æ¦‚ç‡ï¼ˆåšé™¤æ³•ï¼‰")
    print("å¦‚æœçœ‹åˆ°'æˆ‘'ï¼Œä¸‹ä¸€ä¸ªå­—æ˜¯ä»€ä¹ˆçš„æ¦‚ç‡æœ€å¤§ï¼Ÿ")
    print()

    # æ‰¾å‡º"æˆ‘"åé¢è·Ÿçš„æ‰€æœ‰å­—
    me_followers = {}
    me_total = 0

    for (w1, w2), count in word_pairs.items():
        if w1 == 'æˆ‘':
            me_followers[w2] = count
            me_total += count

    print(f"   'æˆ‘' åé¢æ€»å…±æœ‰ {me_total} ä¸ªå­—")
    for follower, count in me_followers.items():
        probability = count / me_total * 100
        print(f"   'æˆ‘' â†’ '{follower}': {count}/{me_total} = {probability:.1f}%")

    print()
    print("ğŸ¯ ç»“è®ºï¼šçœ‹åˆ°'æˆ‘'å­—åï¼Œä¸‹ä¸€ä¸ªå­—æ˜¯'çˆ±'çš„æ¦‚ç‡æœ€é«˜ï¼")
    print()

    # é¢„æµ‹æµ‹è¯•
    print("ğŸ® ç°åœ¨æ¥æµ‹è¯•ä¸€ä¸‹ï¼š")
    test_input = "æˆ‘"
    print(f"è¾“å…¥ï¼š'{test_input}'")
    print("è®¡ç®—æœºçš„é¢„æµ‹ï¼š")

    # æ‰¾åˆ°æ¦‚ç‡æœ€é«˜çš„ä¸‹ä¸€ä¸ªå­—
    if test_input in [pair[0] for pair in word_pairs.keys()]:
        candidates = [(w2, count / me_total * 100) for (w1, w2), count in word_pairs.items() if w1 == test_input]
        candidates.sort(key=lambda x: x[1], reverse=True)

        for i, (word, prob) in enumerate(candidates[:3], 1):
            if i == 1:
                print(f"   ğŸ¥‡ ç¬¬{i}å: '{word}' (æ¦‚ç‡: {prob:.1f}%) â† æœ€æœ‰å¯èƒ½ï¼")
            else:
                print(f"   ğŸ“ ç¬¬{i}å: '{word}' (æ¦‚ç‡: {prob:.1f}%)")

    print()
    print("âœ¨ å°±æ˜¯è¿™æ ·ï¼è®¡ç®—æœºé€šè¿‡'æ•°æ•°'å­¦ä¼šäº†é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ï¼")

demonstrate_simple_counting()


# ğŸ”§ å¹³æ»‘æŠ€æœ¯æ¼”ç¤º
def demonstrate_smoothing():
    """æ¼”ç¤ºä¸åŒå¹³æ»‘æŠ€æœ¯çš„æ•ˆæœ"""

    # ç®€å•çš„è®­ç»ƒæ•°æ®
    sentences = ["æˆ‘ çˆ± å­¦ä¹ ", "æˆ‘ çˆ± ç¼–ç¨‹", "ä»– å–œæ¬¢ å­¦ä¹ "]

    # ç»Ÿè®¡è®¡æ•°
    bigram_counts = defaultdict(int)
    unigram_counts = defaultdict(int)
    vocabulary = set()

    for sentence in sentences:
        words = sentence.split()
        words = ['<s>'] + words + ['</s>']

        for word in words:
            vocabulary.add(word)
            unigram_counts[word] += 1

        for i in range(1, len(words)):
            bigram_counts[(words[i - 1], words[i])] += 1

    V = len(vocabulary)  # è¯æ±‡è¡¨å¤§å°

    print("ğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡ï¼š")
    print(f"è¯æ±‡è¡¨å¤§å°: {V}")
    print(f"è¯æ±‡è¡¨: {sorted(vocabulary)}")

    # æµ‹è¯•æœªè§è¿‡çš„è¯å¯¹
    test_bigram = ("æˆ‘", "è®¨åŒ")

    print(f"\nğŸ§ª æµ‹è¯•è¯å¯¹: {test_bigram}")

    # 1. åŸå§‹æœ€å¤§ä¼¼ç„¶ä¼°è®¡
    if test_bigram in bigram_counts:
        mle_prob = bigram_counts[test_bigram] / unigram_counts[test_bigram[0]]
    else:
        mle_prob = 0
    print(f"æœ€å¤§ä¼¼ç„¶ä¼°è®¡: P(è®¨åŒ|æˆ‘) = {mle_prob}")

    # 2. æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
    laplace_prob = (bigram_counts[test_bigram] + 1) / (unigram_counts[test_bigram[0]] + V)
    print(
        f"æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘: P(è®¨åŒ|æˆ‘) = ({bigram_counts[test_bigram]} + 1) / ({unigram_counts[test_bigram[0]]} + {V}) = {laplace_prob:.4f}")

    # 3. Add-kå¹³æ»‘ (k=0.5)
    k = 0.5
    add_k_prob = (bigram_counts[test_bigram] + k) / (unigram_counts[test_bigram[0]] + k * V)
    print(
        f"Add-kå¹³æ»‘(k=0.5): P(è®¨åŒ|æˆ‘) = ({bigram_counts[test_bigram]} + {k}) / ({unigram_counts[test_bigram[0]]} + {k * V}) = {add_k_prob:.4f}")

    # å¯è§†åŒ–ä¸åŒå¹³æ»‘æ–¹æ³•çš„æ¦‚ç‡åˆ†å¸ƒ
    fig, ax = plt.subplots(figsize=(12, 6))

    methods = ['åŸå§‹MLE', 'æ‹‰æ™®æ‹‰æ–¯', 'Add-k(0.5)']
    probabilities = [mle_prob, laplace_prob, add_k_prob]
    colors = ['red', 'blue', 'green']

    bars = ax.bar(methods, probabilities, color=colors, alpha=0.7)
    ax.set_ylabel('æ¦‚ç‡')
    ax.set_title('ä¸åŒå¹³æ»‘æ–¹æ³•çš„æ¦‚ç‡å¯¹æ¯”\n(æµ‹è¯•è¯å¯¹: "æˆ‘" â†’ "è®¨åŒ")')

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, prob in zip(bars, probabilities):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2., height + 0.0001,
                f'{prob:.4f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

demonstrate_smoothing()


# ğŸ¯ å›°æƒ‘åº¦å…¬å¼å¯è§†åŒ–æ¼”ç¤º
def visualize_perplexity_formula():
    """å¯è§†åŒ–å›°æƒ‘åº¦å…¬å¼çš„è®¡ç®—è¿‡ç¨‹"""

    print("ğŸ¯ å›°æƒ‘åº¦å…¬å¼å¯è§†åŒ–æ¼”ç¤º")
    print("=" * 50)

    # æ¨¡æ‹Ÿä¸¤ç§ä¸åŒçš„é¢„æµ‹åœºæ™¯
    scenarios = {
        "ç¡®å®šé¢„æµ‹": {
            "words": ["æˆ‘", "çˆ±", "ä¸­å›½"],
            "probs": [0.9, 0.8, 0.85],
            "color": "lightgreen"
        },
        "å›°æƒ‘é¢„æµ‹": {
            "words": ["æˆ‘", "çˆ±", "ä¸­å›½"],
            "probs": [0.1, 0.15, 0.2],
            "color": "lightcoral"
        }
    }

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    # 1. æ¦‚ç‡åˆ†å¸ƒæ¯”è¾ƒ
    x_pos = np.arange(len(scenarios["ç¡®å®šé¢„æµ‹"]["words"]))
    width = 0.35

    ax1.bar(x_pos - width / 2, scenarios["ç¡®å®šé¢„æµ‹"]["probs"], width,
            label='ç¡®å®šé¢„æµ‹', color=scenarios["ç¡®å®šé¢„æµ‹"]["color"], alpha=0.7)
    ax1.bar(x_pos + width / 2, scenarios["å›°æƒ‘é¢„æµ‹"]["probs"], width,
            label='å›°æƒ‘é¢„æµ‹', color=scenarios["å›°æƒ‘é¢„æµ‹"]["color"], alpha=0.7)

    ax1.set_xlabel('è¯è¯­')
    ax1.set_ylabel('é¢„æµ‹æ¦‚ç‡')
    ax1.set_title('ä¸åŒé¢„æµ‹åœºæ™¯çš„æ¦‚ç‡åˆ†å¸ƒ')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(scenarios["ç¡®å®šé¢„æµ‹"]["words"])
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. å¯¹æ•°æ¦‚ç‡è®¡ç®—
    log_probs_certain = [math.log2(p) for p in scenarios["ç¡®å®šé¢„æµ‹"]["probs"]]
    log_probs_confused = [math.log2(p) for p in scenarios["å›°æƒ‘é¢„æµ‹"]["probs"]]

    ax2.bar(x_pos - width / 2, log_probs_certain, width,
            label='ç¡®å®šé¢„æµ‹', color=scenarios["ç¡®å®šé¢„æµ‹"]["color"], alpha=0.7)
    ax2.bar(x_pos + width / 2, log_probs_confused, width,
            label='å›°æƒ‘é¢„æµ‹', color=scenarios["å›°æƒ‘é¢„æµ‹"]["color"], alpha=0.7)

    ax2.set_xlabel('è¯è¯­')
    ax2.set_ylabel('logâ‚‚(æ¦‚ç‡)')
    ax2.set_title('å¯¹æ•°æ¦‚ç‡æ¯”è¾ƒ')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(scenarios["ç¡®å®šé¢„æµ‹"]["words"])
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. å›°æƒ‘åº¦è®¡ç®—è¿‡ç¨‹
    def calculate_perplexity(probs):
        """è®¡ç®—å›°æƒ‘åº¦"""
        total_log_prob = sum(math.log2(p) for p in probs)
        avg_log_prob = total_log_prob / len(probs)
        return 2 ** (-avg_log_prob)

    pp_certain = calculate_perplexity(scenarios["ç¡®å®šé¢„æµ‹"]["probs"])
    pp_confused = calculate_perplexity(scenarios["å›°æƒ‘é¢„æµ‹"]["probs"])

    perplexities = [pp_certain, pp_confused]
    labels = ['ç¡®å®šé¢„æµ‹', 'å›°æƒ‘é¢„æµ‹']
    colors = [scenarios["ç¡®å®šé¢„æµ‹"]["color"], scenarios["å›°æƒ‘é¢„æµ‹"]["color"]]

    bars = ax3.bar(labels, perplexities, color=colors, alpha=0.7)
    ax3.set_ylabel('å›°æƒ‘åº¦')
    ax3.set_title('æœ€ç»ˆå›°æƒ‘åº¦æ¯”è¾ƒ')
    ax3.grid(True, alpha=0.3)

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, pp in zip(bars, perplexities):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width() / 2., height + 0.1,
                 f'{pp:.2f}', ha='center', va='bottom', fontsize=12, fontweight='bold')

    # 4. è®¡ç®—æ­¥éª¤è¯¦è§£
    ax4.text(0.05, 0.95, 'ğŸ“Š å›°æƒ‘åº¦è®¡ç®—æ­¥éª¤ï¼š', fontsize=14, fontweight='bold', transform=ax4.transAxes)

    # ç¡®å®šé¢„æµ‹çš„è®¡ç®—
    ax4.text(0.05, 0.85, 'ğŸŸ¢ ç¡®å®šé¢„æµ‹åœºæ™¯ï¼š', fontsize=12, fontweight='bold', color='green', transform=ax4.transAxes)
    total_log_certain = sum(log_probs_certain)
    avg_log_certain = total_log_certain / len(log_probs_certain)
    ax4.text(0.05, 0.80, f'â€¢ æ€»å¯¹æ•°æ¦‚ç‡: {total_log_certain:.3f}', fontsize=10, transform=ax4.transAxes)
    ax4.text(0.05, 0.75, f'â€¢ å¹³å‡å¯¹æ•°æ¦‚ç‡: {avg_log_certain:.3f}', fontsize=10, transform=ax4.transAxes)
    ax4.text(0.05, 0.70, f'â€¢ å›°æƒ‘åº¦: 2^(-{avg_log_certain:.3f}) = {pp_certain:.2f}', fontsize=10,
             transform=ax4.transAxes)

    # å›°æƒ‘é¢„æµ‹çš„è®¡ç®—
    ax4.text(0.05, 0.60, 'ğŸ”´ å›°æƒ‘é¢„æµ‹åœºæ™¯ï¼š', fontsize=12, fontweight='bold', color='red', transform=ax4.transAxes)
    total_log_confused = sum(log_probs_confused)
    avg_log_confused = total_log_confused / len(log_probs_confused)
    ax4.text(0.05, 0.55, f'â€¢ æ€»å¯¹æ•°æ¦‚ç‡: {total_log_confused:.3f}', fontsize=10, transform=ax4.transAxes)
    ax4.text(0.05, 0.50, f'â€¢ å¹³å‡å¯¹æ•°æ¦‚ç‡: {avg_log_confused:.3f}', fontsize=10, transform=ax4.transAxes)
    ax4.text(0.05, 0.45, f'â€¢ å›°æƒ‘åº¦: 2^(-{avg_log_confused:.3f}) = {pp_confused:.2f}', fontsize=10,
             transform=ax4.transAxes)

    # ç»“è®º
    ax4.text(0.05, 0.35, 'ğŸ¯ ç»“è®ºï¼š', fontsize=12, fontweight='bold', transform=ax4.transAxes)
    ax4.text(0.05, 0.30, f'â€¢ ç¡®å®šé¢„æµ‹å›°æƒ‘åº¦æ›´ä½ ({pp_certain:.2f})', fontsize=10, transform=ax4.transAxes)
    ax4.text(0.05, 0.25, f'â€¢ å›°æƒ‘é¢„æµ‹å›°æƒ‘åº¦æ›´é«˜ ({pp_confused:.2f})', fontsize=10, transform=ax4.transAxes)
    ax4.text(0.05, 0.20, 'â€¢ å›°æƒ‘åº¦è¶Šä½ï¼Œæ¨¡å‹è¶Šå¥½ï¼', fontsize=10, color='blue', fontweight='bold',
             transform=ax4.transAxes)

    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')

    plt.tight_layout()
    plt.show()

    # æ‰“å°è¯¦ç»†è®¡ç®—è¿‡ç¨‹
    print("\nğŸ” è¯¦ç»†è®¡ç®—è¿‡ç¨‹ï¼š")
    print(f"ç¡®å®šé¢„æµ‹: æ¦‚ç‡ {scenarios['ç¡®å®šé¢„æµ‹']['probs']} â†’ å›°æƒ‘åº¦ {pp_certain:.2f}")
    print(f"å›°æƒ‘é¢„æµ‹: æ¦‚ç‡ {scenarios['å›°æƒ‘é¢„æµ‹']['probs']} â†’ å›°æƒ‘åº¦ {pp_confused:.2f}")
    print(f"\nğŸ’¡ è§£é‡Šï¼šå›°æƒ‘åº¦ {pp_certain:.2f} æ„å‘³ç€æ¨¡å‹å¹³å‡åœ¨ {pp_certain:.0f} ä¸ªé€‰æ‹©ä¸­çº ç»“")
    print(f"      å›°æƒ‘åº¦ {pp_confused:.2f} æ„å‘³ç€æ¨¡å‹å¹³å‡åœ¨ {pp_confused:.0f} ä¸ªé€‰æ‹©ä¸­çº ç»“")

visualize_perplexity_formula()


# ğŸ“ˆ å›°æƒ‘åº¦è®¡ç®—æ¼”ç¤º
def calculate_perplexity_demo():
    """æ¼”ç¤ºå›°æƒ‘åº¦çš„è®¡ç®—è¿‡ç¨‹"""

    # è®­ç»ƒæ•°æ®
    train_sentences = [
        "æˆ‘ çˆ± å­¦ä¹  ç¼–ç¨‹",
        "æˆ‘ å–œæ¬¢ å­¦ä¹  æ•°å­¦",
        "ä»– çˆ± ç¼–ç¨‹ è¯­è¨€",
        "å¥¹ å–œæ¬¢ æ•°å­¦ å…¬å¼"
    ]

    # æµ‹è¯•æ•°æ®
    test_sentences = [
        "æˆ‘ çˆ± æ•°å­¦",
        "å¥¹ å–œæ¬¢ ç¼–ç¨‹"
    ]

    print("ğŸ‹ï¸ è®­ç»ƒæ•°æ®ï¼š")
    for sentence in train_sentences:
        print(f"  {sentence}")

    print("\nğŸ§ª æµ‹è¯•æ•°æ®ï¼š")
    for sentence in test_sentences:
        print(f"  {sentence}")

    # æ„å»ºBigramæ¨¡å‹
    bigram_counts = defaultdict(int)
    unigram_counts = defaultdict(int)

    # è®­ç»ƒ
    for sentence in train_sentences:
        words = ['<s>'] + sentence.split() + ['</s>']
        for i in range(len(words)):
            unigram_counts[words[i]] += 1
            if i > 0:
                bigram_counts[(words[i - 1], words[i])] += 1

    print("\nğŸ“Š æ¨¡å‹ç»Ÿè®¡ï¼š")
    print(f"æ€»è¯æ•°: {sum(unigram_counts.values())}")
    print(f"ä¸åŒè¯æ•°: {len(unigram_counts)}")
    print(f"ä¸åŒbigramæ•°: {len(bigram_counts)}")

    # è®¡ç®—å›°æƒ‘åº¦
    def calculate_sentence_probability(sentence, smoothing=True):
        """è®¡ç®—å¥å­æ¦‚ç‡"""
        words = ['<s>'] + sentence.split() + ['</s>']
        log_prob = 0.0

        for i in range(1, len(words)):
            w1, w2 = words[i - 1], words[i]

            if smoothing:  # ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
                V = len(unigram_counts)
                prob = (bigram_counts[(w1, w2)] + 1) / (unigram_counts[w1] + V)
            else:  # åŸå§‹MLE
                if unigram_counts[w1] > 0:
                    prob = bigram_counts[(w1, w2)] / unigram_counts[w1]
                else:
                    prob = 1e-10  # é¿å…log(0)

            log_prob += math.log2(prob)

        return log_prob, len(words) - 1  # å‡1å› ä¸ºä¸ç®—<s>

    # è®¡ç®—æ¯ä¸ªæµ‹è¯•å¥å­çš„å›°æƒ‘åº¦
    total_log_prob = 0
    total_words = 0

    print("\nğŸ” è¯¦ç»†è®¡ç®—è¿‡ç¨‹ï¼š")
    for sentence in test_sentences:
        log_prob, num_words = calculate_sentence_probability(sentence)
        sentence_perplexity = 2 ** (-log_prob / num_words)

        print(f"\nå¥å­: \"{sentence}\"")
        print(f"  å¯¹æ•°æ¦‚ç‡: {log_prob:.4f}")
        print(f"  è¯æ•°: {num_words}")
        print(f"  å›°æƒ‘åº¦: 2^(-{log_prob:.4f}/{num_words}) = {sentence_perplexity:.2f}")

        total_log_prob += log_prob
        total_words += num_words

    # æ€»ä½“å›°æƒ‘åº¦
    overall_perplexity = 2 ** (-total_log_prob / total_words)
    cross_entropy = -total_log_prob / total_words

    print(f"\nğŸ“Š æ€»ä½“è¯„ä¼°ç»“æœï¼š")
    print(f"  æ€»å¯¹æ•°æ¦‚ç‡: {total_log_prob:.4f}")
    print(f"  æ€»è¯æ•°: {total_words}")
    print(f"  äº¤å‰ç†µ: {cross_entropy:.4f}")
    print(f"  å›°æƒ‘åº¦: {overall_perplexity:.2f}")

    # å¯è§†åŒ–å›°æƒ‘åº¦
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # å·¦å›¾ï¼šæ¯ä¸ªå¥å­çš„å›°æƒ‘åº¦
    sentence_perplexities = []
    for sentence in test_sentences:
        log_prob, num_words = calculate_sentence_probability(sentence)
        pp = 2 ** (-log_prob / num_words)
        sentence_perplexities.append(pp)

    ax1.bar(range(len(test_sentences)), sentence_perplexities,
            color=['skyblue', 'lightcoral'])
    ax1.set_xlabel('æµ‹è¯•å¥å­')
    ax1.set_ylabel('å›°æƒ‘åº¦')
    ax1.set_title('å„å¥å­å›°æƒ‘åº¦')
    ax1.set_xticks(range(len(test_sentences)))
    ax1.set_xticklabels([f'å¥å­{i + 1}' for i in range(len(test_sentences))])

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for i, pp in enumerate(sentence_perplexities):
        ax1.text(i, pp + 0.5, f'{pp:.1f}', ha='center', va='bottom')

    # å³å›¾ï¼šå›°æƒ‘åº¦è§£é‡Š
    ax2.text(0.1, 0.8, 'å›°æƒ‘åº¦è§£é‡Š:', fontsize=14, fontweight='bold')
    ax2.text(0.1, 0.6, f'â€¢ æ€»ä½“å›°æƒ‘åº¦: {overall_perplexity:.1f}', fontsize=12)
    ax2.text(0.1, 0.5, f'â€¢ æ¨¡å‹å¹³å‡åœ¨ {overall_perplexity:.0f} ä¸ªè¯ä¸­é€‰æ‹©', fontsize=11)
    ax2.text(0.1, 0.3, 'â€¢ å›°æƒ‘åº¦è¶Šä½ï¼Œæ¨¡å‹é¢„æµ‹è¶Šå‡†ç¡®', fontsize=11)
    ax2.text(0.1, 0.2, 'â€¢ ç†æƒ³æƒ…å†µï¼šå›°æƒ‘åº¦ = 1', fontsize=11)
    ax2.set_xlim(0, 1)
    ax2.set_ylim(0, 1)
    ax2.axis('off')

    plt.tight_layout()
    plt.show()

    return overall_perplexity

perplexity = calculate_perplexity_demo()
print(f"\nâœ… å›°æƒ‘åº¦è®¡ç®—å®Œæˆï¼æœ€ç»ˆå›°æƒ‘åº¦: {perplexity:.2f}")


class NgramLanguageModel:
    """N-gramè¯­è¨€æ¨¡å‹å®ç°"""

    def __init__(self, n=2, smoothing='laplace', k=1.0):
        """
        åˆå§‹åŒ–N-gramè¯­è¨€æ¨¡å‹

        å‚æ•°ï¼š
        n: N-gramçš„é˜¶æ•° (1=unigram, 2=bigram, 3=trigram, ...)
        smoothing: å¹³æ»‘æ–¹æ³• ('laplace', 'add_k', 'interpolation')
        k: å¹³æ»‘å‚æ•°
        """
        self.n = n
        self.smoothing = smoothing
        self.k = k
        self.ngram_counts = defaultdict(int)
        self.context_counts = defaultdict(int)
        self.vocabulary = set()
        self.total_words = 0

    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        # ç®€å•çš„åˆ†è¯å’Œæ¸…ç†
        text = re.sub(r'[^\w\s]', '', text.lower())
        words = text.split()
        return words

    def get_ngrams(self, words):
        """è·å–N-gramåºåˆ—"""
        # æ·»åŠ å¼€å§‹å’Œç»“æŸæ ‡è®°
        padded_words = ['<s>'] * (self.n - 1) + words + ['</s>']

        ngrams = []
        for i in range(len(padded_words) - self.n + 1):
            ngram = tuple(padded_words[i:i + self.n])
            ngrams.append(ngram)

        return ngrams

    def train(self, texts):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {self.n}-gram è¯­è¨€æ¨¡å‹...")

        for text in texts:
            words = self.preprocess_text(text)
            self.vocabulary.update(words)
            self.total_words += len(words)

            # è·å–N-gram
            ngrams = self.get_ngrams(words)

            for ngram in ngrams:
                self.ngram_counts[ngram] += 1
                # è®¡ç®—ä¸Šä¸‹æ–‡(å‰n-1ä¸ªè¯)çš„è®¡æ•°
                if self.n > 1:
                    context = ngram[:-1]
                    self.context_counts[context] += 1

        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.vocabulary)}")
        print(f"   æ€»è¯æ•°: {self.total_words}")
        print(f"   {self.n}-gramæ€»æ•°: {len(self.ngram_counts)}")

    def get_probability(self, ngram):
        """è®¡ç®—N-gramæ¦‚ç‡"""
        if self.n == 1:
            # Unigramæ¦‚ç‡
            if self.smoothing == 'laplace':
                return (self.ngram_counts[ngram] + self.k) / (self.total_words + self.k * len(self.vocabulary))
            else:
                return self.ngram_counts[ngram] / self.total_words if self.total_words > 0 else 0
        else:
            # N-gramæ¡ä»¶æ¦‚ç‡
            context = ngram[:-1]

            if self.smoothing == 'laplace':
                numerator = self.ngram_counts[ngram] + self.k
                denominator = self.context_counts[context] + self.k * len(self.vocabulary)
                return numerator / denominator if denominator > 0 else 0
            else:
                if self.context_counts[context] > 0:
                    return self.ngram_counts[ngram] / self.context_counts[context]
                else:
                    return 0

    def sentence_probability(self, sentence):
        """è®¡ç®—å¥å­æ¦‚ç‡"""
        words = self.preprocess_text(sentence)
        ngrams = self.get_ngrams(words)

        log_prob = 0.0
        for ngram in ngrams:
            prob = self.get_probability(ngram)
            if prob > 0:
                log_prob += math.log(prob)
            else:
                log_prob += math.log(1e-10)  # é¿å…log(0)

        return math.exp(log_prob)

    def perplexity(self, test_sentences):
        """è®¡ç®—å›°æƒ‘åº¦"""
        total_log_prob = 0.0
        total_words = 0

        for sentence in test_sentences:
            words = self.preprocess_text(sentence)
            ngrams = self.get_ngrams(words)

            for ngram in ngrams:
                prob = self.get_probability(ngram)
                if prob > 0:
                    total_log_prob += math.log2(prob)
                else:
                    total_log_prob += math.log2(1e-10)
                total_words += 1

        if total_words > 0:
            return 2 ** (-total_log_prob / total_words)
        else:
            return float('inf')

    def generate_text(self, start_words=None, max_length=20):
        """ç”Ÿæˆæ–‡æœ¬"""
        if start_words is None:
            start_words = ['<s>'] * (self.n - 1)

        words = start_words.copy()

        for _ in range(max_length):
            # è·å–å½“å‰ä¸Šä¸‹æ–‡
            if self.n == 1:
                context = tuple()
            else:
                context = tuple(words[-(self.n - 1):])

            # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯
            candidates = []
            for ngram, count in self.ngram_counts.items():
                if self.n == 1 or (len(ngram) == self.n and ngram[:-1] == context):
                    next_word = ngram[-1]
                    prob = self.get_probability(ngram)
                    candidates.append((next_word, prob))

            if not candidates:
                break

            # æ ¹æ®æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªè¯
            candidates.sort(key=lambda x: x[1], reverse=True)

            # ç®€å•çš„è´ªå¿ƒé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„è¯
            next_word = candidates[0][0]

            if next_word == '</s>':
                break

            words.append(next_word)

        # ç§»é™¤ç‰¹æ®Šæ ‡è®°
        generated = [w for w in words if w not in ['<s>', '</s>']]
        return ' '.join(generated)

print("ğŸ¯ N-gramè¯­è¨€æ¨¡å‹ç±»å®šä¹‰å®Œæˆï¼")


# ğŸ® å®æˆ˜æ¼”ç¤ºï¼šå®Œæ•´çš„è¯­è¨€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
def run_language_model_demo():
    """è¿è¡Œå®Œæ•´çš„è¯­è¨€æ¨¡å‹æ¼”ç¤º"""
    # å‡†å¤‡è®­ç»ƒæ•°æ® (æ¨¡æ‹Ÿä¸€äº›ç®€å•çš„ä¸­æ–‡å¥å­)
    train_texts = [
        "æˆ‘çˆ±å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
        "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥ç”¨äºæ–‡æœ¬åˆ†æ",
        "æ·±åº¦å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹ä¸­æœ‰é‡è¦åº”ç”¨",
        "æˆ‘å–œæ¬¢ç ”ç©¶æœºå™¨å­¦ä¹ ç®—æ³•",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•å¾ˆå¿«",
        "æ–‡æœ¬åˆ†æéœ€è¦ç”¨åˆ°ç»Ÿè®¡æ–¹æ³•",
        "è¯­è¨€æ¨¡å‹å¯ä»¥ç”¨äºæ–‡æœ¬ç”Ÿæˆ",
        "æˆ‘å¯¹æ·±åº¦å­¦ä¹ å¾ˆæ„Ÿå…´è¶£",
        "è‡ªç„¶è¯­è¨€ç†è§£æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜",
        "æœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒ",
        "äººå·¥æ™ºèƒ½åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰åº”ç”¨"
    ]

    # æµ‹è¯•æ•°æ®
    test_texts = [
        "æˆ‘çˆ±äººå·¥æ™ºèƒ½",
        "æ·±åº¦å­¦ä¹ å¾ˆæœ‰è¶£",
        "æœºå™¨å­¦ä¹ ç®—æ³•å¾ˆé‡è¦"
    ]

    print("ğŸ“š è®­ç»ƒæ•°æ®ç¤ºä¾‹ï¼š")
    for i, text in enumerate(train_texts[:3], 1):
        print(f"  {i}. {text}")
    print(f"  ... (å…±{len(train_texts)}ä¸ªå¥å­)")

    print("\nğŸ§ª æµ‹è¯•æ•°æ®ï¼š")
    for i, text in enumerate(test_texts, 1):
        print(f"  {i}. {text}")

    # æ¯”è¾ƒä¸åŒçš„N-gramæ¨¡å‹
    models = {}
    perplexities = {}

    for n in [1, 2, 3]:
        print(f"\n{'=' * 50}")
        print(f"ğŸ”„ è®­ç»ƒ {n}-gram æ¨¡å‹")
        print('=' * 50)

        model = NgramLanguageModel(n=n, smoothing='laplace', k=1.0)
        model.train(train_texts)
        models[n] = model

        # è®¡ç®—å›°æƒ‘åº¦
        perplexity = model.perplexity(test_texts)
        perplexities[n] = perplexity
        print(f"ğŸ“Š å›°æƒ‘åº¦: {perplexity:.2f}")

        # è®¡ç®—æ¯ä¸ªæµ‹è¯•å¥å­çš„æ¦‚ç‡
        print("ğŸ“ å¥å­æ¦‚ç‡ï¼š")
        for sentence in test_texts:
            prob = model.sentence_probability(sentence)
            print(f"  '{sentence}': {prob:.2e}")

    # å¯è§†åŒ–æ¯”è¾ƒç»“æœ
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

    # 1. å›°æƒ‘åº¦æ¯”è¾ƒ
    ns = list(perplexities.keys())
    pps = list(perplexities.values())
    bars1 = ax1.bar(ns, pps, color=['lightblue', 'lightgreen', 'lightcoral'])
    ax1.set_xlabel('N-gramé˜¶æ•°')
    ax1.set_ylabel('å›°æƒ‘åº¦')
    ax1.set_title('ä¸åŒN-gramæ¨¡å‹çš„å›°æƒ‘åº¦æ¯”è¾ƒ')
    ax1.set_xticks(ns)

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, pp in zip(bars1, pps):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.5,
                 f'{pp:.1f}', ha='center', va='bottom')

    # 2. è¯æ±‡è¡¨å¤§å°æ¯”è¾ƒ
    vocab_sizes = [len(models[n].vocabulary) for n in ns]
    bars2 = ax2.bar(ns, vocab_sizes, color=['skyblue', 'lightgreen', 'salmon'])
    ax2.set_xlabel('N-gramé˜¶æ•°')
    ax2.set_ylabel('è¯æ±‡è¡¨å¤§å°')
    ax2.set_title('è¯æ±‡è¡¨å¤§å°')
    ax2.set_xticks(ns)

    for bar, size in zip(bars2, vocab_sizes):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width() / 2., height + 0.5,
                 f'{size}', ha='center', va='bottom')

    # 3. N-gramæ•°é‡æ¯”è¾ƒ
    ngram_counts = [len(models[n].ngram_counts) for n in ns]
    bars3 = ax3.bar(ns, ngram_counts, color=['lightcyan', 'lightgreen', 'mistyrose'])
    ax3.set_xlabel('N-gramé˜¶æ•°')
    ax3.set_ylabel('N-gramæ€»æ•°')
    ax3.set_title('N-gramæ€»æ•°æ¯”è¾ƒ')
    ax3.set_xticks(ns)

    for bar, count in zip(bars3, ngram_counts):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width() / 2., height + 1,
                 f'{count}', ha='center', va='bottom')

    # 4. æ¨¡å‹æ€§èƒ½æ€»ç»“
    ax4.text(0.1, 0.8, 'æ¨¡å‹æ€§èƒ½æ€»ç»“:', fontsize=14, fontweight='bold')
    ax4.text(0.1, 0.6, f'â€¢ Unigramå›°æƒ‘åº¦: {perplexities[1]:.1f}', fontsize=12)
    ax4.text(0.1, 0.5, f'â€¢ Bigramå›°æƒ‘åº¦: {perplexities[2]:.1f}', fontsize=12)
    ax4.text(0.1, 0.4, f'â€¢ Trigramå›°æƒ‘åº¦: {perplexities[3]:.1f}', fontsize=12)

    best_model = min(perplexities.items(), key=lambda x: x[1])
    ax4.text(0.1, 0.2, f'ğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}-gram (å›°æƒ‘åº¦: {best_model[1]:.1f})',
             fontsize=12, fontweight='bold', color='red')

    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')

    plt.tight_layout()
    plt.show()

    return models


# è¿è¡Œæ¼”ç¤º
print("ğŸš€ å¼€å§‹è¯­è¨€æ¨¡å‹å®Œæ•´æ¼”ç¤º...")
trained_models = run_language_model_demo()


# ğŸ¨ æ–‡æœ¬ç”Ÿæˆæ¼”ç¤ºï¼ˆä¿®å¤ç‰ˆï¼‰
def demonstrate_text_generation():
    """æ¼”ç¤ºä¸åŒN-gramæ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›"""

    print("ğŸ¨ æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("=" * 50)

    # æ£€æŸ¥trained_modelsæ˜¯å¦å­˜åœ¨
    if 'trained_models' not in globals():
        print("âš ï¸ æ¨¡å‹å°šæœªè®­ç»ƒï¼Œæ­£åœ¨è®­ç»ƒæ¨¡å‹...")
        global trained_models
        trained_models = run_language_model_demo()

    # ä½¿ç”¨ä¹‹å‰è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
    for n in [1, 2, 3]:
        if n in trained_models:
            model = trained_models[n]

            print(f"\nğŸ“ {n}-gram æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š")

            # ç”Ÿæˆå‡ ä¸ªä¸åŒçš„æ–‡æœ¬
            for i in range(3):
                try:
                    if n == 1:
                        # Unigram æ¨¡å‹ï¼Œä»è¯æ±‡è¡¨ä¸­éšæœºå¼€å§‹
                        vocab_list = list(model.vocabulary)
                        if vocab_list:
                            import random
                            start_word = random.choice(vocab_list)
                            generated = model.generate_text_improved([start_word], max_length=8)
                        else:
                            generated = "æ— æ³•ç”Ÿæˆæ–‡æœ¬ï¼ˆè¯æ±‡è¡¨ä¸ºç©ºï¼‰"
                    else:
                        # ä½¿ç”¨ä¸åŒçš„èµ·å§‹è¯
                        start_options = [['æˆ‘'], ['æ·±åº¦'], ['æœºå™¨']]
                        if i < len(start_options):
                            start_words = start_options[i]
                        else:
                            start_words = ['æˆ‘']

                        generated = model.generate_text_improved(start_words, max_length=10)

                    print(f"  ç”Ÿæˆ {i + 1}: {generated}")
                except Exception as e:
                    print(f"  ç”Ÿæˆ {i + 1}: ç”Ÿæˆå¤±è´¥ - {str(e)}")
        else:
            print(f"\nâŒ {n}-gram æ¨¡å‹ä¸å­˜åœ¨")

    # æ¯”è¾ƒä¸åŒèµ·å§‹è¯çš„ç”Ÿæˆæ•ˆæœ
    print(f"\nğŸ” å›ºå®šèµ·å§‹è¯ 'æˆ‘' çš„ç”Ÿæˆæ•ˆæœæ¯”è¾ƒï¼š")
    print("-" * 40)

    for n in [2, 3]:  # Unigramä¸éœ€è¦èµ·å§‹è¯ä¸Šä¸‹æ–‡
        if n in trained_models:
            try:
                model = trained_models[n]
                generated = model.generate_text_improved(['æˆ‘'], max_length=8)
                print(f"{n}-gram: {generated}")
            except Exception as e:
                print(f"{n}-gram: ç”Ÿæˆå¤±è´¥ - {str(e)}")


# ä¸ºNgramLanguageModelç±»æ·»åŠ æ”¹è¿›çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•
def add_improved_generation_method():
    """ä¸ºæ¨¡å‹ç±»æ·»åŠ æ”¹è¿›çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•"""

    def generate_text_improved(self, start_words=None, max_length=20):
        """æ”¹è¿›çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•"""
        import random

        if start_words is None or len(start_words) == 0:
            # éšæœºé€‰æ‹©ä¸€ä¸ªèµ·å§‹è¯
            if self.vocabulary:
                start_words = [random.choice(list(self.vocabulary))]
            else:
                return "æ— æ³•ç”Ÿæˆï¼ˆè¯æ±‡è¡¨ä¸ºç©ºï¼‰"

        words = start_words.copy()

        for step in range(max_length):
            # è·å–å½“å‰ä¸Šä¸‹æ–‡
            if self.n == 1:
                context = tuple()
            else:
                context = tuple(words[-(self.n - 1):])

            # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯ï¼ˆæ’é™¤ç»“æŸç¬¦ï¼‰
            candidates = []
            for ngram, count in self.ngram_counts.items():
                if self.n == 1:
                    next_word = ngram[0]
                    if next_word != '</s>' and next_word != '<s>':
                        prob = self.get_probability(ngram)
                        candidates.append((next_word, prob))
                elif len(ngram) == self.n and ngram[:-1] == context:
                    next_word = ngram[-1]
                    if next_word != '</s>' and next_word != '<s>':
                        prob = self.get_probability(ngram)
                        candidates.append((next_word, prob))

            if not candidates:
                # å¦‚æœæ²¡æœ‰å€™é€‰è¯ï¼Œå°è¯•å›é€€ç­–ç•¥
                if self.n > 1 and len(context) > 0:
                    # å›é€€åˆ°æ›´çŸ­çš„ä¸Šä¸‹æ–‡
                    shorter_context = context[1:] if len(context) > 1 else tuple()
                    for ngram, count in self.ngram_counts.items():
                        if len(ngram) == self.n and ngram[:-2] == shorter_context:
                            next_word = ngram[-1]
                            if next_word != '</s>' and next_word != '<s>':
                                prob = self.get_probability(ngram)
                                candidates.append((next_word, prob))

                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å€™é€‰è¯ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªè¯æ±‡è¡¨ä¸­çš„è¯
                if not candidates and self.vocabulary:
                    vocab_words = [w for w in self.vocabulary if w not in ['<s>', '</s>']]
                    if vocab_words:
                        next_word = random.choice(vocab_words)
                        candidates.append((next_word, 0.01))

            if not candidates:
                break

            # ä½¿ç”¨æ¦‚ç‡åŠ æƒçš„éšæœºé€‰æ‹©ï¼Œè€Œä¸æ˜¯è´ªå¿ƒé€‰æ‹©
            candidates.sort(key=lambda x: x[1], reverse=True)

            # é€‰æ‹©å‰å‡ ä¸ªé«˜æ¦‚ç‡çš„å€™é€‰è¯è¿›è¡Œéšæœºé€‰æ‹©
            top_candidates = candidates[:min(3, len(candidates))]
            total_prob = sum(prob for _, prob in top_candidates)

            if total_prob > 0:
                # æŒ‰æ¦‚ç‡éšæœºé€‰æ‹©
                rand_val = random.random() * total_prob
                cumulative_prob = 0
                selected_word = top_candidates[0][0]  # é»˜è®¤é€‰æ‹©

                for word, prob in top_candidates:
                    cumulative_prob += prob
                    if rand_val <= cumulative_prob:
                        selected_word = word
                        break
            else:
                selected_word = top_candidates[0][0]

            words.append(selected_word)

        # ç§»é™¤ç‰¹æ®Šæ ‡è®°å¹¶è¿”å›ç»“æœ
        generated = [w for w in words if w not in ['<s>', '</s>']]
        return ' '.join(generated) if generated else "æ— æ³•ç”Ÿæˆæ–‡æœ¬"

    # å°†æ–¹æ³•æ·»åŠ åˆ°ç±»ä¸­
    NgramLanguageModel.generate_text_improved = generate_text_improved


# æ·»åŠ æ”¹è¿›çš„ç”Ÿæˆæ–¹æ³•
add_improved_generation_method()
print("âœ… å·²ä¸ºæ¨¡å‹æ·»åŠ æ”¹è¿›çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•")

# è¿è¡Œæ”¹è¿›åçš„æ¼”ç¤º
demonstrate_text_generation()


# ğŸ”¬ æ¨¡å‹åˆ†æä¸ä¼˜åŒ–å®éªŒ
def analyze_and_optimize_models():
    """åˆ†ææ¨¡å‹æ€§èƒ½å¹¶è¿›è¡Œä¼˜åŒ–å®éªŒ"""

    print("ğŸ”¬ è¯­è¨€æ¨¡å‹åˆ†æä¸ä¼˜åŒ–")
    print("=" * 50)

    # 1. å¹³æ»‘å‚æ•°å¯¹æ€§èƒ½çš„å½±å“
    print("\nğŸ“Š å®éªŒ1: å¹³æ»‘å‚æ•°å¯¹Bigramæ¨¡å‹çš„å½±å“")
    print("-" * 40)

    train_texts = [
        "æˆ‘çˆ±å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
        "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥ç”¨äºæ–‡æœ¬åˆ†æ",
        "æ·±åº¦å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹ä¸­æœ‰é‡è¦åº”ç”¨"
    ]

    test_texts = ["æˆ‘çˆ±äººå·¥æ™ºèƒ½", "æ·±åº¦å­¦ä¹ å¾ˆæœ‰è¶£"]

    k_values = [0.01, 0.1, 0.5, 1.0, 2.0]
    perplexities_by_k = []

    for k in k_values:
        model = NgramLanguageModel(n=2, smoothing='laplace', k=k)
        model.train(train_texts)
        pp = model.perplexity(test_texts)
        perplexities_by_k.append(pp)
        print(f"k={k}: å›°æƒ‘åº¦={pp:.2f}")

    # 2. è®­ç»ƒæ•°æ®å¤§å°å¯¹æ€§èƒ½çš„å½±å“
    print("\nğŸ“Š å®éªŒ2: è®­ç»ƒæ•°æ®å¤§å°å¯¹æ€§èƒ½çš„å½±å“")
    print("-" * 40)

    full_train_texts = [
        "æˆ‘çˆ±å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
        "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥ç”¨äºæ–‡æœ¬åˆ†æ",
        "æ·±åº¦å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹ä¸­æœ‰é‡è¦åº”ç”¨",
        "æˆ‘å–œæ¬¢ç ”ç©¶æœºå™¨å­¦ä¹ ç®—æ³•",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•å¾ˆå¿«",
        "æ–‡æœ¬åˆ†æéœ€è¦ç”¨åˆ°ç»Ÿè®¡æ–¹æ³•",
        "è¯­è¨€æ¨¡å‹å¯ä»¥ç”¨äºæ–‡æœ¬ç”Ÿæˆ"
    ]

    data_sizes = [2, 4, 6, 8]
    perplexities_by_size = []

    for size in data_sizes:
        model = NgramLanguageModel(n=2, smoothing='laplace', k=1.0)
        model.train(full_train_texts[:size])
        pp = model.perplexity(test_texts)
        perplexities_by_size.append(pp)
        print(f"è®­ç»ƒå¥å­æ•°={size}: å›°æƒ‘åº¦={pp:.2f}")

    # å¯è§†åŒ–å®éªŒç»“æœ
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))

    # å®éªŒ1ï¼šå¹³æ»‘å‚æ•°å½±å“
    ax1.plot(k_values, perplexities_by_k, 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('å¹³æ»‘å‚æ•° k')
    ax1.set_ylabel('å›°æƒ‘åº¦')
    ax1.set_title('å¹³æ»‘å‚æ•°å¯¹å›°æƒ‘åº¦çš„å½±å“')
    ax1.grid(True, alpha=0.3)

    # åœ¨å›¾ä¸Šæ ‡æ³¨æœ€ä½³å‚æ•°
    best_k_idx = np.argmin(perplexities_by_k)
    best_k = k_values[best_k_idx]
    best_pp = perplexities_by_k[best_k_idx]
    ax1.annotate(f'æœ€ä½³: k={best_k}\\nå›°æƒ‘åº¦={best_pp:.2f}',
                 xy=(best_k, best_pp),
                 xytext=(best_k + 0.3, best_pp + 5),
                 arrowprops=dict(arrowstyle='->', color='red'))

    # å®éªŒ2ï¼šè®­ç»ƒæ•°æ®å¤§å°å½±å“
    ax2.plot(data_sizes, perplexities_by_size, 'ro-', linewidth=2, markersize=8)
    ax2.set_xlabel('è®­ç»ƒå¥å­æ•°é‡')
    ax2.set_ylabel('å›°æƒ‘åº¦')
    ax2.set_title('è®­ç»ƒæ•°æ®å¤§å°å¯¹å›°æƒ‘åº¦çš„å½±å“')
    ax2.grid(True, alpha=0.3)

    # å®éªŒ3ï¼šæ¨¡å‹å¤æ‚åº¦ vs æ€§èƒ½
    if 'trained_models' in globals():
        ns = list(trained_models.keys())
        model_perplexities = []
        for n in ns:
            pp = trained_models[n].perplexity(["æˆ‘çˆ±äººå·¥æ™ºèƒ½", "æ·±åº¦å­¦ä¹ å¾ˆæœ‰è¶£"])
            model_perplexities.append(pp)

        ax3.bar(ns, model_perplexities, color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)
        ax3.set_xlabel('N-gram é˜¶æ•°')
        ax3.set_ylabel('å›°æƒ‘åº¦')
        ax3.set_title('æ¨¡å‹å¤æ‚åº¦ vs æ€§èƒ½')
        ax3.set_xticks(ns)

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
        for i, pp in enumerate(model_perplexities):
            ax3.text(ns[i], pp + 0.5, f'{pp:.1f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

    # 3. æ€§èƒ½åˆ†ææ€»ç»“
    print("\\nğŸ“ˆ æ¨¡å‹åˆ†ææ€»ç»“ï¼š")
    print(f"â€¢ æœ€ä½³å¹³æ»‘å‚æ•°: k={best_k} (å›°æƒ‘åº¦: {best_pp:.2f})")
    print(f"â€¢ æ•°æ®é‡æ•ˆæœ: æ›´å¤šæ•°æ®é€šå¸¸å¸¦æ¥æ›´å¥½æ€§èƒ½")
    print(f"â€¢ æ¨¡å‹å¤æ‚åº¦: éœ€è¦åœ¨å¤æ‚åº¦å’Œæ³›åŒ–èƒ½åŠ›é—´å¹³è¡¡")

    return {
        'best_k': best_k,
        'k_perplexities': dict(zip(k_values, perplexities_by_k)),
        'size_perplexities': dict(zip(data_sizes, perplexities_by_size))
    }


# è¿è¡Œåˆ†æ
analysis_results = analyze_and_optimize_models()

