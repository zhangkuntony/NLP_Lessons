# å¯¼å…¥å¿…è¦çš„åº“
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import re

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)

print("âœ… ç¯å¢ƒé…ç½®å®Œæˆï¼")

# å®šä¹‰åªèƒ½è¯»å–å‡½æ•°
def smart_read_csv(file_path, sample_size=1000):
    """
    æ™ºèƒ½è¯»å–CSVæ–‡ä»¶ï¼Œè‡ªåŠ¨å°è¯•ä¸åŒç¼–ç 

    å‚æ•°:
        file_path: æ–‡ä»¶è·¯å¾„
        sample_size: æµ‹è¯•æ ·æœ¬å¤§å°

    è¿”å›:
        df_full: è¯»å–çš„å®Œæ•´æ•°æ®æ¡†
        encoding: æˆåŠŸçš„ç¼–ç æ ¼å¼
    """
    # å¸¸è§çš„ä¸­æ–‡ç¼–ç åˆ—è¡¨
    encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin1']

    for encoding in encodings:
        try:
            print(f"ğŸ” å°è¯•ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶...")

            # å…ˆè¯»å–æ ·æœ¬æµ‹è¯•ç¼–ç æ˜¯å¦æ­£ç¡®
            pd.read_csv(file_path, encoding=encoding, nrows=sample_size)
            print(f"âœ… æˆåŠŸï¼ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶")

            # æµ‹è¯•æˆåŠŸåè¯»å–å®Œæ•´æ–‡ä»¶
            df_full = pd.read_csv(file_path, encoding=encoding)
            return df_full, encoding

        except Exception as ex:
            print(f"âŒ {encoding} ç¼–ç å¤±è´¥: {str(ex)[:50]}...")
            continue

    print("ğŸ˜± æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥äº†ï¼")
    return None, None

print("ğŸ› ï¸ æ™ºèƒ½è¯»å–å‡½æ•°å®šä¹‰å®Œæˆï¼")

# ğŸ“– è¯»å–ç”µå½±æ•°æ®
print("ğŸ“– æ­£åœ¨è¯»å–ç”µå½±ä¿¡æ¯æ•°æ®...")

# ä½¿ç”¨åªèƒ½è¯»å–å‡½æ•°åŠ è½½ç”µå½±æ•°æ®
movies_df, movies_encoding = smart_read_csv('douban-dataset/movies.csv')

# æ£€æŸ¥è¯»å–ç»“æœ
if movies_df is not None:
    print(f"ğŸ¬ ç”µå½±æ•°æ®è¯»å–æˆåŠŸï¼å…±æœ‰ {len(movies_df)} éƒ¨ç”µå½±")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {movies_df.shape}")
    print(f"ğŸ”¤ ä½¿ç”¨ç¼–ç : {movies_encoding}")
else:
    print("ğŸ’” ç”µå½±æ•°æ®è¯»å–å¤±è´¥ï¼")

# ğŸ’¬ è¯»å–è¯„è®ºæ•°æ®ï¼ˆåˆ†æ‰¹å¤„ç†å¤§æ–‡ä»¶ï¼‰
print("ğŸ’¬ æ­£åœ¨è¯»å–è¯„è®ºæ•°æ®...")
print("âš ï¸  ç”±äºè¯„è®ºæ–‡ä»¶è¾ƒå¤§(68MB)ï¼Œæˆ‘ä»¬å…ˆè¯»å–å‰10000æ¡è¿›è¡Œæ¢ç´¢")

# ä½¿ç”¨åªèƒ½è¯»å–å‡½æ•°å¤„ç†è¯„è®ºæ•°æ®
try:
    # æ–¹æ³•1ï¼šä½¿ç”¨åªèƒ½è¯»å–å‡½æ•°ï¼ˆæ¨èï¼‰
    comments_df, comment_encoding = smart_read_csv('douban-dataset/comments.csv', sample_size=1000)

    # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œåªå–å‰10000æ¡
    if comments_df is not None and len(comments_df) > 10000:
        comments_df = comments_df.head(10000)
        print(f"ğŸ“ ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œåªä¿ç•™å‰10000æ¡è¯„è®º")

    if comments_df is not None:
        print(f"âœ… è¯„è®ºæ•°æ®è¯»å–æˆåŠŸï¼è¯»å–äº† {len(comments_df)} æ¡è¯„è®º")
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {comments_df.shape}")
        print(f"ğŸ”¤ ä½¿ç”¨ç¼–ç : {comment_encoding}")

except Exception as e:
    print(f"ğŸ’” è¯„è®ºæ•°æ®è¯»å–å¤±è´¥: {e}")
    comments_df = None


# æ•°æ®è´¨é‡å…¨é¢è¯Šæ–­
if comments_df is not None:
    print("=== ğŸ“Š æ•°æ®è´¨é‡è¯Šæ–­æŠ¥å‘Š ===")

    # 1. åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“‹ æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š")
    print(f"â€¢ æ•°æ®è¡Œæ•°ï¼š{len(comments_df):,}")
    print(f"â€¢ æ•°æ®åˆ—æ•°ï¼š{len(comments_df.columns)}")
    print(f"â€¢ æ•°æ®å¤§å°ï¼š{comments_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

    # 2. ç¼ºå¤±å€¼æ£€æŸ¥
    print(f"\nğŸ•³ï¸ ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
    missing_stats = comments_df.isnull().sum()
    missing_percent = (missing_stats / len(comments_df) * 100).round(2)

    for col in comments_df.columns:
        if missing_stats[col] > 0:
            print(f"â€¢ {col}: {missing_stats[col]:,} ä¸ªç¼ºå¤±å€¼ ({missing_percent[col]}%)")
        else:
            print(f"â€¢ {col}: æ— ç¼ºå¤±å€¼ âœ…")

    # 3. é‡å¤å€¼æ£€æŸ¥
    duplicates = comments_df.duplicated().sum()
    print(f"\nğŸ”„ é‡å¤æ•°æ®ï¼š{duplicates:,} è¡Œ ({duplicates / len(comments_df) * 100:.2f}%)")

    # 4. æ•°æ®ç±»å‹æ£€æŸ¥
    print(f"\nğŸ·ï¸ æ•°æ®ç±»å‹ï¼š")
    for col in comments_df.columns:
        print(f"â€¢ {col}: {comments_df[col].dtype}")

    # 5. æ–‡æœ¬åˆ—è´¨é‡æ£€æŸ¥ï¼ˆCONTENTåˆ—ï¼‰
    if 'CONTENT' in comments_df.columns:
        print(f"\nğŸ“ è¯„è®ºæ–‡æœ¬è´¨é‡åˆ†æï¼š")
        content_lengths = comments_df['CONTENT'].astype(str).str.len()

        print(f"â€¢ å¹³å‡é•¿åº¦ï¼š{content_lengths.mean():.1f} å­—ç¬¦")
        print(f"â€¢ æœ€çŸ­è¯„è®ºï¼š{content_lengths.min()} å­—ç¬¦")
        print(f"â€¢ æœ€é•¿è¯„è®ºï¼š{content_lengths.max()} å­—ç¬¦")
        print(f"â€¢ ä¸­ä½æ•°é•¿åº¦ï¼š{content_lengths.median():.0f} å­—ç¬¦")

        # æ£€æŸ¥å¼‚å¸¸çŸ­çš„è¯„è®º
        very_short = (content_lengths <= 5).sum()
        print(f"â€¢ è¿‡çŸ­è¯„è®º(â‰¤5å­—ç¬¦)ï¼š{very_short} æ¡ ({very_short / len(comments_df) * 100:.2f}%)")

        # æ£€æŸ¥å¼‚å¸¸é•¿çš„è¯„è®º
        very_long = (content_lengths >= 500).sum()
        print(f"â€¢ è¿‡é•¿è¯„è®º(â‰¥500å­—ç¬¦)ï¼š{very_long} æ¡ ({very_long/len(comments_df)*100:.2f}%)")

    print(f"\nâœ… æ•°æ®è´¨é‡è¯Šæ–­å®Œæˆï¼å‘ç° {missing_stats.sum()} ä¸ªç¼ºå¤±å€¼ï¼Œ{duplicates} ä¸ªé‡å¤å€¼")
else:
    print("âŒ æ•°æ®æœªæˆåŠŸåŠ è½½ï¼Œè·³è¿‡è´¨é‡è¯Šæ–­")


# ç³»ç»ŸåŒ–ç¼ºå¤±å€¼å¤„ç†
comments_cleaned = comments_df.copy()
if comments_df is not None:
    print("=== ğŸ¯ ç¼ºå¤±å€¼å¤„ç†æ–¹æ¡ˆ ===")

    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    original_shape = comments_cleaned.shape
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶ï¼š{original_shape}")

    # è®¡ç®—ç¼ºå¤±å€¼æ¯”ä¾‹
    missing_ratios = comments_cleaned.isnull().sum() / len(comments_cleaned)

    print("\nğŸ“Š å„åˆ—ç¼ºå¤±å€¼æ¯”ä¾‹ï¼š")
    for col in comments_cleaned.columns:
        ratio = missing_ratios[col] * 100
        status = "ğŸ”´é«˜é£é™©" if ratio > 20 else "ğŸŸ¡ä¸­ç­‰" if ratio > 5 else "ğŸŸ¢ä½é£é™©"
        print(f"â€¢ {col}: {ratio:.2f}% {status}")

    # å¤„ç†ç­–ç•¥æ‰§è¡Œ
    print("\nğŸ”§ æ‰§è¡Œå¤„ç†ç­–ç•¥ï¼š")

    # 1. æ ¸å¿ƒå­—æ®µï¼šè¯„è®ºå†…å®¹ä¸èƒ½ä¸ºç©ºï¼Œç›´æ¥åˆ é™¤
    if 'CONTENT' in comments_cleaned.columns:
        before_count = len(comments_cleaned)
        comments_cleaned = comments_cleaned.dropna(subset=['CONTENT'])
        removed_count = before_count - len(comments_cleaned)
        if removed_count > 0:
            print(f"âœ… åˆ é™¤æ— è¯„è®ºå†…å®¹çš„è®°å½•ï¼š{removed_count} æ¡")

    # 2. ç”¨æˆ·IDå­—æ®µï¼šç”¨â€œæœªçŸ¥ç”¨æˆ·â€å¡«è¡¥
    if 'CREATOR' in comments_cleaned.columns:
        creator_missing = comments_cleaned['CREATOR'].isnull().sum()
        if creator_missing > 0:
            comments_cleaned['CREATOR'] = comments_cleaned['CREATOR'].fillna('æœªçŸ¥ç”¨æˆ·')
            print(f"âœ… ç”¨æˆ·åç¼ºå¤±å€¼å¡«è¡¥ï¼š{creator_missing} æ¡ â†’ 'æœªçŸ¥ç”¨æˆ·'")

    # 3. è¯„åˆ†å­—æ®µï¼šç”¨ä¸­ä½æ•°å¡«è¡¥ï¼ˆé¿å…å¼‚å¸¸å€¼å½±å“ï¼‰
    if 'RATING' in comments_cleaned.columns:
        rating_missing = comments_cleaned['RATING'].isnull().sum()
        if rating_missing > 0:
            # å°†è¯„åˆ†è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            comments_cleaned['RATING'] = pd.to_numeric(comments_cleaned['RATING'], errors='coerce')
            median_rating = comments_cleaned['RATING'].median()
            comments_cleaned['RATING'] = comments_cleaned['RATING'].fillna(median_rating)
            print(f"âœ… è¯„åˆ†ç¼ºå¤±å€¼å¡«è¡¥ï¼š{rating_missing} æ¡ â†’ {median_rating}")

    # 4. å…¶ä»–å­—æ®µï¼šç”¨â€œæœªçŸ¥â€å¡«è¡¥
    text_columns = ['ID', 'TIME', 'MOVIEID', 'ADD_TIME']
    for col in text_columns:
        if col in comments_cleaned.columns:
            missing_count = comments_cleaned[col].isnull().sum()
            if missing_count > 0:
                comments_cleaned[col] = comments_cleaned[col].fillna('æœªçŸ¥')
                print(f"âœ… {col}ç¼ºå¤±å€¼å¡«è¡¥ï¼š{missing_count} æ¡ â†’ 'æœªçŸ¥'")

    # å¤„ç†ç»“æœç»Ÿè®¡
    final_shape = comments_cleaned.shape
    final_missing = comments_cleaned.isnull().sum().sum()

    print(f"\nğŸ“ˆ å¤„ç†ç»“æœï¼š")
    print(f"â€¢ å¤„ç†å‰ï¼š{original_shape[0]:,} è¡Œ")
    print(f"â€¢ å¤„ç†åï¼š{final_shape[0]:,} è¡Œ")
    print(f"â€¢ æ•°æ®ä¿ç•™ç‡ï¼š{final_shape[0] / original_shape[0] * 100:.1f}%")
    print(f"â€¢ å‰©ä½™ç¼ºå¤±å€¼ï¼š{final_missing} ä¸ª")

    if final_missing == 0:
        print("ğŸ‰ æ‰€æœ‰ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼")
    else:
        print(f"âš ï¸ ä»æœ‰ {final_missing} ä¸ªç¼ºå¤±å€¼éœ€è¦å¤„ç†")

else:
    print("âŒ æ•°æ®æœªåŠ è½½ï¼Œè·³è¿‡ç¼ºå¤±å€¼å¤„ç†")


# é‡å¤æ•°æ®å…¨é¢å¤„ç†
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== ğŸ”„ é‡å¤æ•°æ®æ£€æµ‹ä¸å¤„ç† ===")

    original_count = len(comments_cleaned)
    print(f"å¤„ç†å‰æ•°æ®é‡ï¼š{original_count:,} æ¡")

    # 1. å®Œå…¨é‡å¤æ£€æµ‹
    print(f"\nğŸ” æ£€æµ‹å®Œå…¨é‡å¤...")
    duplicate_all = comments_cleaned.duplicated()
    duplicate_count_all = duplicate_all.sum()

    print(f"â€¢ å®Œå…¨é‡å¤è®°å½•ï¼š{duplicate_count_all:,} æ¡ ({duplicate_count_all / original_count * 100:.2f}%)")

    if duplicate_count_all > 0:
        # åˆ é™¤å®Œå…¨é‡å¤
        comments_cleaned = comments_cleaned.drop_duplicates()
        print(f"âœ… å·²åˆ é™¤å®Œå…¨é‡å¤è®°å½•")

    # 2. å†…å®¹é‡å¤æ£€æµ‹ï¼ˆåŒä¸€ç”¨æˆ·å¯¹åŒä¸€ç”µå½±çš„é‡å¤è¯„è®ºï¼‰
    print(f"\nğŸ” æ£€æµ‹å†…å®¹é‡å¤...")
    if 'CREATOR' in comments_cleaned.columns and 'MOVIEID' in comments_cleaned.columns and 'CONTENT' in comments_cleaned.columns:

        # æ£€æµ‹åŒç”¨æˆ·åŒç”µå½±çš„é‡å¤è¯„è®º
        content_duplicates = comments_cleaned.duplicated(subset=['CREATOR', 'MOVIEID', 'CONTENT'])
        content_duplicate_count = content_duplicates.sum()

        print(f"â€¢ å†…å®¹é‡å¤è®°å½•ï¼š{content_duplicate_count:,} æ¡ ({content_duplicate_count / len(comments_cleaned) * 100:.2f}%)")

        if content_duplicate_count > 0:
            # åˆ é™¤å†…å®¹é‡å¤ï¼Œä¿ç•™æœ€åä¸€æ¡
            if 'ADD_TIME' in comments_cleaned.columns:
                # å¦‚æœæœ‰æ—¶é—´å­—æ®µï¼Œä¿ç•™æœ€æ–°çš„
                comments_cleaned = comments_cleaned.sort_values('ADD_TIME').drop_duplicates(
                    subset=['CREATOR', 'MOVIEID', 'CONTENT'], keep='last'
                )
                print(f"âœ… å·²åˆ é™¤å†…å®¹é‡å¤è®°å½•ï¼Œä¿ç•™æœ€æ–°çš„")
            else:
                # æ²¡æœ‰æ—¶é—´å­—æ®µï¼Œä¿ç•™ç¬¬ä¸€æ¡
                comments_cleaned = comments_cleaned.drop_duplicates(
                    subset=['CREATOR', 'MOVIEID', 'CONTENT'], keep='first'
                )
                print(f"âœ… å·²åˆ é™¤å†…å®¹é‡å¤è®°å½•ï¼Œä¿ç•™ç¬¬ä¸€æ¡")

    # 3. ç”¨æˆ·é‡å¤è¯„è®ºæ£€æµ‹ï¼ˆå¯é€‰å¤„ç†ï¼‰
    print(f"\nğŸ” æ£€æµ‹ç”¨æˆ·é‡å¤è¯„è®º...")
    if 'CREATOR' in comments_cleaned.columns and 'MOVIEID' in comments_cleaned.columns:

        # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·å¯¹æ¯éƒ¨ç”µå½±çš„è¯„è®ºæ•°
        user_movie_counts = comments_cleaned.groupby(['CREATOR', 'MOVIEID']).size()
        multiple_reviews = user_movie_counts[user_movie_counts > 1]

        total_multiple = multiple_reviews.sum()
        unique_user_movie = len(multiple_reviews)

        print(f"â€¢ é‡å¤è¯„è®ºçš„ç”¨æˆ·-ç”µå½±ç»„åˆï¼š{unique_user_movie:,} ç»„")
        print(f"â€¢ æ¶‰åŠé‡å¤è¯„è®ºæ€»æ•°ï¼š{total_multiple:,} æ¡")

        if unique_user_movie > 0:
            print(f"ğŸ’¡ å»ºè®®ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚å†³å®šæ˜¯å¦ä¿ç•™ç”¨æˆ·çš„å¤šæ¡è¯„è®º")
            print(f"   - æƒ…æ„Ÿåˆ†æï¼šå¯ä¿ç•™å¤šæ¡ï¼Œä½“ç°æƒ…æ„Ÿå˜åŒ–")
            print(f"   - ç»Ÿè®¡åˆ†æï¼šå»ºè®®æ¯ç”¨æˆ·æ¯ç”µå½±åªä¿ç•™ä¸€æ¡")

    # å¤„ç†ç»“æœç»Ÿè®¡
    final_count = len(comments_cleaned)
    removed_total = original_count - final_count

    print(f"\nğŸ“ˆ å»é‡å¤„ç†ç»“æœï¼š")
    print(f"â€¢ åŸå§‹è®°å½•ï¼š{original_count:,} æ¡")
    print(f"â€¢ å¤„ç†åè®°å½•ï¼š{final_count:,} æ¡")
    print(f"â€¢ åˆ é™¤è®°å½•ï¼š{removed_total:,} æ¡")
    print(f"â€¢ æ•°æ®ä¿ç•™ç‡ï¼š{final_count / original_count * 100:.1f}%")

    # éªŒè¯å»é‡æ•ˆæœ
    remaining_duplicates = comments_cleaned.duplicated().sum()
    if remaining_duplicates == 0:
        print("ğŸ‰ æ‰€æœ‰é‡å¤è®°å½•å·²æˆåŠŸå¤„ç†ï¼")
    else:
        print(f"âš ï¸ ä»æœ‰ {remaining_duplicates} æ¡é‡å¤è®°å½•")

    print(f"\nğŸ’¾ å»é‡åçš„æ•°æ®å·²ä¿å­˜ä¸º comments_cleaned")

else:
    print("âŒ æ•°æ®æœªå‡†å¤‡å¥½ï¼Œè·³è¿‡é‡å¤æ•°æ®å¤„ç†")


# å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†
