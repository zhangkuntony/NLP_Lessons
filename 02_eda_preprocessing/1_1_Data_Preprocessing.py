# å¯¼å…¥å¿…è¦çš„åº“
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)

print("âœ… ç¯å¢ƒé…ç½®å®Œæˆï¼")

# å®šä¹‰åªèƒ½è¯»å–å‡½æ•°
def smart_read_csv(file_path, data_size=1000):
    """
    æ™ºèƒ½è¯»å–CSVæ–‡ä»¶ï¼Œè‡ªåŠ¨å°è¯•ä¸åŒç¼–ç 

    å‚æ•°:
        file_path: æ–‡ä»¶è·¯å¾„
        sample_size: æµ‹è¯•æ ·æœ¬å¤§å°

    è¿”å›:
        df_full: è¯»å–çš„å®Œæ•´æ•°æ®æ¡†
        encoding: æˆåŠŸçš„ç¼–ç æ ¼å¼
    """
    # å¸¸è§çš„ä¸­æ–‡ç¼–ç åˆ—è¡¨
    encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin1']

    for encoding in encodings:
        try:
            print(f"ğŸ” å°è¯•ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶...")

            # å…ˆè¯»å–æ ·æœ¬æµ‹è¯•ç¼–ç æ˜¯å¦æ­£ç¡®
            pd.read_csv(file_path, encoding=encoding, nrows=data_size)
            print(f"âœ… æˆåŠŸï¼ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶")

            # æµ‹è¯•æˆåŠŸåè¯»å–å®Œæ•´æ–‡ä»¶
            df_full = pd.read_csv(file_path, encoding=encoding)
            return df_full, encoding

        except Exception as ex:
            print(f"âŒ {encoding} ç¼–ç å¤±è´¥: {str(ex)[:50]}...")
            continue

    print("ğŸ˜± æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥äº†ï¼")
    return None, None

print("ğŸ› ï¸ æ™ºèƒ½è¯»å–å‡½æ•°å®šä¹‰å®Œæˆï¼")

# ğŸ“– è¯»å–ç”µå½±æ•°æ®
print("ğŸ“– æ­£åœ¨è¯»å–ç”µå½±ä¿¡æ¯æ•°æ®...")

# ä½¿ç”¨åªèƒ½è¯»å–å‡½æ•°åŠ è½½ç”µå½±æ•°æ®
movies_df, movies_encoding = smart_read_csv('douban-dataset/movies.csv')

# æ£€æŸ¥è¯»å–ç»“æœ
if movies_df is not None:
    print(f"ğŸ¬ ç”µå½±æ•°æ®è¯»å–æˆåŠŸï¼å…±æœ‰ {len(movies_df)} éƒ¨ç”µå½±")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {movies_df.shape}")
    print(f"ğŸ”¤ ä½¿ç”¨ç¼–ç : {movies_encoding}")
else:
    print("ğŸ’” ç”µå½±æ•°æ®è¯»å–å¤±è´¥ï¼")

# ğŸ’¬ è¯»å–è¯„è®ºæ•°æ®ï¼ˆåˆ†æ‰¹å¤„ç†å¤§æ–‡ä»¶ï¼‰
print("ğŸ’¬ æ­£åœ¨è¯»å–è¯„è®ºæ•°æ®...")
print("âš ï¸  ç”±äºè¯„è®ºæ–‡ä»¶è¾ƒå¤§(68MB)ï¼Œæˆ‘ä»¬å…ˆè¯»å–å‰10000æ¡è¿›è¡Œæ¢ç´¢")

# ä½¿ç”¨åªèƒ½è¯»å–å‡½æ•°å¤„ç†è¯„è®ºæ•°æ®
try:
    # æ–¹æ³•1ï¼šä½¿ç”¨åªèƒ½è¯»å–å‡½æ•°ï¼ˆæ¨èï¼‰
    comments_df, comment_encoding = smart_read_csv('douban-dataset/comments.csv', data_size=1000)

    # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œåªå–å‰10000æ¡
    if comments_df is not None and len(comments_df) > 10000:
        comments_df = comments_df.head(10000)
        print(f"ğŸ“ ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œåªä¿ç•™å‰10000æ¡è¯„è®º")

    if comments_df is not None:
        print(f"âœ… è¯„è®ºæ•°æ®è¯»å–æˆåŠŸï¼è¯»å–äº† {len(comments_df)} æ¡è¯„è®º")
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {comments_df.shape}")
        print(f"ğŸ”¤ ä½¿ç”¨ç¼–ç : {comment_encoding}")

except Exception as e:
    print(f"ğŸ’” è¯„è®ºæ•°æ®è¯»å–å¤±è´¥: {e}")
    comments_df = None


# æ•°æ®è´¨é‡å…¨é¢è¯Šæ–­
if comments_df is not None:
    print("=== ğŸ“Š æ•°æ®è´¨é‡è¯Šæ–­æŠ¥å‘Š ===")

    # 1. åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“‹ æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š")
    print(f"â€¢ æ•°æ®è¡Œæ•°ï¼š{len(comments_df):,}")
    print(f"â€¢ æ•°æ®åˆ—æ•°ï¼š{len(comments_df.columns)}")
    print(f"â€¢ æ•°æ®å¤§å°ï¼š{comments_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

    # 2. ç¼ºå¤±å€¼æ£€æŸ¥
    print(f"\nğŸ•³ï¸ ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
    missing_stats = comments_df.isnull().sum()
    missing_percent = (missing_stats / len(comments_df) * 100).round(2)

    for col in comments_df.columns:
        if missing_stats[col] > 0:
            print(f"â€¢ {col}: {missing_stats[col]:,} ä¸ªç¼ºå¤±å€¼ ({missing_percent[col]}%)")
        else:
            print(f"â€¢ {col}: æ— ç¼ºå¤±å€¼ âœ…")

    # 3. é‡å¤å€¼æ£€æŸ¥
    duplicates = comments_df.duplicated().sum()
    print(f"\nğŸ”„ é‡å¤æ•°æ®ï¼š{duplicates:,} è¡Œ ({duplicates / len(comments_df) * 100:.2f}%)")

    # 4. æ•°æ®ç±»å‹æ£€æŸ¥
    print(f"\nğŸ·ï¸ æ•°æ®ç±»å‹ï¼š")
    for col in comments_df.columns:
        print(f"â€¢ {col}: {comments_df[col].dtype}")

    # 5. æ–‡æœ¬åˆ—è´¨é‡æ£€æŸ¥ï¼ˆCONTENTåˆ—ï¼‰
    if 'CONTENT' in comments_df.columns:
        print(f"\nğŸ“ è¯„è®ºæ–‡æœ¬è´¨é‡åˆ†æï¼š")
        content_lengths = comments_df['CONTENT'].astype(str).str.len()

        print(f"â€¢ å¹³å‡é•¿åº¦ï¼š{content_lengths.mean():.1f} å­—ç¬¦")
        print(f"â€¢ æœ€çŸ­è¯„è®ºï¼š{content_lengths.min()} å­—ç¬¦")
        print(f"â€¢ æœ€é•¿è¯„è®ºï¼š{content_lengths.max()} å­—ç¬¦")
        print(f"â€¢ ä¸­ä½æ•°é•¿åº¦ï¼š{content_lengths.median():.0f} å­—ç¬¦")

        # æ£€æŸ¥å¼‚å¸¸çŸ­çš„è¯„è®º
        very_short = (content_lengths <= 5).sum()
        print(f"â€¢ è¿‡çŸ­è¯„è®º(â‰¤5å­—ç¬¦)ï¼š{very_short} æ¡ ({very_short / len(comments_df) * 100:.2f}%)")

        # æ£€æŸ¥å¼‚å¸¸é•¿çš„è¯„è®º
        very_long = (content_lengths >= 500).sum()
        print(f"â€¢ è¿‡é•¿è¯„è®º(â‰¥500å­—ç¬¦)ï¼š{very_long} æ¡ ({very_long/len(comments_df)*100:.2f}%)")

    print(f"\nâœ… æ•°æ®è´¨é‡è¯Šæ–­å®Œæˆï¼å‘ç° {missing_stats.sum()} ä¸ªç¼ºå¤±å€¼ï¼Œ{duplicates} ä¸ªé‡å¤å€¼")
else:
    print("âŒ æ•°æ®æœªæˆåŠŸåŠ è½½ï¼Œè·³è¿‡è´¨é‡è¯Šæ–­")


# ç³»ç»ŸåŒ–ç¼ºå¤±å€¼å¤„ç†
comments_cleaned = comments_df.copy()
if comments_df is not None:
    print("=== ğŸ¯ ç¼ºå¤±å€¼å¤„ç†æ–¹æ¡ˆ ===")

    # åˆ›å»ºæ•°æ®å‰¯æœ¬
    original_shape = comments_cleaned.shape
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶ï¼š{original_shape}")

    # è®¡ç®—ç¼ºå¤±å€¼æ¯”ä¾‹
    missing_ratios = comments_cleaned.isnull().sum() / len(comments_cleaned)

    print("\nğŸ“Š å„åˆ—ç¼ºå¤±å€¼æ¯”ä¾‹ï¼š")
    for col in comments_cleaned.columns:
        ratio = missing_ratios[col] * 100
        status = "ğŸ”´é«˜é£é™©" if ratio > 20 else "ğŸŸ¡ä¸­ç­‰" if ratio > 5 else "ğŸŸ¢ä½é£é™©"
        print(f"â€¢ {col}: {ratio:.2f}% {status}")

    # å¤„ç†ç­–ç•¥æ‰§è¡Œ
    print("\nğŸ”§ æ‰§è¡Œå¤„ç†ç­–ç•¥ï¼š")

    # 1. æ ¸å¿ƒå­—æ®µï¼šè¯„è®ºå†…å®¹ä¸èƒ½ä¸ºç©ºï¼Œç›´æ¥åˆ é™¤
    if 'CONTENT' in comments_cleaned.columns:
        before_count = len(comments_cleaned)
        comments_cleaned = comments_cleaned.dropna(subset=['CONTENT'])
        removed_count = before_count - len(comments_cleaned)
        if removed_count > 0:
            print(f"âœ… åˆ é™¤æ— è¯„è®ºå†…å®¹çš„è®°å½•ï¼š{removed_count} æ¡")

    # 2. ç”¨æˆ·IDå­—æ®µï¼šç”¨â€œæœªçŸ¥ç”¨æˆ·â€å¡«è¡¥
    if 'CREATOR' in comments_cleaned.columns:
        creator_missing = comments_cleaned['CREATOR'].isnull().sum()
        if creator_missing > 0:
            comments_cleaned['CREATOR'] = comments_cleaned['CREATOR'].fillna('æœªçŸ¥ç”¨æˆ·')
            print(f"âœ… ç”¨æˆ·åç¼ºå¤±å€¼å¡«è¡¥ï¼š{creator_missing} æ¡ â†’ 'æœªçŸ¥ç”¨æˆ·'")

    # 3. è¯„åˆ†å­—æ®µï¼šç”¨ä¸­ä½æ•°å¡«è¡¥ï¼ˆé¿å…å¼‚å¸¸å€¼å½±å“ï¼‰
    if 'RATING' in comments_cleaned.columns:
        rating_missing = comments_cleaned['RATING'].isnull().sum()
        if rating_missing > 0:
            # å°†è¯„åˆ†è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            comments_cleaned['RATING'] = pd.to_numeric(comments_cleaned['RATING'], errors='coerce')
            median_rating = comments_cleaned['RATING'].median()
            comments_cleaned['RATING'] = comments_cleaned['RATING'].fillna(median_rating)
            print(f"âœ… è¯„åˆ†ç¼ºå¤±å€¼å¡«è¡¥ï¼š{rating_missing} æ¡ â†’ {median_rating}")

    # 4. å…¶ä»–å­—æ®µï¼šç”¨â€œæœªçŸ¥â€å¡«è¡¥
    text_columns = ['ID', 'TIME', 'MOVIEID', 'ADD_TIME']
    for col in text_columns:
        if col in comments_cleaned.columns:
            missing_count = comments_cleaned[col].isnull().sum()
            if missing_count > 0:
                comments_cleaned[col] = comments_cleaned[col].fillna('æœªçŸ¥')
                print(f"âœ… {col}ç¼ºå¤±å€¼å¡«è¡¥ï¼š{missing_count} æ¡ â†’ 'æœªçŸ¥'")

    # å¤„ç†ç»“æœç»Ÿè®¡
    final_shape = comments_cleaned.shape
    final_missing = comments_cleaned.isnull().sum().sum()

    print(f"\nğŸ“ˆ å¤„ç†ç»“æœï¼š")
    print(f"â€¢ å¤„ç†å‰ï¼š{original_shape[0]:,} è¡Œ")
    print(f"â€¢ å¤„ç†åï¼š{final_shape[0]:,} è¡Œ")
    print(f"â€¢ æ•°æ®ä¿ç•™ç‡ï¼š{final_shape[0] / original_shape[0] * 100:.1f}%")
    print(f"â€¢ å‰©ä½™ç¼ºå¤±å€¼ï¼š{final_missing} ä¸ª")

    if final_missing == 0:
        print("ğŸ‰ æ‰€æœ‰ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼")
    else:
        print(f"âš ï¸ ä»æœ‰ {final_missing} ä¸ªç¼ºå¤±å€¼éœ€è¦å¤„ç†")

else:
    print("âŒ æ•°æ®æœªåŠ è½½ï¼Œè·³è¿‡ç¼ºå¤±å€¼å¤„ç†")


# é‡å¤æ•°æ®å…¨é¢å¤„ç†
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== ğŸ”„ é‡å¤æ•°æ®æ£€æµ‹ä¸å¤„ç† ===")

    original_count = len(comments_cleaned)
    print(f"å¤„ç†å‰æ•°æ®é‡ï¼š{original_count:,} æ¡")

    # 1. å®Œå…¨é‡å¤æ£€æµ‹
    print(f"\nğŸ” æ£€æµ‹å®Œå…¨é‡å¤...")
    duplicate_all = comments_cleaned.duplicated()
    duplicate_count_all = duplicate_all.sum()

    print(f"â€¢ å®Œå…¨é‡å¤è®°å½•ï¼š{duplicate_count_all:,} æ¡ ({duplicate_count_all / original_count * 100:.2f}%)")

    if duplicate_count_all > 0:
        # åˆ é™¤å®Œå…¨é‡å¤
        comments_cleaned = comments_cleaned.drop_duplicates()
        print(f"âœ… å·²åˆ é™¤å®Œå…¨é‡å¤è®°å½•")

    # 2. å†…å®¹é‡å¤æ£€æµ‹ï¼ˆåŒä¸€ç”¨æˆ·å¯¹åŒä¸€ç”µå½±çš„é‡å¤è¯„è®ºï¼‰
    print(f"\nğŸ” æ£€æµ‹å†…å®¹é‡å¤...")
    if 'CREATOR' in comments_cleaned.columns and 'MOVIEID' in comments_cleaned.columns and 'CONTENT' in comments_cleaned.columns:

        # æ£€æµ‹åŒç”¨æˆ·åŒç”µå½±çš„é‡å¤è¯„è®º
        content_duplicates = comments_cleaned.duplicated(subset=['CREATOR', 'MOVIEID', 'CONTENT'])
        content_duplicate_count = content_duplicates.sum()

        print(f"â€¢ å†…å®¹é‡å¤è®°å½•ï¼š{content_duplicate_count:,} æ¡ ({content_duplicate_count / len(comments_cleaned) * 100:.2f}%)")

        if content_duplicate_count > 0:
            # åˆ é™¤å†…å®¹é‡å¤ï¼Œä¿ç•™æœ€åä¸€æ¡
            if 'ADD_TIME' in comments_cleaned.columns:
                # å¦‚æœæœ‰æ—¶é—´å­—æ®µï¼Œä¿ç•™æœ€æ–°çš„
                comments_cleaned = comments_cleaned.sort_values('ADD_TIME').drop_duplicates(
                    subset=['CREATOR', 'MOVIEID', 'CONTENT'], keep='last'
                )
                print(f"âœ… å·²åˆ é™¤å†…å®¹é‡å¤è®°å½•ï¼Œä¿ç•™æœ€æ–°çš„")
            else:
                # æ²¡æœ‰æ—¶é—´å­—æ®µï¼Œä¿ç•™ç¬¬ä¸€æ¡
                comments_cleaned = comments_cleaned.drop_duplicates(
                    subset=['CREATOR', 'MOVIEID', 'CONTENT'], keep='first'
                )
                print(f"âœ… å·²åˆ é™¤å†…å®¹é‡å¤è®°å½•ï¼Œä¿ç•™ç¬¬ä¸€æ¡")

    # 3. ç”¨æˆ·é‡å¤è¯„è®ºæ£€æµ‹ï¼ˆå¯é€‰å¤„ç†ï¼‰
    print(f"\nğŸ” æ£€æµ‹ç”¨æˆ·é‡å¤è¯„è®º...")
    if 'CREATOR' in comments_cleaned.columns and 'MOVIEID' in comments_cleaned.columns:

        # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·å¯¹æ¯éƒ¨ç”µå½±çš„è¯„è®ºæ•°
        user_movie_counts = comments_cleaned.groupby(['CREATOR', 'MOVIEID']).size()
        multiple_reviews = user_movie_counts[user_movie_counts > 1]

        total_multiple = multiple_reviews.sum()
        unique_user_movie = len(multiple_reviews)

        print(f"â€¢ é‡å¤è¯„è®ºçš„ç”¨æˆ·-ç”µå½±ç»„åˆï¼š{unique_user_movie:,} ç»„")
        print(f"â€¢ æ¶‰åŠé‡å¤è¯„è®ºæ€»æ•°ï¼š{total_multiple:,} æ¡")

        if unique_user_movie > 0:
            print(f"ğŸ’¡ å»ºè®®ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚å†³å®šæ˜¯å¦ä¿ç•™ç”¨æˆ·çš„å¤šæ¡è¯„è®º")
            print(f"   - æƒ…æ„Ÿåˆ†æï¼šå¯ä¿ç•™å¤šæ¡ï¼Œä½“ç°æƒ…æ„Ÿå˜åŒ–")
            print(f"   - ç»Ÿè®¡åˆ†æï¼šå»ºè®®æ¯ç”¨æˆ·æ¯ç”µå½±åªä¿ç•™ä¸€æ¡")

    # å¤„ç†ç»“æœç»Ÿè®¡
    final_count = len(comments_cleaned)
    removed_total = original_count - final_count

    print(f"\nğŸ“ˆ å»é‡å¤„ç†ç»“æœï¼š")
    print(f"â€¢ åŸå§‹è®°å½•ï¼š{original_count:,} æ¡")
    print(f"â€¢ å¤„ç†åè®°å½•ï¼š{final_count:,} æ¡")
    print(f"â€¢ åˆ é™¤è®°å½•ï¼š{removed_total:,} æ¡")
    print(f"â€¢ æ•°æ®ä¿ç•™ç‡ï¼š{final_count / original_count * 100:.1f}%")

    # éªŒè¯å»é‡æ•ˆæœ
    remaining_duplicates = comments_cleaned.duplicated().sum()
    if remaining_duplicates == 0:
        print("ğŸ‰ æ‰€æœ‰é‡å¤è®°å½•å·²æˆåŠŸå¤„ç†ï¼")
    else:
        print(f"âš ï¸ ä»æœ‰ {remaining_duplicates} æ¡é‡å¤è®°å½•")

    print(f"\nğŸ’¾ å»é‡åçš„æ•°æ®å·²ä¿å­˜ä¸º comments_cleaned")

else:
    print("âŒ æ•°æ®æœªå‡†å¤‡å¥½ï¼Œè·³è¿‡é‡å¤æ•°æ®å¤„ç†")


# å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== ğŸš¨ å¼‚å¸¸å€¼æ£€æµ‹åˆ†æ ===")

    # 1. æ–‡æœ¬é•¿åº¦å¼‚å¸¸æ£€æµ‹
    print("ğŸ” æ£€æµ‹æ–‡æœ¬é•¿åº¦å¼‚å¸¸...")
    content_lengths = comments_cleaned['CONTENT'].astype(str).str.len()

    # è¿‡çŸ­æ–‡æœ¬
    too_short = content_lengths <= 3
    short_count = too_short.sum()
    print(f"â€¢ è¿‡çŸ­è¯„è®º(â‰¤3å­—ç¬¦)ï¼š{short_count:,} æ¡ ({short_count / len(comments_cleaned) * 100:.2f}%)")

    # è¿‡é•¿æ–‡æœ¬
    too_long = content_lengths >= 1000
    long_count = too_long.sum()
    print(f"â€¢ è¿‡é•¿è¯„è®º(â‰¥1000å­—ç¬¦)ï¼š{long_count:,} æ¡ ({long_count / len(comments_cleaned) * 100:.2f}%)")

    # å±•ç¤ºå¼‚å¸¸ä¾‹å­
    if short_count > 0:
        print("ğŸ“ è¿‡çŸ­è¯„è®ºç¤ºä¾‹ï¼š")
        short_samples = comments_cleaned[too_short]['CONTENT'].head(3)
        for i, content in enumerate(short_samples, 1):
            print(f"   {i}. '{content}' (é•¿åº¦:{len(str(content))})")

    # 2. é‡å¤å­—ç¬¦å¼‚å¸¸æ£€æµ‹
    print(f"\nğŸ” æ£€æµ‹é‡å¤å­—ç¬¦å¼‚å¸¸...")
    def has_excessive_repetition(text):
        """æ£€æµ‹æ˜¯å¦æœ‰è¿‡å¤šé‡å¤å­—ç¬¦"""
        if pd.isna(text):
            return False
        text = str(text)
        # æ£€æŸ¥è¿ç»­4ä¸ªä»¥ä¸Šç›¸åŒå­—ç¬¦
        pattern = r'(.)\\1{3,}'
        return bool(re.search(pattern, text))

    repetitive_mask = comments_cleaned['CONTENT'].apply(has_excessive_repetition)
    repetitive_count = repetitive_mask.sum()
    print(f"â€¢ è¿‡åº¦é‡å¤å­—ç¬¦ï¼š{repetitive_count:,} æ¡ ({repetitive_count / len(comments_cleaned) * 100:.2f}%)")

    if repetitive_count > 0:
        print("ğŸ“ é‡å¤å­—ç¬¦å¼‚å¸¸ç¤ºä¾‹ï¼š")
        rep_samples = comments_cleaned[repetitive_mask]['CONTENT'].head(3)
        for i, content in enumerate(rep_samples, 1):
            preview = str(content)[:50] + "..." if len(str(content)) > 50 else str(content)
            print(f"   {i}. '{preview}'")

    # 3. ç‰¹æ®Šå­—ç¬¦å æ¯”å¼‚å¸¸
    print(f"\nğŸ” æ£€æµ‹ç‰¹æ®Šå­—ç¬¦å æ¯”å¼‚å¸¸...")
    def calc_special_char_ratio(text):
        """è®¡ç®—ç‰¹æ®Šå­—ç¬¦å æ¯”"""
        if pd.isna(text):
            return 0
        text = str(text)
        if len(text) == 0:
            return 0

        # è®¡ç®—éä¸­æ–‡ã€éè‹±æ–‡ã€éæ•°å­—å­—ç¬¦çš„æ¯”ä¾‹
        special_count = 0
        for char in text:
            if not (char.isalnum() or '\\u4e00' <= char <= '\\u9fff'):
                special_count += 1

        return special_count / len(text)

    special_ratios = comments_cleaned['CONTENT'].apply(calc_special_char_ratio)
    high_special = special_ratios > 0.5             # ç‰¹æ®Šå­—ç¬¦è¶…è¿‡50%
    high_special_count = high_special.sum()

    print(f"â€¢ ç‰¹æ®Šå­—ç¬¦å æ¯”>50%ï¼š{high_special_count:,} æ¡ ({high_special_count / len(comments_cleaned) * 100:.2f}%)")

    # 4. è¯„åˆ†å¼‚å¸¸æ£€æµ‹
    if 'RATING' in comments_cleaned.columns:
        print(f"\nğŸ” æ£€æµ‹è¯„åˆ†å¼‚å¸¸...")

        # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹è¿›è¡Œæ£€æŸ¥
        numeric_ratings = pd.to_numeric(comments_cleaned['RATING'], errors='coerce')

        # æ£€æµ‹è¯„åˆ†èŒƒå›´å¼‚å¸¸ï¼ˆå‡è®¾æ­£å¸¸èŒƒå›´æ˜¯1-5ï¼‰
        invalid_ratings = (numeric_ratings < 1) | (numeric_ratings > 5)
        invalid_rating_count = invalid_ratings.sum()

        print(f"â€¢ å¼‚å¸¸è¯„åˆ†(ä¸åœ¨1-5èŒƒå›´)ï¼š{invalid_rating_count:,} æ¡")

        if invalid_rating_count > 0:
            print("ğŸ“Š å¼‚å¸¸è¯„åˆ†åˆ†å¸ƒï¼š")
            abnormal_ratings = numeric_ratings[invalid_ratings].value_counts().head(5)
            for rating, count in abnormal_ratings.items():
                print(f"   è¯„åˆ†{rating}: {count}æ¡")

    # 5. å¼‚å¸¸å¤„ç†å†³ç­–
    print(f"\nğŸ”§ å¼‚å¸¸å¤„ç†å»ºè®®ï¼š")

    # è®¡ç®—æ€»å¼‚å¸¸æ•°é‡
    total_anomalies = short_count + repetitive_count + high_special_count
    anomaly_rate = total_anomalies / len(comments_cleaned) * 100

    print(f"â€¢ æ€»å¼‚å¸¸è®°å½•ï¼š{total_anomalies:,} æ¡ ({anomaly_rate:.2f}%)")

    if anomaly_rate < 2:
        print("âœ… å¼‚å¸¸ç‡è¾ƒä½ï¼Œæ•°æ®è´¨é‡è‰¯å¥½")
    elif anomaly_rate < 5:
        print("âš ï¸ å¼‚å¸¸ç‡ä¸­ç­‰ï¼Œå»ºè®®å…³æ³¨ä½†å¯æ¥å—")
    else:
        print("ğŸ”´ å¼‚å¸¸ç‡è¾ƒé«˜ï¼Œå»ºè®®æ·±å…¥åˆ†æåŸå› ")

    # å¯é€‰ï¼šåˆ é™¤æ˜æ˜¾çš„å¼‚å¸¸æ•°æ®
    severe_anomalies = too_short | (special_ratios > 0.8)       # è¿‡çŸ­æˆ–ç‰¹æ®Šå­—ç¬¦å æ¯”è¿‡é«˜
    severe_count = severe_anomalies.sum()

    if severe_count > 0:
        print(f"\nğŸ’¡ å‘ç°ä¸¥é‡å¼‚å¸¸ {severe_count} æ¡ï¼Œæ˜¯å¦åˆ é™¤ï¼Ÿ")
        print("   - åˆ é™¤åæ•°æ®æ›´çº¯å‡€ï¼Œä½†å¯èƒ½ä¸¢å¤±ä¿¡æ¯")
        print("   - ä¿ç•™åä¾¿äºåç»­æ·±å…¥åˆ†æ")
        print("   ğŸ“ å½“å‰é€‰æ‹©ï¼šä¿ç•™æ‰€æœ‰æ•°æ®ï¼Œæ·»åŠ å¼‚å¸¸æ ‡è®°")

        # æ·»åŠ å¼‚å¸¸æ ‡è®°åˆ—
        comments_cleaned['is_anomaly'] = severe_anomalies
        print(f"âœ… å·²æ·»åŠ å¼‚å¸¸æ ‡è®°åˆ— 'is_anomaly'")

    print(f"\nğŸ“Š å¼‚å¸¸æ£€æµ‹å®Œæˆï¼æ•°æ®è´¨é‡è¯„ä¼°ç»“æœå·²ç”Ÿæˆ")

else:
    print("âŒ æ•°æ®æœªå‡†å¤‡å¥½ï¼Œè·³è¿‡å¼‚å¸¸å€¼æ£€æµ‹")

# NLPä¸“ä¸šæ–‡æœ¬æ¸…æ´—å‡½æ•°
def clean_text_for_nlp(text):
    """NLPä¸“ç”¨æ–‡æœ¬æ¸…æ´—å‡½æ•°"""

    if pd.isna(text) or text is None:
        return ""

    text = str(text)

    # 1. å»é™¤HTMLæ ‡ç­¾å’Œå®ä½“
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'&[a-zA-Z]+;', ' ', text)
    text = re.sub(r'&lt;|&gt;|&nbsp;', ' ', text)

    # 2. å¤„ç†ç½‘ç»œç”¨è¯­æ ‡å‡†åŒ–
    text = re.sub(r'h{3,}', 'å“ˆå“ˆ', text)  # hhh -> å“ˆå“ˆ
    text = re.sub(r'2333+', 'å“ˆå“ˆ', text)  # 2333 -> å“ˆå“ˆ
    text = re.sub(r'6{4,}', 'å‰å®³', text)  # 6666 -> å‰å®³
    text = re.sub(r'\\d{4,}', '', text)  # å»é™¤é•¿æ•°å­—ä¸²

    # 3. å¤„ç†é‡å¤å­—ç¬¦ï¼ˆä¿ç•™ä¸€å®šçš„é‡å¤è¡¨è¾¾æƒ…æ„Ÿï¼‰
    text = re.sub(r'(.)\\1{4,}', r'\\1\\1\\1', text)  # è¶…è¿‡4ä¸ªé‡å¤å‡å°‘åˆ°3ä¸ª
    text = re.sub(r'[ï¼!]{4,}', 'ï¼ï¼ï¼', text)  # å¤šä¸ªæ„Ÿå¹å·
    text = re.sub(r'[ï¼Ÿ?]{4,}', 'ï¼Ÿï¼Ÿï¼Ÿ', text)  # å¤šä¸ªé—®å·
    text = re.sub(r'[ã€‚.]{3,}', '...', text)  # å¤šä¸ªå¥å·

    # 4. å»é™¤ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
    text = re.sub(r'[â˜…â˜†â€»@#$%^&*\\[]{}|\\\\]', '', text)
    text = re.sub(r'[~ï½]', '', text)

    # 5. æ¸…ç†ç©ºç™½å­—ç¬¦
    text = re.sub(r'\\s+', ' ', text)
    text = text.strip()

    return text

# åº”ç”¨æ–‡æœ¬æ¸…æ´—
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== ğŸ§¹ NLPæ–‡æœ¬æ·±åº¦æ¸…æ´— ===")

    print("ğŸ”§ æ­£åœ¨è¿›è¡Œæ–‡æœ¬æ¸…æ´—...")

    # ä¿å­˜åŸå§‹å†…å®¹ç”¨äºå¯¹æ¯”
    original_content_sample = comments_cleaned['CONTENT'].head(3).tolist()

    # åº”ç”¨æ¸…æ´—å‡½æ•°
    comments_cleaned['CONTENT_CLEANED'] = comments_cleaned['CONTENT'].apply(clean_text_for_nlp)

    # ç»Ÿè®¡æ¸…æ´—æ•ˆæœ
    original_total_length = comments_cleaned['CONTENT'].astype(str).str.len().sum()
    cleaned_total_length = comments_cleaned['CONTENT_CLEANED'].astype(str).str.len().sum()
    reduction_rate = (1 - cleaned_total_length / original_total_length) * 100

    print(f"ğŸ“Š æ¸…æ´—æ•ˆæœç»Ÿè®¡ï¼š")
    print(f"â€¢ åŸå§‹æ€»å­—ç¬¦æ•°ï¼š{original_total_length:,}")
    print(f"â€¢ æ¸…æ´—åå­—ç¬¦æ•°ï¼š{cleaned_total_length:,}")
    print(f"â€¢ å‹ç¼©ç‡ï¼š{reduction_rate:.1f}%")

    # å±•ç¤ºæ¸…æ´—å‰åå¯¹æ¯”
    print(f"\nğŸ“ æ¸…æ´—å‰åå¯¹æ¯”ç¤ºä¾‹ï¼š")
    cleaned_content_sample = comments_cleaned['CONTENT_CLEANED'].head(3).tolist()

    for i, (original, cleaned) in enumerate(zip(original_content_sample, cleaned_content_sample), 1):
        print(f"\n{i}. åŸæ–‡ï¼š{original}")
        print(f"   æ¸…æ´—åï¼š{cleaned}")

        # è®¡ç®—ä¸ªä¾‹å‹ç¼©ç‡
        if len(original) > 0:
            individual_reduction = (1 - len(cleaned) / len(original)) * 100
            print(f"   å‹ç¼©ç‡ï¼š{individual_reduction:.1f}%")

    # æ£€æŸ¥æ¸…æ´—è´¨é‡
    empty_after_cleaning = np.sum(comments_cleaned['CONTENT_CLEANED'].str.len() == 0)
    if empty_after_cleaning > 0:
        print(f"\nâš ï¸ è­¦å‘Šï¼šæœ‰ {empty_after_cleaning} æ¡è¯„è®ºæ¸…æ´—åå˜ä¸ºç©º")
        print("å»ºè®®æ£€æŸ¥æ¸…æ´—è§„åˆ™æ˜¯å¦è¿‡äºä¸¥æ ¼")
    else:
        print(f"\nâœ… æ¸…æ´—è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œæ— è¯„è®ºå˜ä¸ºç©º")

    print(f"\nğŸ’¾ æ¸…æ´—åçš„æ–‡æœ¬å·²ä¿å­˜åˆ° 'CONTENT_CLEANED' åˆ—")

else:
    print("âŒ æ•°æ®æœªå‡†å¤‡å¥½ï¼Œè·³è¿‡æ–‡æœ¬æ¸…æ´—")

# ä¸­æ–‡åˆ†è¯å¤„ç†
# å®‰è£…å’Œå¯¼å…¥jieba
import jieba
# è‡ªç„¶è¯­è¨€å¤„ç†
# è‡ªç„¶ è¯­è¨€ å¤„ç†

# ç”µå½±é¢†åŸŸè‡ªå®šä¹‰è¯å…¸
movie_words = [
    # ç”µå½±åç§°
    "å¤ä»‡è€…è”ç›Ÿ", "é’¢é“ä¾ ", "ç¾å›½é˜Ÿé•¿", "é»‘å¯¡å¦‡", "é›·ç¥", "ç»¿å·¨äºº",
    "èœ˜è››ä¾ ", "å¥‡å¼‚åšå£«", "é»‘è±¹", "æƒŠå¥‡é˜Ÿé•¿", "æµæµªåœ°çƒ", "å“ªå’",

    # å¯¼æ¼”å’Œæ¼”å‘˜
    "è¯ºå…°", "æ–¯çš®å°”ä¼¯æ ¼", "å¼ è‰ºè°‹", "å†¯å°åˆš", "å¾å³¥", "ç‹å®å¼º",
    "æ¼«å¨", "DC", "è¿ªå£«å°¼", "ç¯çƒå½±ä¸š", "ç´¢å°¼å½±ä¸š",

    # ç”µå½±æœ¯è¯­
    "ç‰¹æ•ˆ", "å‰§æƒ…", "æ¼”æŠ€", "é…ä¹", "æ‘„å½±", "å‰ªè¾‘", "ç¼–å‰§",
    "ç¥¨æˆ¿", "å£ç¢‘", "è¯„åˆ†", "é¦–æ˜ ", "ä¸Šæ˜ ", "ä¸‹æ˜ ", "ç‚¹æ˜ ",
    "IMAX", "3D", "4D", "æœæ¯”", "å·¨å¹•"
]

# æ·»åŠ è‡ªå®šä¹‰è¯æ±‡åˆ°jiebaè¯å…¸
print("ğŸ”§ åŠ è½½ç”µå½±é¢†åŸŸè¯å…¸...")
for word in movie_words:
    jieba.add_word(word)
print(f"âœ… å·²æ·»åŠ  {len(movie_words)} ä¸ªç”µå½±é¢†åŸŸä¸“æœ‰è¯æ±‡")

# åˆ†è¯å¤„ç†å‡½æ•°
def segment_text(text, mode='accurate'):
    """
    ä¸­æ–‡åˆ†è¯å‡½æ•°
    mode: 'accurate'(ç²¾ç¡®), 'full'(å…¨æ¨¡å¼), 'search'(æœç´¢æ¨¡å¼)
    """
    if pd.isna(text) or not text.strip():
        return []

    text = str(text).strip()

    if mode == 'accurate':
        words = jieba.lcut(text)
    elif mode == 'full':
        words = jieba.lcut(text, cut_all=True)
    elif mode == 'search':
        words = jieba.lcut_for_search(text)
    else:
        words = jieba.lcut(text)

    # è¿‡æ»¤é•¿åº¦å°äº2çš„è¯å’Œçº¯æ ‡ç‚¹ç¬¦å·
    filtered_words = []
    for filter_word in words:
        filter_word = filter_word.strip()
        if (len(filter_word) >= 2 and
                not all(char in 'ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘' for char in filter_word)):
            filtered_words.append(filter_word)

    return filtered_words

text = "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"

print(segment_text(text, mode="accurate"))
print(segment_text(text, mode="full"))
print(segment_text(text, mode="search"))

jieba.add_word("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†")
print(segment_text(text, mode="accurate"))
print(segment_text(text, mode="full"))

# åº”ç”¨åˆ†è¯å¤„ç†
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("\n=== âœ‚ï¸ ä¸­æ–‡åˆ†è¯å¤„ç† ===")

    # é€‰æ‹©ç”¨äºåˆ†è¯çš„æ–‡æœ¬åˆ—
    text_column = 'CONTENT_CLEANED' if 'CONTENT_CLEANED' in comments_cleaned.columns else 'CONTENT'
    print(f"ğŸ”§ ä½¿ç”¨ '{text_column}' åˆ—è¿›è¡Œåˆ†è¯...")

    # å¯¹å‰1000æ¡è¿›è¡Œåˆ†è¯æ¼”ç¤ºï¼ˆé¿å…å¤„ç†æ—¶é—´è¿‡é•¿ï¼‰
    sample_size = min(1000, len(comments_cleaned))
    sample_data = comments_cleaned.head(sample_size).copy()

    print(f"ğŸ“Š å¤„ç†æ ·æœ¬ï¼š{sample_size} æ¡è¯„è®º")

    # æ‰§è¡Œåˆ†è¯
    print("ğŸ”„ æ­£åœ¨è¿›è¡Œåˆ†è¯å¤„ç†...")
    sample_data['WORDS'] = sample_data[text_column].apply(
        lambda x: segment_text(x, mode='accurate')
    )

    # ç»Ÿè®¡åˆ†è¯æ•ˆæœ
    total_words = sample_data['WORDS'].apply(len).sum()
    avg_words_per_review = total_words / len(sample_data)

    print(f"ğŸ“ˆ åˆ†è¯ç»Ÿè®¡ç»“æœï¼š")
    print(f"â€¢ æ€»è¯æ±‡æ•°ï¼š{total_words:,}")
    print(f"â€¢ å¹³å‡æ¯æ¡è¯„è®ºè¯æ•°ï¼š{avg_words_per_review:.1f}")

    # å±•ç¤ºåˆ†è¯ç¤ºä¾‹
    print(f"\nğŸ“ åˆ†è¯æ•ˆæœç¤ºä¾‹ï¼š")
    for i in range(3):
        if i < len(sample_data):
            original = sample_data.iloc[i][text_column]
            words = sample_data.iloc[i]['WORDS']

            print(f"\n{i + 1}. åŸæ–‡ï¼š{original[:80]}...")
            print(f"   åˆ†è¯ï¼š{' / '.join(words[:15])}...")
            print(f"   è¯æ•°ï¼š{len(words)}")

    # è¯é¢‘ç»Ÿè®¡
    print(f"\nğŸ“Š é«˜é¢‘è¯æ±‡åˆ†æï¼š")
    all_words = []
    for words_list in sample_data['WORDS']:
        all_words.extend(words_list)

    from collections import Counter

    word_freq = Counter(all_words)
    top_words = word_freq.most_common(10)

    print("ğŸ” TOP10é«˜é¢‘è¯æ±‡ï¼š")
    for word, freq in top_words:
        print(f"â€¢ {word}: {freq} æ¬¡")

    # æ£€æŸ¥è‡ªå®šä¹‰è¯å…¸æ•ˆæœ
    custom_words_found = []
    for word in movie_words:
        if word in all_words:
            custom_words_found.append((word, word_freq[word]))

    if custom_words_found:
        print(f"\nğŸ¬ å‘ç°çš„ç”µå½±ç›¸å…³è¯æ±‡ï¼š")
        for word, freq in sorted(custom_words_found, key=lambda x: x[1], reverse=True)[:5]:
            print(f"â€¢ {word}: {freq} æ¬¡")

    print(f"\nğŸ’¾ åˆ†è¯ç»“æœå·²ä¿å­˜åˆ° 'WORDS' åˆ—")

    # ä¿å­˜åˆ°ä¸»æ•°æ®é›†
    comments_cleaned = comments_cleaned.head(sample_size).copy()
    comments_cleaned['WORDS'] = sample_data['WORDS']

else:
    print("âŒ æ•°æ®æœªå‡†å¤‡å¥½ï¼Œè·³è¿‡åˆ†è¯å¤„ç†")

# åœç”¨è¯å¤„ç†
def create_stopwords_for_movie_reviews():
    """æ„å»ºé€‚åˆç”µå½±è¯„è®ºçš„åœç”¨è¯è¡¨"""

    # åŸºç¡€åœç”¨è¯
    basic_stopwords = {
        # åŠ©è¯å’Œè™šè¯
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
        'ä¸Š', 'ä¹Ÿ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'è¿™ä¸ª', 'é‚£ä¸ª',

        # è¯­æ°”è¯
        'å§', 'å‘¢', 'å•Š', 'å—¯', 'å“¦', 'å‘€', 'å˜›', 'å‘',

        # è¿æ¥è¯
        'ä½†æ˜¯', 'ç„¶å', 'å› ä¸º', 'æ‰€ä»¥', 'è€Œä¸”', 'ä¸è¿‡', 'è™½ç„¶', 'å¦‚æœ', 'é‚£ä¹ˆ', 'æˆ–è€…',

        # ä»£è¯
        'è¿™', 'é‚£', 'è¿™äº›', 'é‚£äº›', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬',

        # ä»‹è¯
        'ä»', 'å‘', 'å¯¹', 'å¯¹äº', 'å…³äº', 'ç”±äº', 'é€šè¿‡', 'æ ¹æ®', 'æŒ‰ç…§',

        # æ—¶é—´è¯
        'ç°åœ¨', 'å½“æ—¶', 'ä¹‹å‰', 'ä»¥å‰', 'ä¹‹å', 'ä»¥å', 'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©'
    }

    # ç”µå½±è¯„è®ºç‰¹å®šåœç”¨è¯
    movie_stopwords = {
        'ç”µå½±', 'å½±ç‰‡', 'ç‰‡å­', 'è¿™éƒ¨', 'è¿™ä¸ª', 'ä¸€éƒ¨', 'æ•´éƒ¨',
        'è§‚çœ‹', 'çœ‹äº†', 'çœ‹è¿‡', 'è§‚å½±', 'çœ‹åˆ°', 'çœ‹è§',
        'æ„Ÿè§‰', 'è§‰å¾—', 'è®¤ä¸º', 'ä¸ªäºº', 'æˆ‘è§‰å¾—', 'æˆ‘è®¤ä¸º', 'æˆ‘æ„Ÿè§‰',
        'è¿˜æ˜¯', 'å°±æ˜¯', 'åªæ˜¯', 'çœŸçš„', 'ç¡®å®', 'çš„ç¡®'
    }

    # æ ‡ç‚¹ç¬¦å·
    punctuation = {
        'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š', '"', '"', ''', ''',
        'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€', 'â€¦', 'â€”', 'Â·'
    }

    return basic_stopwords | movie_stopwords | punctuation

def filter_stopwords(filter_words, scenario='general'):
    """
    æ ¹æ®ä¸åŒåœºæ™¯è¿‡æ»¤åœç”¨è¯
    scenario: 'general', 'sentiment', 'topic'
    """
    stopwords = create_stopwords_for_movie_reviews()

    if scenario == 'sentiment':
        # æƒ…æ„Ÿåˆ†æï¼šä¿ç•™ç¨‹åº¦è¯
        degree_words = {'å¾ˆ', 'éå¸¸', 'ç‰¹åˆ«', 'å¤ª', 'è¶…çº§', 'æœ€', 'æå…¶', 'ç›¸å½“', 'æ¯”è¾ƒ', 'æœ‰ç‚¹', 'ç¨å¾®'}
        stopwords = stopwords - degree_words
    elif scenario == 'topic':
        # ä¸»é¢˜åˆ†æï¼šæ›´ä¸¥æ ¼çš„è¿‡æ»¤
        extra_stopwords = {
            'æ¯”è¾ƒ', 'å¯èƒ½', 'åº”è¯¥', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ',
            'æ—¶å€™', 'åœ°æ–¹', 'æ–¹é¢', 'é—®é¢˜', 'ä¸œè¥¿', 'äº‹æƒ…', 'æ–¹æ³•'
        }
        stopwords = stopwords | extra_stopwords

    # è¿‡æ»¤åœç”¨è¯
    filtered_words = [filter_word for filter_word in filter_words if filter_word not in stopwords]
    return filtered_words


# åº”ç”¨åœç”¨è¯è¿‡æ»¤
if 'comments_cleaned' in locals() and 'WORDS' in comments_cleaned.columns:
    print("=== ğŸš« åœç”¨è¯è¿‡æ»¤å¤„ç† ===")

    # æ„å»ºåœç”¨è¯è¡¨
    stopwords = create_stopwords_for_movie_reviews()
    print(f"ğŸ“‹ åœç”¨è¯è¡¨å¤§å°ï¼š{len(stopwords)} ä¸ªè¯")
    print(f"ğŸ” åœç”¨è¯ç¤ºä¾‹ï¼š{list(stopwords)[:15]}...")

    # åº”ç”¨ä¸åŒåœºæ™¯çš„åœç”¨è¯è¿‡æ»¤
    scenarios = {
        'general': 'é€šç”¨åœºæ™¯',
        'sentiment': 'æƒ…æ„Ÿåˆ†æ',
        'topic': 'ä¸»é¢˜åˆ†æ'
    }

    for scenario, desc in scenarios.items():
        col_name = f'WORDS_{scenario.upper()}'
        comments_cleaned[col_name] = comments_cleaned['WORDS'].apply(
            lambda x: filter_stopwords(x, scenario)
        )

        # ç»Ÿè®¡è¿‡æ»¤æ•ˆæœ
        original_word_count = comments_cleaned['WORDS'].apply(len).sum()
        filtered_word_count = comments_cleaned[col_name].apply(len).sum()
        reduction_rate = (1 - filtered_word_count / original_word_count) * 100

        print(f"\\nğŸ¯ {desc}è¿‡æ»¤ç»“æœï¼š")
        print(f"â€¢ åŸå§‹è¯æ•°ï¼š{original_word_count:,}")
        print(f"â€¢ è¿‡æ»¤åè¯æ•°ï¼š{filtered_word_count:,}")
        print(f"â€¢ è¿‡æ»¤ç‡ï¼š{reduction_rate:.1f}%")

    # å±•ç¤ºè¿‡æ»¤æ•ˆæœç¤ºä¾‹
    print(f"\\nğŸ“ åœç”¨è¯è¿‡æ»¤æ•ˆæœå¯¹æ¯”ï¼š")
    for i in range(3):
        if i < len(comments_cleaned):
            original_words = comments_cleaned.iloc[i]['WORDS']
            general_words = comments_cleaned.iloc[i]['WORDS_GENERAL']
            sentiment_words = comments_cleaned.iloc[i]['WORDS_SENTIMENT']
            topic_words = comments_cleaned.iloc[i]['WORDS_TOPIC']

            print(f"\\n{i + 1}. åŸå§‹åˆ†è¯ï¼š{' / '.join(original_words[:10])}...")
            print(f"   é€šç”¨è¿‡æ»¤ï¼š{' / '.join(general_words[:10])}...")
            print(f"   æƒ…æ„Ÿä¿ç•™ï¼š{' / '.join(sentiment_words[:10])}...")
            print(f"   ä¸»é¢˜è¿‡æ»¤ï¼š{' / '.join(topic_words[:10])}...")

    # åˆ†æé«˜é¢‘è¯å˜åŒ–
    print(f"\\nğŸ“Š åœç”¨è¯è¿‡æ»¤åçš„é«˜é¢‘è¯åˆ†æï¼š")

    # ç»Ÿè®¡é€šç”¨è¿‡æ»¤åçš„è¯é¢‘
    all_filtered_words = []
    for words_list in comments_cleaned['WORDS_GENERAL']:
        all_filtered_words.extend(words_list)

    if all_filtered_words:
        from collections import Counter

        filtered_word_freq = Counter(all_filtered_words)
        top_filtered_words = filtered_word_freq.most_common(10)

        print("ğŸ” åœç”¨è¯è¿‡æ»¤åTOP10è¯æ±‡ï¼š")
        for word, freq in top_filtered_words:
            print(f"â€¢ {word}: {freq} æ¬¡")

    print(f"\\nğŸ’¾ åœç”¨è¯è¿‡æ»¤ç»“æœå·²ä¿å­˜åˆ°å¯¹åº”åˆ—")
    print("âœ… å¯æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„è¿‡æ»¤ç»“æœ")

else:
    print("âŒ åˆ†è¯ç»“æœæœªå‡†å¤‡å¥½ï¼Œè·³è¿‡åœç”¨è¯å¤„ç†")

# æ•°æ®è´¨é‡å…¨é¢éªŒè¯
def comprehensive_data_quality_check(df, original_df=None):
    """
    å…¨é¢çš„æ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°
    """
    print("=== âœ… æ•°æ®è´¨é‡ç»¼åˆéªŒè¯æŠ¥å‘Š ===")

    quality_score = 0
    max_score = 100

    # 1. å®Œæ•´æ€§éªŒè¯ (30åˆ†)
    print("\\n1ï¸âƒ£ å®Œæ•´æ€§éªŒè¯:")
    missing_count = df.isnull().sum().sum()
    total_cells = len(df) * len(df.columns)
    completeness_rate = (1 - missing_count / total_cells) * 100

    print(f"â€¢ æ€»ç¼ºå¤±å€¼ï¼š{missing_count}")
    print(f"â€¢ å®Œæ•´æ€§ï¼š{completeness_rate:.1f}%")

    completeness_score = min(30, (completeness_rate / 100) * 30)
    quality_score += completeness_score
    print(f"â€¢ å®Œæ•´æ€§å¾—åˆ†ï¼š{completeness_score:.1f}/30")

    # 2. ä¸€è‡´æ€§éªŒè¯ (25åˆ†)
    print("\\n2ï¸âƒ£ ä¸€è‡´æ€§éªŒè¯:")
    consistency_issues = 0

    # æ£€æŸ¥æ•°æ®ç±»å‹ä¸€è‡´æ€§
    for col in df.columns:
        if df[col].dtype == 'object':
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸çš„æ•°æ®ç±»å‹æ··åˆ
            sample_values = df[col].dropna().head(100)
            types = set(type(val).__name__ for val in sample_values)
            if len(types) > 1:
                consistency_issues += 1
                print(f"â€¢ {col}åˆ—å­˜åœ¨æ··åˆæ•°æ®ç±»å‹ï¼š{types}")

    # æ£€æŸ¥æ–‡æœ¬æ ¼å¼ä¸€è‡´æ€§
    if 'CONTENT_CLEANED' in df.columns:
        cleaned_texts = df['CONTENT_CLEANED'].dropna()
        html_tags = cleaned_texts.str.contains('<[^>]+>', na=False).sum()
        if html_tags > 0:
            consistency_issues += 1
            print(f"â€¢ ä»æœ‰{html_tags}æ¡è®°å½•åŒ…å«HTMLæ ‡ç­¾")

    consistency_score = max(0, 25 - consistency_issues * 5)
    quality_score += consistency_score
    print(f"â€¢ ä¸€è‡´æ€§å¾—åˆ†ï¼š{consistency_score:.1f}/25")

    # 3. å‡†ç¡®æ€§éªŒè¯ (25åˆ†)
    print("\\n3ï¸âƒ£ å‡†ç¡®æ€§éªŒè¯:")
    accuracy_issues = 0

    # æ£€æŸ¥åˆ†è¯ç»“æœå‡†ç¡®æ€§
    if 'WORDS' in df.columns:
        # æ£€æŸ¥åˆ†è¯ç»“æœæ˜¯å¦åˆç†
        word_lengths = []
        for words_list in df['WORDS'].dropna():
            if isinstance(words_list, list):
                word_lengths.extend([len(word) for word in words_list])

        if word_lengths:
            avg_word_length = sum(word_lengths) / len(word_lengths)
            print(f"â€¢ å¹³å‡è¯é•¿ï¼š{avg_word_length:.1f}å­—ç¬¦")

            # ä¸­æ–‡è¯æ±‡å¹³å‡é•¿åº¦åº”è¯¥åœ¨1.5-3ä¹‹é—´
            if avg_word_length < 1.5 or avg_word_length > 4:
                accuracy_issues += 1
                print(f"â€¢ è­¦å‘Šï¼šå¹³å‡è¯é•¿å¼‚å¸¸ï¼Œå¯èƒ½å­˜åœ¨åˆ†è¯é—®é¢˜")

    # æ£€æŸ¥åœç”¨è¯è¿‡æ»¤æ•ˆæœ
    if 'WORDS_GENERAL' in df.columns:
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¸¸è§åœç”¨è¯
        common_stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰'}
        all_filtered_words = []
        for words_list in df['WORDS_GENERAL'].dropna():
            if isinstance(words_list, list):
                all_filtered_words.extend(words_list)

        remaining_stopwords = [word for word in all_filtered_words if word in common_stopwords]
        if len(remaining_stopwords) > len(all_filtered_words) * 0.05:  # è¶…è¿‡5%
            accuracy_issues += 1
            print(f"â€¢ è­¦å‘Šï¼šä»æœ‰è¾ƒå¤šåœç”¨è¯æœªè¿‡æ»¤")

    accuracy_score = max(0, 25 - accuracy_issues * 8)
    quality_score += accuracy_score
    print(f"â€¢ å‡†ç¡®æ€§å¾—åˆ†ï¼š{accuracy_score:.1f}/25")

    # 4. æœ‰æ•ˆæ€§éªŒè¯ (20åˆ†)
    print("\\n4ï¸âƒ£ æœ‰æ•ˆæ€§éªŒè¯:")
    validity_issues = 0

    # æ£€æŸ¥è¯„åˆ†èŒƒå›´æœ‰æ•ˆæ€§
    if 'RATING' in df.columns:
        numeric_ratings = pd.to_numeric(df['RATING'], errors='coerce')
        valid_ratings = numeric_ratings.dropna()
        if len(valid_ratings) > 0:
            invalid_count = ((valid_ratings < 1) | (valid_ratings > 5)).sum()
            if invalid_count > 0:
                validity_issues += 1
                print(f"â€¢ å‘ç°{invalid_count}ä¸ªæ— æ•ˆè¯„åˆ†")

    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦æœ‰æ•ˆæ€§
    if 'CONTENT_CLEANED' in df.columns:
        text_lengths = df['CONTENT_CLEANED'].astype(str).str.len()
        too_short = (text_lengths <= 2).sum()
        if too_short > len(df) * 0.02:  # è¶…è¿‡2%
            validity_issues += 1
            print(f"â€¢ è¿‡çŸ­æ–‡æœ¬è¿‡å¤šï¼š{too_short}æ¡")

    validity_score = max(0, 20 - validity_issues * 7)
    quality_score += validity_score
    print(f"â€¢ æœ‰æ•ˆæ€§å¾—åˆ†ï¼š{validity_score:.1f}/20")

    # æ€»ä½“è¯„åˆ†
    print(f"\\nğŸ† æ•°æ®è´¨é‡æ€»åˆ†ï¼š{quality_score:.1f}/{max_score}")

    if quality_score >= 90:
        grade = "A+ ä¼˜ç§€"
        emoji = "ğŸ¥‡"
    elif quality_score >= 80:
        grade = "A è‰¯å¥½"
        emoji = "ğŸ¥ˆ"
    elif quality_score >= 70:
        grade = "B åˆæ ¼"
        emoji = "ğŸ¥‰"
    else:
        grade = "C éœ€æ”¹è¿›"
        emoji = "âš ï¸"

    print(f"{emoji} è´¨é‡ç­‰çº§ï¼š{grade}")

    return quality_score


# è¿›è¡Œæ•°æ®è´¨é‡éªŒè¯
if 'comments_cleaned' in locals() and comments_cleaned is not None:

    # å¯¹æ¯”å¤„ç†å‰åçš„æ•°æ®é‡
    if 'comments_df' in locals() and comments_df is not None:
        print("ğŸ“Š æ•°æ®å¤„ç†å‰åå¯¹æ¯”ï¼š")
        print(f"â€¢ åŸå§‹æ•°æ®ï¼š{len(comments_df):,} æ¡")
        print(f"â€¢ å¤„ç†åæ•°æ®ï¼š{len(comments_cleaned):,} æ¡")
        print(f"â€¢ æ•°æ®ä¿ç•™ç‡ï¼š{len(comments_cleaned) / len(comments_df) * 100:.1f}%")

    # æ‰§è¡Œè´¨é‡æ£€æŸ¥
    quality_score = comprehensive_data_quality_check(comments_cleaned,
                                                     comments_df if 'comments_df' in locals() else None)

    # è¾“å‡ºæœ€ç»ˆæ•°æ®æ¦‚è§ˆ
    print(f"\\nğŸ“‹ æœ€ç»ˆæ•°æ®é›†æ¦‚è§ˆï¼š")
    print(f"â€¢ æ•°æ®è¡Œæ•°ï¼š{len(comments_cleaned):,}")
    print(f"â€¢ æ•°æ®åˆ—æ•°ï¼š{len(comments_cleaned.columns)}")
    print(f"â€¢ ä¸»è¦åˆ—ï¼š{list(comments_cleaned.columns)}")

    # å±•ç¤ºæœ€ç»ˆå¤„ç†åçš„æ•°æ®æ ·æœ¬
    print(f"\\nğŸ“ å¤„ç†åæ•°æ®æ ·æœ¬ï¼š")
    if len(comments_cleaned) > 0:
        for i in range(min(2, len(comments_cleaned))):
            row = comments_cleaned.iloc[i]
            print(f"\\næ ·æœ¬ {i + 1}:")
            print(f"â€¢ åŸè¯„è®ºï¼š{str(row.get('CONTENT', 'N/A'))[:60]}...")
            print(f"â€¢ æ¸…æ´—åï¼š{str(row.get('CONTENT_CLEANED', 'N/A'))[:60]}...")
            if 'WORDS_GENERAL' in row:
                words = row['WORDS_GENERAL']
                if isinstance(words, list) and len(words) > 0:
                    print(f"â€¢ å…³é”®è¯ï¼š{' / '.join(words[:8])}...")

    print(f"\\nğŸ‰ æ•°æ®é¢„å¤„ç†æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“Š æ•°æ®è´¨é‡è¯„åˆ†ï¼š{quality_score:.1f}/100")
    print(f"ğŸ’¾ æœ€ç»ˆæ•°æ®é›†å·²ä¿å­˜åœ¨ comments_cleaned å˜é‡ä¸­")

else:
    print("âŒ æ²¡æœ‰å¯éªŒè¯çš„æ•°æ®é›†")
