# ğŸ› ï¸ æ­¥éª¤1ï¼šç¯å¢ƒæ­å»ºä¸å·¥å…·å‡†å¤‡
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import re
from collections import Counter

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# å®‰è£…jiebaåˆ†è¯åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
try:
    import jieba
    print("âœ… jiebaåº“å·²å®‰è£…")
except ImportError:
    print("ğŸ“¦ æ­£åœ¨å®‰è£…jiebaåº“...")
    import subprocess
    subprocess.check_call(['pip', 'install', 'jieba'])
    import jieba
    print("âœ… jiebaåº“å®‰è£…å®Œæˆ")

print("ğŸ‰ ç¯å¢ƒé…ç½®å®Œæˆï¼æ‰€æœ‰å¿…è¦çš„åº“å·²å¯¼å…¥ã€‚")


# ğŸ“‚ æ­¥éª¤2ï¼šæ•°æ®åŠ è½½ä¸ç¼–ç å¤„ç†
def load_data_with_encoding():
    """å°è¯•ä¸åŒç¼–ç æ–¹å¼è¯»å–æ•°æ®"""
    encodings = ['utf-8', 'gbk', 'gb18030']

    for encoding in encodings:
        try:
            load_data_with_encoding_comments_df = pd.read_csv('douban-dataset/comments.csv', encoding=encoding)
            print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æˆåŠŸï¼")
            print(f"æ•°æ®å½¢çŠ¶ï¼š{load_data_with_encoding_comments_df.shape}")
            return load_data_with_encoding_comments_df
        except FileNotFoundError as e:
            print(f"âŒ {encoding} ç¼–ç å¤±è´¥: {e}")
            continue

    print("âŒ æ‰€æœ‰ç¼–ç æ–¹å¼éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
    return None


# åŠ è½½æ•°æ®
comments_df = load_data_with_encoding()

if comments_df is not None:
    print("\nğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š")
    print(comments_df.info())
    print(f"\nğŸ” å‰3è¡Œæ•°æ®ï¼š")
    print(comments_df.head(3))
    print(f"\nğŸ“‹ åˆ—åï¼š{list(comments_df.columns)}")
else:
    print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œç¼–ç ")


# ğŸ” æ­¥éª¤3ï¼šæ•°æ®è´¨é‡è¯Šæ–­
if comments_df is not None:
    print("=== ğŸ“Š æ•°æ®è´¨é‡è¯Šæ–­æŠ¥å‘Š ===")

    # 1. åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“‹ æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š")
    print(f"â€¢ æ•°æ®è¡Œæ•°ï¼š{len(comments_df):,}")
    print(f"â€¢ æ•°æ®åˆ—æ•°ï¼š{len(comments_df.columns)}")
    print(f"â€¢ æ•°æ®å¤§å°ï¼š{comments_df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB")

    # 2. ç¼ºå¤±å€¼æ£€æŸ¥
    print(f"\nğŸ•³ï¸ ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
    missing_stats = comments_df.isnull().sum()
    missing_percent = (missing_stats / len(comments_df) * 100).round(2)

    missing_summary = pd.DataFrame({
        'ç¼ºå¤±æ•°é‡': missing_stats,
        'ç¼ºå¤±æ¯”ä¾‹(%)': missing_percent
    })
    print(missing_summary[missing_summary['ç¼ºå¤±æ•°é‡'] > 0])

    # 3. é‡å¤å€¼æ£€æŸ¥
    duplicates = comments_df.duplicated().sum()
    print(f"\nğŸ”„ é‡å¤æ•°æ®ï¼š{duplicates:,} è¡Œ ({duplicates / len(comments_df) * 100:.2f}%)")

    # 4. æ•°æ®ç±»å‹æ£€æŸ¥
    print(f"\nğŸ·ï¸ æ•°æ®ç±»å‹ï¼š")
    for col in comments_df.columns:
        print(f"â€¢ {col}: {comments_df[col].dtype}")

    # 5. æ–‡æœ¬åˆ—åŸºæœ¬ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰CONTENTåˆ—ï¼‰
    if 'CONTENT' in comments_df.columns:
        content_stats = comments_df['CONTENT'].astype(str).str.len().describe()
        print(f"\nğŸ“ è¯„è®ºæ–‡æœ¬é•¿åº¦ç»Ÿè®¡ï¼š")
        print(f"â€¢ å¹³å‡é•¿åº¦ï¼š{content_stats['mean']:.1f} å­—ç¬¦")
        print(f"â€¢ æœ€çŸ­è¯„è®ºï¼š{content_stats['min']:.0f} å­—ç¬¦")
        print(f"â€¢ æœ€é•¿è¯„è®ºï¼š{content_stats['max']:.0f} å­—ç¬¦")
        print(f"â€¢ ä¸­ä½æ•°é•¿åº¦ï¼š{content_stats['50%']:.0f} å­—ç¬¦")

    # 6. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    rows_with_missing = comments_df.isnull().any(axis=1).sum()
    total_rows = len(comments_df)
    print(f"\nğŸ“ˆ æ•°æ®å®Œæ•´æ€§ï¼š")
    print(f"â€¢ æœ‰ç¼ºå¤±å€¼çš„è¡Œæ•°ï¼š{rows_with_missing:,}")
    print(f"â€¢ å®Œå…¨æ— ç¼ºå¤±çš„è¡Œæ•°ï¼š{total_rows - rows_with_missing:,}")
    print(f"â€¢ æ•°æ®å®Œæ•´åº¦ï¼š{((total_rows - rows_with_missing) / total_rows * 100):.1f}%")
else:
    print("âŒ æ•°æ®æœªåŠ è½½ï¼Œè·³è¿‡è´¨é‡è¯Šæ–­")


# ğŸ§¹ æ­¥éª¤4ï¼šç¼ºå¤±å€¼å¤„ç†
# åˆ¶ä½œæ•°æ®å¤‡ä»½ï¼Œé¿å…ä¿®æ”¹åŸæ•°æ®
comments_cleaned = comments_df.copy()

if comments_df is not None:
    print("=== ğŸ§¹ ç¼ºå¤±å€¼å¤„ç† ===")

    print("ğŸ“‹ å¤„ç†å‰çš„ç¼ºå¤±å€¼çŠ¶å†µï¼š")
    missing_before = comments_cleaned.isnull().sum()
    print(missing_before[missing_before > 0])

    print("\nğŸ”§ å¼€å§‹ç¼ºå¤±å€¼å¤„ç†...")

    # å¤„ç†ç­–ç•¥ï¼š
    # 1. å¯¹äºè¯„è®ºå†…å®¹CONTENTï¼Œå¦‚æœç¼ºå¤±åˆ™åˆ é™¤è¯¥è¡Œï¼ˆæ ¸å¿ƒæ•°æ®ä¸èƒ½ä¸ºç©ºï¼‰
    if 'CONTENT' in comments_cleaned.columns:
        before_count = len(comments_cleaned)
        comments_cleaned = comments_cleaned.dropna(subset=['CONTENT'])
        after_count = len(comments_cleaned)
        if before_count != after_count:
            print(f"âœ… åˆ é™¤äº† {before_count - after_count} æ¡æ— è¯„è®ºå†…å®¹çš„è®°å½•")

    # 2. å¯¹äºå…¶ä»–æ–‡æœ¬å­—æ®µï¼Œç”¨"æœªçŸ¥"å¡«è¡¥
    text_columns = comments_cleaned.select_dtypes(include=['object']).columns
    for col in text_columns:
        if comments_cleaned[col].isnull().sum() > 0:
            comments_cleaned[col] = comments_cleaned[col].fillna('æœªçŸ¥')
            print(f"âœ… {col}åˆ—çš„ç¼ºå¤±å€¼å·²ç”¨'æœªçŸ¥'å¡«è¡¥")

    # 3. å¯¹äºæ•°å€¼å­—æ®µï¼Œç”¨é€‚å½“çš„ç»Ÿè®¡å€¼å¡«è¡¥
    numeric_columns = comments_cleaned.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if comments_cleaned[col].isnull().sum() > 0:
            if col == 'RATING':  # è¯„åˆ†ç”¨ä¸­ä½æ•°å¡«è¡¥
                fill_value = comments_cleaned[col].median()
                comments_cleaned[col] = comments_cleaned[col].fillna(fill_value)
                print(f"âœ… {col}åˆ—çš„ç¼ºå¤±å€¼å·²ç”¨ä¸­ä½æ•° {fill_value} å¡«è¡¥")
            else:  # å…¶ä»–æ•°å€¼ç”¨å‡å€¼å¡«è¡¥
                fill_value = comments_cleaned[col].mean()
                comments_cleaned[col] = comments_cleaned[col].fillna(fill_value)
                print(f"âœ… {col}åˆ—çš„ç¼ºå¤±å€¼å·²ç”¨å‡å€¼ {fill_value:.2f} å¡«è¡¥")

    print("\nğŸ“Š å¤„ç†åçš„ç¼ºå¤±å€¼çŠ¶å†µï¼š")
    missing_after = comments_cleaned.isnull().sum()

    if missing_after.sum() == 0:
        print("ğŸ‰ æ‰€æœ‰ç¼ºå¤±å€¼å·²å¤„ç†å®Œæ¯•ï¼")
    else:
        print("å‰©ä½™ç¼ºå¤±å€¼ï¼š")
        print(missing_after[missing_after > 0])

    print(f"\nğŸ“ˆ ç¼ºå¤±å€¼å¤„ç†æ•ˆæœï¼š")
    print(f"â€¢ å¤„ç†å‰æ•°æ®é‡ï¼š{len(comments_df):,} è¡Œ")
    print(f"â€¢ å¤„ç†åæ•°æ®é‡ï¼š{len(comments_cleaned):,} è¡Œ")
    print(f"â€¢ æ•°æ®ä¿ç•™ç‡ï¼š{len(comments_cleaned) / len(comments_df) * 100:.1f}%")

else:
    print("âŒ æ•°æ®æœªåŠ è½½ï¼Œè·³è¿‡ç¼ºå¤±å€¼å¤„ç†")


# ğŸ”„ æ­¥éª¤5ï¼šé‡å¤æ•°æ®å¤„ç†
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== ğŸ”„ é‡å¤æ•°æ®æ£€æµ‹ä¸å¤„ç† ===")

    print(f"ğŸ“Š å¤„ç†å‰æ•°æ®é‡ï¼š{len(comments_cleaned):,} æ¡")

    # 1. å®Œå…¨é‡å¤è®°å½•æ£€æµ‹
    duplicated_all = comments_cleaned.duplicated()
    duplicate_count_all = duplicated_all.sum()
    print(f"\nğŸ” å®Œå…¨é‡å¤çš„è®°å½•æ•°ï¼š{duplicate_count_all:,} ({duplicate_count_all / len(comments_cleaned) * 100:.2f}%)")

    # 2. å†…å®¹é‡å¤æ£€æµ‹ï¼ˆåŒä¸€ç”¨æˆ·å¯¹åŒä¸€ç”µå½±çš„é‡å¤è¯„è®ºï¼‰
    if 'CREATOR' in comments_cleaned.columns and 'MOVIEID' in comments_cleaned.columns and 'CONTENT' in comments_cleaned.columns:
        content_duplicated = comments_cleaned.duplicated(subset=['CREATOR', 'MOVIEID', 'CONTENT'])
        content_duplicate_count = content_duplicated.sum()
        print(
            f"ğŸ” å†…å®¹é‡å¤çš„è®°å½•æ•°ï¼š{content_duplicate_count:,} ({content_duplicate_count / len(comments_cleaned) * 100:.2f}%)")

    # 3. å»é‡å¤„ç†
    print(f"\nğŸ”§ å¼€å§‹å»é‡å¤„ç†...")

    # åˆ é™¤å®Œå…¨é‡å¤çš„è®°å½•
    comments_deduped = comments_cleaned.drop_duplicates()
    step1_removed = len(comments_cleaned) - len(comments_deduped)
    print(f"âœ… åˆ é™¤å®Œå…¨é‡å¤è®°å½•ï¼š{step1_removed} æ¡")

    # åˆ é™¤å†…å®¹é‡å¤çš„è®°å½•ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
    if 'ADD_TIME' in comments_deduped.columns:
        # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
        comments_deduped = comments_deduped.sort_values('ADD_TIME').drop_duplicates(
            subset=['CREATOR', 'MOVIEID', 'CONTENT'], keep='last'
        )
    else:
        # å¦‚æœæ²¡æœ‰æ—¶é—´å­—æ®µï¼Œå°±ä¿ç•™ç¬¬ä¸€æ¡
        comments_deduped = comments_deduped.drop_duplicates(
            subset=['CREATOR', 'MOVIEID', 'CONTENT'], keep='first'
        )

    step2_removed = len(comments_cleaned) - step1_removed - len(comments_deduped)
    print(f"âœ… åˆ é™¤å†…å®¹é‡å¤è®°å½•ï¼š{step2_removed} æ¡")

    # 4. å»é‡æ•ˆæœç»Ÿè®¡
    total_removed = len(comments_cleaned) - len(comments_deduped)
    print(f"\nğŸ“ˆ å»é‡å¤„ç†æ€»ç»“ï¼š")
    print(f"â€¢ åŸå§‹æ•°æ®ï¼š{len(comments_cleaned):,} æ¡")
    print(f"â€¢ å»é‡åæ•°æ®ï¼š{len(comments_deduped):,} æ¡")
    print(f"â€¢ æ€»å…±å»é™¤ï¼š{total_removed:,} æ¡é‡å¤è®°å½•")
    print(f"â€¢ æ•°æ®ä¿ç•™ç‡ï¼š{len(comments_deduped) / len(comments_cleaned) * 100:.1f}%")

    # æ›´æ–°æ¸…æ´—åçš„æ•°æ®
    comments_cleaned = comments_deduped

else:
    print("âŒ æ¸…æ´—åçš„æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡é‡å¤æ•°æ®å¤„ç†")


# ğŸš¨ æ­¥éª¤6ï¼šå¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== ğŸš¨ å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç† ===")

    # 1. æ–‡æœ¬é•¿åº¦å¼‚å¸¸æ£€æµ‹
    if 'CONTENT' in comments_cleaned.columns:
        print("ğŸ“ æ–‡æœ¬é•¿åº¦å¼‚å¸¸æ£€æµ‹ï¼š")
        comments_cleaned['text_length'] = comments_cleaned['CONTENT'].astype(str).str.len()

        length_stats = comments_cleaned['text_length'].describe()
        print(f"â€¢ å¹³å‡è¯„è®ºé•¿åº¦ï¼š{length_stats['mean']:.1f} å­—ç¬¦")
        print(f"â€¢ æœ€çŸ­è¯„è®ºï¼š{length_stats['min']:.0f} å­—ç¬¦")
        print(f"â€¢ æœ€é•¿è¯„è®ºï¼š{length_stats['max']:.0f} å­—ç¬¦")
        print(f"â€¢ ä¸­ä½æ•°é•¿åº¦ï¼š{length_stats['50%']:.0f} å­—ç¬¦")

        # æ£€æµ‹è¿‡çŸ­è¯„è®ºï¼ˆå¯èƒ½æ˜¯æ— æ•ˆè¯„è®ºï¼‰
        very_short = comments_cleaned[comments_cleaned['text_length'] <= 3]
        print(f"â€¢ è¿‡çŸ­è¯„è®ºï¼ˆâ‰¤3å­—ç¬¦ï¼‰ï¼š{len(very_short)} æ¡ ({len(very_short) / len(comments_cleaned) * 100:.2f}%)")
        if len(very_short) > 0:
            print(f"  ç¤ºä¾‹ï¼š{list(very_short['CONTENT'].head(3))}")

        # æ£€æµ‹è¿‡é•¿è¯„è®ºï¼ˆå¯èƒ½æ˜¯å¼‚å¸¸ï¼‰
        q99 = comments_cleaned['text_length'].quantile(0.99)
        very_long = comments_cleaned[comments_cleaned['text_length'] > q99]
        print(
            f"â€¢ è¿‡é•¿è¯„è®ºï¼ˆ>99%åˆ†ä½æ•° {q99:.0f}å­—ç¬¦ï¼‰ï¼š{len(very_long)} æ¡ ({len(very_long) / len(comments_cleaned) * 100:.2f}%)")

    # 2. é‡å¤å­—ç¬¦å¼‚å¸¸æ£€æµ‹
    print(f"\nğŸ” æ–‡æœ¬å†…å®¹å¼‚å¸¸æ£€æµ‹ï¼š")

    def has_excessive_repetition(text_to_check_repetition):
        """æ£€æµ‹æ˜¯å¦æœ‰è¿‡å¤šé‡å¤å­—ç¬¦"""
        if pd.isna(text_to_check_repetition):
            return False
        text_to_check_repetition = str(text_to_check_repetition)
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­4ä¸ªä»¥ä¸Šç›¸åŒå­—ç¬¦
        pattern = r'(.)\\1{3,}'
        return bool(re.search(pattern, text_to_check_repetition))

    def calculate_special_char_ratio(text_to_calc_special_char_ratio):
        """è®¡ç®—ç‰¹æ®Šå­—ç¬¦å æ¯”"""
        if pd.isna(text_to_calc_special_char_ratio):
            return 0
        text_to_calc_special_char_ratio = str(text_to_calc_special_char_ratio)
        if len(text_to_calc_special_char_ratio) == 0:
            return 0

        # è®¡ç®—éä¸­æ–‡ã€éè‹±æ–‡ã€éæ•°å­—å­—ç¬¦çš„æ¯”ä¾‹
        special_chars = 0
        for char in text_to_calc_special_char_ratio:
            if not (char.isalnum() or '\\u4e00' <= char <= '\\u9fff'):
                special_chars += 1

        return special_chars / len(text_to_calc_special_char_ratio)

    # æ£€æµ‹é‡å¤å­—ç¬¦å¼‚å¸¸
    repetitive_mask = comments_cleaned['CONTENT'].apply(has_excessive_repetition)
    repetitive_count = repetitive_mask.sum()
    print(f"â€¢ åŒ…å«è¿‡å¤šé‡å¤å­—ç¬¦çš„è¯„è®ºï¼š{repetitive_count} æ¡ ({repetitive_count / len(comments_cleaned) * 100:.2f}%)")

    # æ£€æµ‹ç‰¹æ®Šå­—ç¬¦å¼‚å¸¸
    comments_cleaned['special_char_ratio'] = comments_cleaned['CONTENT'].apply(calculate_special_char_ratio)
    high_special_char = comments_cleaned[comments_cleaned['special_char_ratio'] > 0.3]
    print(
        f"â€¢ ç‰¹æ®Šå­—ç¬¦å æ¯”>30%çš„è¯„è®ºï¼š{len(high_special_char)} æ¡ ({len(high_special_char) / len(comments_cleaned) * 100:.2f}%)")

    # 3. å¼‚å¸¸å€¼å¤„ç†å†³ç­–
    print(f"\nğŸ”§ å¼‚å¸¸å€¼å¤„ç†ï¼š")

    # åˆ é™¤è¿‡çŸ­çš„æ— æ•ˆè¯„è®ºï¼ˆâ‰¤2ä¸ªå­—ç¬¦ï¼‰
    before_count = len(comments_cleaned)
    comments_cleaned = comments_cleaned[comments_cleaned['text_length'] > 2]
    removed_short = before_count - len(comments_cleaned)
    if removed_short > 0:
        print(f"âœ… åˆ é™¤è¿‡çŸ­è¯„è®ºï¼š{removed_short} æ¡")

    # å¯é€‰ï¼šåˆ é™¤ç‰¹æ®Šå­—ç¬¦å æ¯”è¿‡é«˜çš„è¯„è®ºï¼ˆå¯èƒ½æ˜¯ä¹±ç ï¼‰
    before_count = len(comments_cleaned)
    comments_cleaned = comments_cleaned[comments_cleaned['special_char_ratio'] <= 0.5]
    removed_special = before_count - len(comments_cleaned)
    if removed_special > 0:
        print(f"âœ… åˆ é™¤ç‰¹æ®Šå­—ç¬¦è¿‡å¤šçš„è¯„è®ºï¼š{removed_special} æ¡")

    print(f"\nğŸ“ˆ å¼‚å¸¸å€¼å¤„ç†æ€»ç»“ï¼š")
    print(f"â€¢ å¤„ç†åæ•°æ®é‡ï¼š{len(comments_cleaned):,} æ¡")
    print(f"â€¢ æ€»è®¡åˆ é™¤å¼‚å¸¸è®°å½•ï¼š{removed_short + removed_special} æ¡")

else:
    print("âŒ æ¸…æ´—åçš„æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡å¼‚å¸¸å€¼æ£€æµ‹")


# ğŸ“ æ­¥éª¤7ï¼šæ–‡æœ¬æ·±åº¦æ¸…æ´—
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== ğŸ“ NLPæ–‡æœ¬æ·±åº¦æ¸…æ´— ===")

    def clean_text_comprehensive(text_to_clean):
        """
        ç»¼åˆæ–‡æœ¬æ¸…æ´—å‡½æ•° - NLPé¢„å¤„ç†æ ¸å¿ƒæ­¥éª¤
        """
        if pd.isna(text_to_clean) or text_to_clean is None:
            return ""

        text_to_clean = str(text_to_clean)

        # 1. å»é™¤HTMLæ ‡ç­¾å’Œå®ä½“
        text_to_clean = re.sub(r'<[^>]+>', '', text_to_clean)
        text_to_clean = re.sub(r'&[a-zA-Z]+;', ' ', text_to_clean)
        text_to_clean = re.sub(r'&lt;|&gt;', '', text_to_clean)

        # 2. å»é™¤ç‰¹æ®Šç¬¦å·å’Œå™ªå£°å­—ç¬¦
        text_to_clean = re.sub(r'[â˜…â˜†â€»@#$%^&*]', '', text_to_clean)
        text_to_clean = re.sub(r'[ï¿½â–¡\ue000-\uf8ff]', '', text_to_clean)

        # 3. å¤„ç†è¡¨æƒ…ç¬¦å·ï¼ˆä¿ç•™éƒ¨åˆ†æƒ…æ„Ÿä¿¡æ¯ï¼‰
        text_to_clean = re.sub(r'[\U0001F600-\U0001F64F]', '[è¡¨æƒ…]', text_to_clean)
        text_to_clean = re.sub(r'[\U0001F300-\U0001F5FF]', '', text_to_clean)

        # 4. æ ‡å‡†åŒ–ç½‘ç»œç”¨è¯­
        text_to_clean = re.sub(r'h{3,}', 'å“ˆå“ˆ', text_to_clean, flags=re.IGNORECASE)
        text_to_clean = re.sub(r'2333+', 'å“ˆå“ˆ', text_to_clean)
        text_to_clean = re.sub(r'6{4,}', 'å‰å®³', text_to_clean)

        # 5. å»é™¤è¿‡åº¦é‡å¤çš„å­—ç¬¦
        text_to_clean = re.sub(r'(.)\\1{3,}', r'\\1\\1', text_to_clean)
        text_to_clean = re.sub(r'[ï¼!]{3,}', 'ï¼ï¼', text_to_clean)
        text_to_clean = re.sub(r'[ï¼Ÿ?]{3,}', 'ï¼Ÿï¼Ÿ', text_to_clean)
        text_to_clean = re.sub(r'[ã€‚.]{3,}', '...', text_to_clean)

        # 6. æ¸…ç†ç©ºç™½å­—ç¬¦
        text_to_clean = re.sub(r'\\s+', ' ', text_to_clean)
        text_to_clean = text_to_clean.strip()

        return text_to_clean


    # åº”ç”¨æ–‡æœ¬æ¸…æ´—
    print("ğŸ”§ å¼€å§‹æ–‡æœ¬æ¸…æ´—å¤„ç†...")

    # å±•ç¤ºæ¸…æ´—æ•ˆæœç¤ºä¾‹
    if 'CONTENT' in comments_cleaned.columns:
        # éšæœºé€‰æ‹©å‡ ä¸ªè¯„è®ºå±•ç¤ºæ¸…æ´—æ•ˆæœ
        sample_comments = comments_cleaned['CONTENT'].dropna().sample(n=min(3, len(comments_cleaned)), random_state=42)

        print("\\nğŸ” æ–‡æœ¬æ¸…æ´—ç¤ºä¾‹ï¼š")
        for i, (idx, original) in enumerate(sample_comments.items(), 1):
            cleaned = clean_text_comprehensive(original)
            print(f"{i}. åŸæ–‡ï¼š{str(original)[:80]}...")
            print(f"   æ¸…æ´—åï¼š{cleaned[:80]}...")
            print()

        # æ‰¹é‡æ¸…æ´—æ‰€æœ‰è¯„è®º
        print("ğŸ’¾ æ‰¹é‡æ¸…æ´—æ‰€æœ‰è¯„è®ºæ–‡æœ¬...")
        comments_cleaned['CONTENT_CLEANED'] = comments_cleaned['CONTENT'].apply(clean_text_comprehensive)

        # ç»Ÿè®¡æ¸…æ´—æ•ˆæœ
        original_avg_len = comments_cleaned['CONTENT'].astype(str).str.len().mean()
        cleaned_avg_len = comments_cleaned['CONTENT_CLEANED'].str.len().mean()
        length_reduction = (1 - cleaned_avg_len / original_avg_len) * 100

        print(f"\\nğŸ“Š æ–‡æœ¬æ¸…æ´—æ•ˆæœç»Ÿè®¡ï¼š")
        print(f"â€¢ æ¸…æ´—å‰å¹³å‡é•¿åº¦ï¼š{original_avg_len:.1f} å­—ç¬¦")
        print(f"â€¢ æ¸…æ´—åå¹³å‡é•¿åº¦ï¼š{cleaned_avg_len:.1f} å­—ç¬¦")
        print(f"â€¢ å¹³å‡é•¿åº¦å‡å°‘ï¼š{length_reduction:.1f}%")

        # æ£€æŸ¥æ¸…æ´—åçš„ç©ºæ–‡æœ¬
        empty_after_clean = np.sum(comments_cleaned['CONTENT_CLEANED'].str.len() == 0)
        print(f"â€¢ æ¸…æ´—åå˜ä¸ºç©ºçš„æ–‡æœ¬ï¼š{empty_after_clean} æ¡")

        # è¿‡æ»¤æ‰æ¸…æ´—åä¸ºç©ºçš„æ–‡æœ¬
        if empty_after_clean > 0:
            before_filter = len(comments_cleaned)
            comments_cleaned = comments_cleaned[comments_cleaned['CONTENT_CLEANED'].str.len() > 0]
            print(f"âœ… åˆ é™¤æ¸…æ´—åä¸ºç©ºçš„è®°å½•ï¼š{before_filter - len(comments_cleaned)} æ¡")

        print(f"âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆï¼å½“å‰æ•°æ®é‡ï¼š{len(comments_cleaned):,} æ¡")

else:
    print("âŒ æ¸…æ´—åçš„æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡æ–‡æœ¬æ¸…æ´—")


# âœ‚ï¸ æ­¥éª¤8ï¼šä¸­æ–‡åˆ†è¯å¤„ç†
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== âœ‚ï¸ ä¸­æ–‡åˆ†è¯å¤„ç† ===")

    # æ·»åŠ ç”µå½±é¢†åŸŸè‡ªå®šä¹‰è¯æ±‡
    movie_terms = [
        "å¤ä»‡è€…è”ç›Ÿ", "é’¢é“ä¾ ", "ç¾å›½é˜Ÿé•¿", "é»‘å¯¡å¦‡", "é›·ç¥", "ç»¿å·¨äºº",
        "æ¼«å¨", "DC", "è¯ºå…°", "æ¼«å¨ç”µå½±å®‡å®™", "ç‰¹æ•ˆ", "å‰§æƒ…", "æ¼”æŠ€",
        "å¯¼æ¼”", "ç¼–å‰§", "é…ä¹", "ç¥¨æˆ¿", "å£ç¢‘", "è±†ç“£", "è¯„åˆ†"
    ]

    for term in movie_terms:
        jieba.add_word(term)

    print(f"âœ… å·²æ·»åŠ  {len(movie_terms)} ä¸ªç”µå½±é¢†åŸŸä¸“ä¸šè¯æ±‡")


    def tokenize_text(text_to_tokenize):
        """
        ä¸­æ–‡åˆ†è¯å¤„ç†å‡½æ•°
        """
        if pd.isna(text_to_tokenize) or not text_to_tokenize.strip():
            return []

        # ä½¿ç”¨jiebaç²¾ç¡®æ¨¡å¼åˆ†è¯
        tokenized_words = jieba.lcut(str(text_to_tokenize).strip())

        # è¿‡æ»¤é•¿åº¦å°äº2çš„è¯å’Œçº¯æ ‡ç‚¹ç¬¦å·
        filtered_tokenized_words = []
        for tokenized_word in tokenized_words:
            tokenized_word = tokenized_word.strip()
            if (len(tokenized_word) >= 2 and
                    not re.match(r'^[^\w\u4e00-\u9fff]+$', tokenized_word)):  # ä¸æ˜¯çº¯æ ‡ç‚¹ç¬¦å·
                filtered_tokenized_words.append(tokenized_word)

        return filtered_tokenized_words


    # å±•ç¤ºåˆ†è¯æ•ˆæœ
    if 'CONTENT_CLEANED' in comments_cleaned.columns:
        print("\\nğŸ” åˆ†è¯æ•ˆæœæ¼”ç¤ºï¼š")

        # é€‰æ‹©å‡ ä¸ªæ ·æœ¬å±•ç¤ºåˆ†è¯æ•ˆæœ
        sample_texts = comments_cleaned['CONTENT_CLEANED'].dropna().sample(n=min(3, len(comments_cleaned)),
                                                                           random_state=42)

        for i, (idx, text) in enumerate(sample_texts.items(), 1):
            if len(text) > 10:  # åªå¤„ç†æœ‰å†…å®¹çš„æ–‡æœ¬
                words = tokenize_text(text)
                print(f"{i}. åŸæ–‡ï¼š{text[:60]}...")
                print(f"   åˆ†è¯ï¼š{' / '.join(words[:15])}...")
                print(f"   è¯æ•°ï¼š{len(words)}")
                print()

        # æ‰¹é‡åˆ†è¯å¤„ç†
        print("ğŸ’¾ æ‰¹é‡åˆ†è¯å¤„ç†ä¸­...")
        comments_cleaned['WORDS'] = comments_cleaned['CONTENT_CLEANED'].apply(tokenize_text)

        # ç»Ÿè®¡åˆ†è¯æ•ˆæœ
        word_counts = comments_cleaned['WORDS'].apply(len)
        avg_words = word_counts.mean()

        print(f"\\nğŸ“Š åˆ†è¯å¤„ç†æ•ˆæœç»Ÿè®¡ï¼š")
        print(f"â€¢ å¹³å‡æ¯æ¡è¯„è®ºè¯æ•°ï¼š{avg_words:.1f}")
        print(f"â€¢ æœ€å¤šè¯æ•°ï¼š{word_counts.max()}")
        print(f"â€¢ æœ€å°‘è¯æ•°ï¼š{word_counts.min()}")
        print(f"â€¢ ä¸­ä½æ•°è¯æ•°ï¼š{word_counts.median():.1f}")

        # ç»Ÿè®¡é«˜é¢‘è¯æ±‡ï¼ˆå‰å¤„ç†ï¼‰
        all_words = []
        for words_list in comments_cleaned['WORDS'].head(1000):  # å–å‰1000æ¡è¿›è¡Œè¯é¢‘ç»Ÿè®¡
            all_words.extend(words_list)

        if all_words:
            word_freq = Counter(all_words)
            print(f"\\nğŸ” é«˜é¢‘è¯æ±‡ï¼ˆå‰10ï¼‰ï¼š")
            for word, freq in word_freq.most_common(10):
                print(f"â€¢ {word}: {freq} æ¬¡")

        print(f"âœ… åˆ†è¯å¤„ç†å®Œæˆï¼")

else:
    print("âŒ æ¸…æ´—åçš„æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ†è¯å¤„ç†")


# ğŸš« æ­¥éª¤9ï¼šåœç”¨è¯è¿‡æ»¤
if 'comments_cleaned' in locals() and comments_cleaned is not None and 'WORDS' in comments_cleaned.columns:
    print("=== ğŸš« åœç”¨è¯è¿‡æ»¤å¤„ç† ===")

    # æ„å»ºåœç”¨è¯è¡¨
    def create_stopwords():
        """åˆ›å»ºé€‚åˆç”µå½±è¯„è®ºçš„åœç”¨è¯è¡¨"""
        basic_stopwords = {
            # åŸºç¡€åœç”¨è¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™æ ·', 'ç°åœ¨', 'æ¯”å¦‚', 'ä»€ä¹ˆ', 'å¦‚æœ', 'è¿˜æ˜¯', 'åªæ˜¯', 'è¿™ä¸ª', 'é‚£ä¸ª',
            'å¯ä»¥', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥', 'è™½ç„¶', 'ç„¶å', 'è€Œä¸”', 'æˆ–è€…',
            # è¯­æ°”è¯
            'å§', 'å‘¢', 'å•Š', 'å—¯', 'å“¦', 'å‘€', 'å˜›', 'å‘',
            # ä»£è¯
            'è¿™', 'é‚£', 'è¿™äº›', 'é‚£äº›', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬',
            # ä»‹è¯
            'ä»', 'å‘', 'å¯¹', 'å¯¹äº', 'å…³äº', 'ç”±äº', 'é€šè¿‡', 'æ ¹æ®',
            # ç”µå½±è¯„è®ºå¸¸è§åœç”¨è¯
            'ç”µå½±', 'å½±ç‰‡', 'ç‰‡å­', 'è¿™éƒ¨', 'è§‚çœ‹', 'çœ‹äº†', 'çœ‹è¿‡', 'æ„Ÿè§‰', 'è§‰å¾—',
            'è®¤ä¸º', 'ä¸ªäºº', 'æˆ‘è§‰å¾—', 'æˆ‘è®¤ä¸º', 'æˆ‘æ„Ÿè§‰', 'ä¸€éƒ¨', 'ä¸€ä¸ª', 'çœŸçš„'
        }

        # ä¸ºäº†æƒ…æ„Ÿåˆ†æï¼Œä¿ç•™ä¸€äº›ç¨‹åº¦å‰¯è¯
        sentiment_words = {'å¾ˆ', 'éå¸¸', 'ç‰¹åˆ«', 'ååˆ†', 'è¶…çº§', 'å¤ª', 'æœ€', 'æå…¶', 'ç›¸å½“'}

        return basic_stopwords - sentiment_words


    stopwords = create_stopwords()
    print(f"ğŸ“‹ åœç”¨è¯è¡¨å¤§å°ï¼š{len(stopwords)} ä¸ªè¯")


    def remove_stopwords(words_list_to_remove):
        """å»é™¤åœç”¨è¯"""
        if not words_list_to_remove:
            return []

        return [word_in_list for word_in_list in words_list_to_remove if word_in_list not in stopwords]


    # å±•ç¤ºåœç”¨è¯è¿‡æ»¤æ•ˆæœ
    print("\\nğŸ” åœç”¨è¯è¿‡æ»¤æ¼”ç¤ºï¼š")

    sample_indices = comments_cleaned[comments_cleaned['WORDS'].apply(len) > 5].sample(n=min(3, len(comments_cleaned)),
                                                                                       random_state=42).index

    for i, idx in enumerate(sample_indices, 1):
        original_words = comments_cleaned.loc[idx, 'WORDS']
        filtered_words = remove_stopwords(original_words)

        print(f"{i}. åŸå§‹åˆ†è¯ï¼š{' / '.join(original_words)}")
        print(f"   è¿‡æ»¤åœç”¨è¯ï¼š{' / '.join(filtered_words)}")
        print(
            f"   è¯æ•°å˜åŒ–ï¼š{len(original_words)} â†’ {len(filtered_words)} (å‡å°‘{len(original_words) - len(filtered_words)}è¯)")
        print()

    # æ‰¹é‡å¤„ç†åœç”¨è¯è¿‡æ»¤
    print("ğŸ’¾ æ‰¹é‡åœç”¨è¯è¿‡æ»¤ä¸­...")
    comments_cleaned['WORDS_FILTERED'] = comments_cleaned['WORDS'].apply(remove_stopwords)

    # ç»Ÿè®¡åœç”¨è¯è¿‡æ»¤æ•ˆæœ
    original_word_counts = comments_cleaned['WORDS'].apply(len)
    filtered_word_counts = comments_cleaned['WORDS_FILTERED'].apply(len)

    avg_reduction = (original_word_counts.mean() - filtered_word_counts.mean()) / original_word_counts.mean() * 100

    print(f"\\nğŸ“Š åœç”¨è¯è¿‡æ»¤æ•ˆæœç»Ÿè®¡ï¼š")
    print(f"â€¢ è¿‡æ»¤å‰å¹³å‡è¯æ•°ï¼š{original_word_counts.mean():.1f}")
    print(f"â€¢ è¿‡æ»¤åå¹³å‡è¯æ•°ï¼š{filtered_word_counts.mean():.1f}")
    print(f"â€¢ å¹³å‡è¯æ•°å‡å°‘ï¼š{avg_reduction:.1f}%")

    # ç»Ÿè®¡è¿‡æ»¤åçš„é«˜é¢‘è¯æ±‡
    all_filtered_words = []
    for words_list in comments_cleaned['WORDS_FILTERED'].head(1000):
        all_filtered_words.extend(words_list)

    if all_filtered_words:
        filtered_word_freq = Counter(all_filtered_words)
        print(f"\\nğŸ” åœç”¨è¯è¿‡æ»¤åé«˜é¢‘è¯æ±‡ï¼ˆå‰10ï¼‰ï¼š")
        for word, freq in filtered_word_freq.most_common(10):
            print(f"â€¢ {word}: {freq} æ¬¡")

    # è¿‡æ»¤æ‰åœç”¨è¯å¤„ç†åä¸ºç©ºçš„è®°å½•
    empty_after_filter = np.sum(comments_cleaned['WORDS_FILTERED'].apply(len) == 0)
    if empty_after_filter > 0:
        before_filter = len(comments_cleaned)
        comments_cleaned = comments_cleaned[comments_cleaned['WORDS_FILTERED'].apply(len) > 0]
        print(f"\\nâœ… åˆ é™¤åœç”¨è¯è¿‡æ»¤åä¸ºç©ºçš„è®°å½•ï¼š{before_filter - len(comments_cleaned)} æ¡")

    print(f"âœ… åœç”¨è¯è¿‡æ»¤å®Œæˆï¼å½“å‰æ•°æ®é‡ï¼š{len(comments_cleaned):,} æ¡")

else:
    print("âŒ åˆ†è¯æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡åœç”¨è¯å¤„ç†")


# âœ… æ­¥éª¤10ï¼šæœ€ç»ˆæ•°æ®è´¨é‡éªŒè¯
if 'comments_cleaned' in locals() and comments_cleaned is not None:
    print("=== âœ… æœ€ç»ˆæ•°æ®è´¨é‡éªŒè¯ ===")

    retention_rate = 0
    # 1. æ•°æ®é‡å˜åŒ–ç»Ÿè®¡
    if 'comments_df' in locals() and comments_df is not None:
        original_count = len(comments_df)
        final_count = len(comments_cleaned)
        retention_rate = final_count / original_count * 100

        print(f"ğŸ“Š æ•°æ®å¤„ç†æ€»ç»“ï¼š")
        print(f"â€¢ åŸå§‹æ•°æ®é‡ï¼š{original_count:,} æ¡")
        print(f"â€¢ æœ€ç»ˆæ•°æ®é‡ï¼š{final_count:,} æ¡")
        print(f"â€¢ æ•°æ®ä¿ç•™ç‡ï¼š{retention_rate:.1f}%")
        print(f"â€¢ æ€»è®¡æ¸…ç†ï¼š{original_count - final_count:,} æ¡")

    # 2. æ•°æ®å®Œæ•´æ€§éªŒè¯
    print(f"\\nğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼š")
    missing_check = comments_cleaned.isnull().sum()
    if missing_check.sum() == 0:
        print("âœ… æ²¡æœ‰ç¼ºå¤±å€¼")
    else:
        print("âš ï¸ ä»æœ‰ç¼ºå¤±å€¼ï¼š")
        print(missing_check[missing_check > 0])

    print(comments_cleaned.head(5))

    # 3. é‡å¤æ•°æ®éªŒè¯
    # æ’é™¤WORDSå’ŒWORDS_FILTEREDåˆ—æ¥æ£€æµ‹é‡å¤æ•°æ®
    cols_to_exclude = ['WORDS', 'WORDS_FILTERED']
    cols_to_check = [col for col in comments_cleaned.columns if col not in cols_to_exclude]
    remaining_duplicates = comments_cleaned[cols_to_check].duplicated().sum()
    print(f"ğŸ”„ é‡å¤æ•°æ®æ£€æŸ¥ï¼š{remaining_duplicates} æ¡ (åº”ä¸º0)")

    # 4. æ–‡æœ¬è´¨é‡éªŒè¯
    if 'CONTENT_CLEANED' in comments_cleaned.columns:
        print(f"\\nğŸ“ æ–‡æœ¬è´¨é‡æ£€æŸ¥ï¼š")

        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
        text_lengths = comments_cleaned['CONTENT_CLEANED'].str.len()
        print(f"â€¢ å¹³å‡æ–‡æœ¬é•¿åº¦ï¼š{text_lengths.mean():.1f} å­—ç¬¦")
        print(f"â€¢ æœ€çŸ­æ–‡æœ¬ï¼š{text_lengths.min()} å­—ç¬¦")
        print(f"â€¢ æœ€é•¿æ–‡æœ¬ï¼š{text_lengths.max()} å­—ç¬¦")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ–‡æœ¬
        empty_texts = np.sum(text_lengths == 0)
        print(f"â€¢ ç©ºæ–‡æœ¬æ•°é‡ï¼š{empty_texts} æ¡ (åº”ä¸º0)")

    # 5. åˆ†è¯è´¨é‡éªŒè¯
    if 'WORDS_FILTERED' in comments_cleaned.columns:
        print(f"\\nâœ‚ï¸ åˆ†è¯è´¨é‡æ£€æŸ¥ï¼š")

        word_counts = comments_cleaned['WORDS_FILTERED'].apply(len)
        print(f"â€¢ å¹³å‡è¯æ•°ï¼š{word_counts.mean():.1f}")
        print(f"â€¢ æœ€å°‘è¯æ•°ï¼š{word_counts.min()}")
        print(f"â€¢ æœ€å¤šè¯æ•°ï¼š{word_counts.max()}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºè¯åˆ—è¡¨
        empty_words = np.sum(word_counts == 0)
        print(f"â€¢ ç©ºè¯åˆ—è¡¨æ•°é‡ï¼š{empty_words} æ¡ (åº”ä¸º0)")

    # 6. æ•°æ®é‡‡æ ·æ£€æŸ¥
    print(f"\\nğŸ¯ æœ€ç»ˆæ•°æ®é‡‡æ ·æ£€æŸ¥ï¼š")

    if len(comments_cleaned) > 0:
        sample_data = comments_cleaned.sample(n=min(3, len(comments_cleaned)), random_state=42)

        for i, (idx, row) in enumerate(sample_data.iterrows(), 1):
            print(f"\\næ ·æœ¬ {i}:")
            print(f"  åŸå§‹è¯„è®ºï¼š{str(row.get('CONTENT', 'N/A'))[:50]}...")
            if 'CONTENT_CLEANED' in row:
                print(f"  æ¸…æ´—åï¼š{str(row['CONTENT_CLEANED'])[:50]}...")
            if 'WORDS_FILTERED' in row and row['WORDS_FILTERED']:
                print(f"  å…³é”®è¯ï¼š{' / '.join(row['WORDS_FILTERED'][:8])}...")

    # 7. æ•°æ®è´¨é‡è¯„åˆ†
    print(f"\\nğŸ† æ•°æ®è´¨é‡ç»¼åˆè¯„åˆ†ï¼š")

    score = 100
    issues = []

    # æ£€æŸ¥å„é¡¹æŒ‡æ ‡
    if 'comments_df' in locals():
        if retention_rate < 80:
            score -= 20
            issues.append("æ•°æ®ä¿ç•™ç‡åä½")
        elif retention_rate < 90:
            score -= 10
            issues.append("æ•°æ®ä¿ç•™ç‡ä¸€èˆ¬")

    if missing_check.sum() > 0:
        score -= 15
        issues.append("ä»æœ‰ç¼ºå¤±å€¼")

    if remaining_duplicates > 0:
        score -= 10
        issues.append("ä»æœ‰é‡å¤æ•°æ®")

    if 'CONTENT_CLEANED' in comments_cleaned.columns:
        if np.sum(comments_cleaned['CONTENT_CLEANED'].str.len() == 0) > 0:
            score -= 15
            issues.append("å­˜åœ¨ç©ºæ–‡æœ¬")

    if score >= 90:
        grade = "A+ ä¼˜ç§€"
        emoji = "ğŸ¥‡"
    elif score >= 80:
        grade = "A è‰¯å¥½"
        emoji = "ğŸ¥ˆ"
    elif score >= 70:
        grade = "B åˆæ ¼"
        emoji = "ğŸ¥‰"
    else:
        grade = "C éœ€æ”¹è¿›"
        emoji = "âš ï¸"

    print(f"{emoji} ç»¼åˆè´¨é‡è¯„åˆ†ï¼š{score}/100 ({grade})")

    if issues:
        print(f"ğŸ”§ éœ€è¦å…³æ³¨çš„é—®é¢˜ï¼š")
        for issue in issues:
            print(f"  â€¢ {issue}")
    else:
        print("ğŸ‰ æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œæ— æ˜æ˜¾é—®é¢˜ï¼")

    # 8. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    print(f"\\nğŸ’¾ æ•°æ®å¤„ç†å®Œæˆæ€»ç»“ï¼š")
    print(f"â€¢ æœ€ç»ˆæ¸…æ´—åæ•°æ®å·²ä¿å­˜åœ¨ comments_cleaned å˜é‡ä¸­")
    print(f"â€¢ ä¸»è¦å­—æ®µï¼š")
    for col in comments_cleaned.columns:
        print(f"  - {col}: {comments_cleaned[col].dtype}")

    print(f"\\nğŸŠ æ­å–œï¼æ•°æ®é¢„å¤„ç†æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“ˆ å¤„ç†åçš„æ•°æ®å¯ä»¥ç”¨äºåç»­çš„æœºå™¨å­¦ä¹ ã€æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜å»ºæ¨¡ç­‰ä»»åŠ¡")

else:
    print("âŒ æ²¡æœ‰å¯éªŒè¯çš„æ•°æ®")
