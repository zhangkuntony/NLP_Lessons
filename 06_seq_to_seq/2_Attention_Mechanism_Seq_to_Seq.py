# è§£å†³OpenMPå†²çªé—®é¢˜
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import jieba
from collections import Counter

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ä¸­æ–‡æ˜¾ç¤ºé…ç½®
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼")

#
# """
# ğŸ¨ å¯è§†åŒ–æ¼”ç¤ºï¼šåŸºç¡€Seq2Seqæ¨¡å‹çš„é—®é¢˜
#
# è¿™ä¸ªå‡½æ•°ç”¨ç”ŸåŠ¨çš„å›¾è¡¨æ¥å±•ç¤ºä¼ ç»ŸSeq2Seqæ¨¡å‹çš„ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼š
# 1. ä¿¡æ¯ç“¶é¢ˆé—®é¢˜ï¼šæ‰€æœ‰ä¿¡æ¯è¢«å‹ç¼©åˆ°ä¸€ä¸ªå°å‘é‡ä¸­
# 2. å¯¹é½é—®é¢˜ï¼šæ¨¡å‹ä¸çŸ¥é“è¾“å‡ºè¯å¯¹åº”å“ªä¸ªè¾“å…¥è¯
# """
#
# def visualize_basic_seq2seq_problem():
#     """ç”¨å›¾è¡¨å±•ç¤ºåŸºç¡€çš„seq2seqçš„é—®é¢˜ï¼Œè®©æŠ½è±¡æ¦‚å¿µå˜å¾—ç›´è§‚æ˜“æ‡‚"""
#
#     # åˆ›å»ºä¸¤ä¸ªå­å›¾ï¼šä¸Šæ–¹å±•ç¤ºä¿¡æ¯ç“¶é¢ˆï¼Œä¸‹æ–¹å±•ç¤ºå¯¹é½é—®é¢˜
#     fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))
#
#     # ===================ç¬¬ä¸€ä¸ªå›¾ï¼šä¿¡æ¯ç“¶é¢ˆé—®é¢˜===================
#     ax1.set_title('é—®é¢˜1ï¼šä¿¡æ¯ç“¶é¢ˆ - å°±åƒæŠŠæ•´æœ¬ä¹¦çš„å†…å®¹å†™åœ¨ä¾¿æ¡çº¸ä¸Š',
#                   fontsize=16, fontweight='bold', pad=20)
#
#     # ç”¨ä¸€ä¸ªé•¿å¥å­æ¥æ¼”ç¤ºä¿¡æ¯ç“¶é¢ˆé—®é¢˜
#     input_words = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']
#     print(f"ğŸ“ ç¤ºä¾‹å¥å­ï¼š{' '.join(input_words)}")
#     print("   è¿™ä¸ªå¥å­æœ‰9ä¸ªè¯ï¼ŒåŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ï¼ˆé¢œè‰²ã€åŠ¨ä½œã€å¯¹è±¡ç­‰ï¼‰")
#
#     # ç»˜åˆ¶è¾“å…¥è¯æ±‡ï¼ˆç”¨è“è‰²æ–¹å—è¡¨ç¤ºï¼‰
#     for i, word in enumerate(input_words):
#         # æ¯ä¸ªè¯ç”¨ä¸€ä¸ªå°æ–¹å—è¡¨ç¤º
#         rect = plt.Rectangle((i-0.3, 3), 0.6, 0.6, facecolor='lightblue', edgecolor='blue')
#         ax1.add_patch(rect)
#         ax1.text(i, 3.3, word, ha='center', va='center', fontsize=10)
#
#     # ç»˜åˆ¶ä¿¡æ¯æµåŠ¨ç®­å¤´ï¼ˆå±•ç¤ºåºåˆ—å¤„ç†è¿‡ç¨‹ï¼‰
#     for i in range(len(input_words) - 1):
#         ax1.arrow(i + 0.3, 3.3, 0.4, 0, head_width=0.05, head_length=0.05, fc='gray', ec='gray')
#
#     # ç»˜åˆ¶ç“¶é¢ˆå‘é‡ï¼ˆç”¨çº¢è‰²æ–¹å—è¡¨ç¤ºå‹ç¼©åçš„ä¿¡æ¯ï¼‰
#     ax1.add_patch(plt.Rectangle((4, 1.5), 1, 0.8, facecolor='red', edgecolor='darkred'))
#     ax1.text(4.5, 1.9, 'è¯­ä¹‰å‘é‡\n(ä¿¡æ¯ç“¶é¢ˆ)', ha='center', va='center',
#              fontsize=12, fontweight='bold', color='white')
#
#     # ç»˜åˆ¶å‹ç¼©ç®­å¤´ï¼ˆæ‰€æœ‰ä¿¡æ¯æ±‡èšåˆ°ä¸€ä¸ªå‘é‡ï¼‰
#     ax1.arrow(8.3, 3, -3.5, -0.7, head_width=0.08, head_length=0.1, fc='red', ec='red', linewidth=2)
#     ax1.text(6, 2.5, 'æ‰€æœ‰ä¿¡æ¯\nè¢«å‹ç¼©ï¼', ha='center', va='center',
#              fontsize=11, color='red', fontweight='bold')
#
#     # æ·»åŠ è¯´æ˜æ–‡å­—
#     ax1.text(1, 0.8, 'é—®é¢˜ï¼š9ä¸ªè¯çš„ä¸°å¯Œä¿¡æ¯ â†’ 1ä¸ªå›ºå®šå¤§å°çš„å‘é‡',
#              fontsize=12, fontweight='bold', color='red')
#     ax1.text(1, 0.5, '   å°±åƒæŠŠä¸€æ•´æœ¬å°è¯´å‹ç¼©æˆä¸€å¥è¯ï¼',
#              fontsize=10, color='darkred')
#
#     ax1.set_xlim(-1, 10)
#     ax1.set_ylim(0.3, 4)
#     ax1.axis('off')
#
#     # ===================ç¬¬äºŒä¸ªå›¾ï¼šå¯¹é½é—®é¢˜===================
#     ax2.set_title('é—®é¢˜2ï¼šæ— æ³•å¯¹é½ - ä¸çŸ¥é“å“ªä¸ªä¸­æ–‡è¯å¯¹åº”å“ªä¸ªè‹±æ–‡è¯',
#                   fontsize=16, fontweight='bold', pad=20)
#
#     # ç”¨ç®€å•çš„ç¿»è¯‘ä¾‹å­æ¼”ç¤ºå¯¹é½é—®é¢˜
#     en_words = ['I', 'love', 'machine', 'learning']
#     zh_words = ['æˆ‘', 'å–œæ¬¢', 'æœºå™¨', 'å­¦ä¹ ']
#
#     print(f"\nç¿»è¯‘ç¤ºä¾‹ï¼š")
#     print(f"   è‹±æ–‡ï¼š{' '.join(en_words)}")
#     print(f"   ä¸­æ–‡ï¼š{''.join(zh_words)}")
#     print(f"   é—®é¢˜ï¼šæ¨¡å‹ä¸çŸ¥é“'machine'å¯¹åº”'æœºå™¨'")
#
#     # ç»˜åˆ¶è‹±æ–‡è¯æ±‡ï¼ˆç»¿è‰²æ–¹å—ï¼‰
#     for i, word in enumerate(en_words):
#         rect = plt.Rectangle((i * 2 - 0.4, 2.5), 0.8, 0.6, facecolor='lightgreen', edgecolor='green')
#         ax2.add_patch(rect)
#         ax2.text(i * 2, 2.8, word, ha='center', va='center', fontsize=12)
#
#     # ç»˜åˆ¶ä¸­æ–‡è¯æ±‡ï¼ˆé»„è‰²æ–¹å—ï¼‰
#     for i, word in enumerate(zh_words):
#         rect = plt.Rectangle((i * 2 - 0.4, 0.5), 0.8, 0.6, facecolor='lightyellow', edgecolor='orange')
#         ax2.add_patch(rect)
#         ax2.text(i * 2, 0.8, word, ha='center', va='center', fontsize=12)
#
#     # ç»˜åˆ¶æ­£ç¡®çš„å¯¹åº”å…³ç³»ï¼ˆç»¿è‰²è™šçº¿è¡¨ç¤ºç†æƒ³çš„å¯¹åº”ï¼‰
#     correct_alignments = [(0, 0), (1, 1), (2, 2), (3, 3)]
#     for en_idx, zh_idx in correct_alignments:
#         ax2.plot([en_idx * 2, zh_idx * 2], [2.5, 1.1], 'g--', linewidth=2, alpha=0.7)
#
#     # æ·»åŠ é—®å·å’Œè¯´æ˜ï¼ˆè¡¨ç¤ºæ¨¡å‹çš„å›°æƒ‘ï¼‰
#     ax2.text(3, 1.7, 'ï¼Ÿ', fontsize=30, ha='center', va='center')
#     ax2.text(5, 1.7, 'æ¨¡å‹ä¸çŸ¥é“\nå¯¹åº”å…³ç³»ï¼', ha='center', va='center',
#              fontsize=12, color='red', fontweight='bold')
#
#     # æ·»åŠ è¯´æ˜æ–‡å­—
#     ax2.text(0.5, 3.3, 'US è‹±æ–‡è¾“å…¥', fontsize=12, fontweight='bold', color='green')
#     ax2.text(0.5, 0.1, 'CN ä¸­æ–‡è¾“å‡º', fontsize=12, fontweight='bold', color='orange')
#     ax2.text(4.5, 1.3, 'è™šçº¿ = ç†æƒ³çš„å¯¹åº”å…³ç³»', fontsize=10, color='green')
#
#     ax2.set_xlim(-1, 8)
#     ax2.set_ylim(0, 3.5)
#     ax2.axis('off')
#
#     plt.tight_layout()
#     plt.show()
#
# # è¿è¡Œå¯è§†åŒ–æ¼”ç¤º
# print("ğŸ¨ å¼€å§‹å¯è§†åŒ–æ¼”ç¤ºåŸºç¡€Seq2Seqçš„é—®é¢˜...")
# print("=" * 60)
# visualize_basic_seq2seq_problem()
# print("=" * 60)
# print("ğŸ¯ çœ‹åˆ°è¿™äº›é—®é¢˜äº†å—ï¼Ÿè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ³¨æ„åŠ›æœºåˆ¶ï¼")
# print("ğŸ’¡ æ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜è€Œè¯ç”Ÿçš„ï¼")
#
#
# # å¯è§†åŒ–æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†
# def visualize_attention_mechanism():
#     fig, ax = plt.subplots(1, 1, figsize=(15, 8))
#
#     # è¾“å…¥åºåˆ—
#     input_words = ['The', 'red', 'car', 'is', 'very', 'fast']
#     output_words = ['è¿™', 'çº¢è‰²', 'æ±½è½¦', 'éå¸¸', 'å¿«']
#
#     # åˆ›å»ºæ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼ˆç¤ºä¾‹ï¼‰
#     attention_weights = np.array([
#         [0.8, 0.1, 0.05, 0.03, 0.01, 0.01],  # ç¿»è¯‘"è¿™"æ—¶ä¸»è¦å…³æ³¨"The"
#         [0.1, 0.8, 0.05, 0.02, 0.02, 0.01],  # ç¿»è¯‘"çº¢è‰²"æ—¶ä¸»è¦å…³æ³¨"red"
#         [0.05, 0.1, 0.8, 0.03, 0.01, 0.01],  # ç¿»è¯‘"æ±½è½¦"æ—¶ä¸»è¦å…³æ³¨"car"
#         [0.02, 0.02, 0.03, 0.1, 0.8, 0.03],  # ç¿»è¯‘"éå¸¸"æ—¶ä¸»è¦å…³æ³¨"very"
#         [0.01, 0.01, 0.02, 0.06, 0.1, 0.8],  # ç¿»è¯‘"å¿«"æ—¶ä¸»è¦å…³æ³¨"fast"
#     ])
#
#     # ç»˜åˆ¶çƒ­åŠ›å›¾
#     im = ax.imshow(attention_weights, cmap='Reds', aspect='auto')
#
#     # è®¾ç½®æ ‡ç­¾
#     ax.set_xticks(range(len(input_words)))
#     ax.set_yticks(range(len(output_words)))
#     ax.set_xticklabels(input_words, fontsize=14)
#     ax.set_yticklabels(output_words, fontsize=14)
#
#     # æ·»åŠ æ•°å€¼æ ‡æ³¨
#     for i in range(len(output_words)):
#         for j in range(len(input_words)):
#             text = ax.text(j, i, f'{attention_weights[i, j]:.2f}',
#                            ha="center", va="center", color="black" if attention_weights[i, j] < 0.5 else "white",
#                            fontsize=10, fontweight='bold')
#
#     # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
#     ax.set_title('æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼šæ¨¡å‹å…³æ³¨çš„ç„¦ç‚¹', fontsize=16, fontweight='bold', pad=20)
#     ax.set_xlabel('è¾“å…¥è¯ï¼ˆè‹±æ–‡ï¼‰', fontsize=14, fontweight='bold')
#     ax.set_ylabel('è¾“å‡ºè¯ï¼ˆä¸­æ–‡ï¼‰', fontsize=14, fontweight='bold')
#
#     # æ·»åŠ é¢œè‰²æ¡
#     cbar = plt.colorbar(im, ax=ax)
#     cbar.set_label('æ³¨æ„åŠ›æƒé‡', fontsize=12, fontweight='bold')
#
#     # æ·»åŠ è§£é‡Šæ–‡å­—
#     ax.text(len(input_words) + 0.5, len(output_words) // 2,
#             'é¢œè‰²è¶Šæ·±\n= å…³æ³¨åº¦è¶Šé«˜\n\næ¯è¡Œæƒé‡\nä¹‹å’Œä¸º1',
#             fontsize=12, ha='center', va='center',
#             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
#
#     plt.tight_layout()
#     plt.show()
#
# visualize_attention_mechanism()
# print("ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹çŸ¥é“è¯¥å…³æ³¨ä»€ä¹ˆï¼")
#
#
# """
# ğŸ”§ åŠ¨æ‰‹å®ç°ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒç®—æ³•
#
# è®©æˆ‘ä»¬æŠŠåˆšæ‰å­¦åˆ°çš„"é€‰æœ‹å‹å¸®å¿™"ç†è®ºè½¬æ¢æˆå®é™…çš„ä»£ç ï¼
# è¿™ä¸ªç®€å•çš„æ³¨æ„åŠ›æ¨¡å—å®Œç¾æ¼”ç¤ºäº†ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤ã€‚
# """
#
# class SimpleAttention(nn.Module):
#     """
#     ç®€å•æ³¨æ„åŠ›æ¨¡å— - æŠŠç†è®ºå˜æˆä»£ç ï¼
#
#     è¿™å°±æ˜¯æˆ‘ä»¬åˆšæ‰è®¨è®ºçš„"é€‰æœ‹å‹å¸®å¿™"ç®—æ³•çš„ä»£ç å®ç°ï¼š
#     1. è¯„ä¼°æ¯ä¸ªæœ‹å‹çš„æœ‰ç”¨ç¨‹åº¦
#     2. å†³å®šå‘æ¯ä¸ªæœ‹å‹æ±‚åŠ©çš„æ¯”ä¾‹
#     3. ç»¼åˆæ‰€æœ‰æœ‹å‹çš„å»ºè®®
#     """
#
#     def __init__(self, hidden_dim):
#         super(SimpleAttention, self).__init__()
#         self.hidden_dim = hidden_dim
#         print(f"ğŸ—ï¸ åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—ï¼Œéšè—ç»´åº¦: {hidden_dim}")
#
#     def forward(self, decoder_hidden, encoder_outputs):
#         """
#         æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒè®¡ç®—è¿‡ç¨‹
#
#         å‚æ•°è§£é‡Šï¼ˆç”¨æˆ‘ä»¬çš„æ¯”å–»ï¼‰:
#             decoder_hidden: ä½ å½“å‰çš„"å›°æƒ‘çŠ¶æ€" [batch_size, hidden_dim]
#                           ï¼ˆä½ è¦ç¿»è¯‘ä»€ä¹ˆè¯ï¼Ÿä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿï¼‰
#             encoder_outputs: æ‰€æœ‰æœ‹å‹çš„"ä¸“ä¸šçŸ¥è¯†" [batch_size, seq_len, hidden_dim]
#                            ï¼ˆæ¯ä¸ªè¾“å…¥è¯èƒ½æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿï¼‰
#
#         è¿”å›ç»“æœ:
#             context_vector: "ç»¼åˆæ‰€æœ‰å»ºè®®çš„æœ€ç»ˆç­”æ¡ˆ" [batch_size, hidden_dim]
#             attention_weights: "å‘æ¯ä¸ªæœ‹å‹æ±‚åŠ©çš„æ¯”ä¾‹" [batch_size, seq_len]
#         """
#
#         print(f"\nğŸ§  å¼€å§‹æ³¨æ„åŠ›è®¡ç®—...")
#         print(f"   å½“å‰çŠ¶æ€å½¢çŠ¶: {decoder_hidden.shape}")
#         print(f"   è¾“å…¥ä¿¡æ¯å½¢çŠ¶: {encoder_outputs.shape}")
#
#         # =================== æ­¥éª¤1: è¯„ä¼°æœ‹å‹çš„æœ‰ç”¨ç¨‹åº¦ ===================
#         print("\nğŸ¯ æ­¥éª¤1: è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆè¯„ä¼°æœ‹å‹æœ‰ç”¨ç¨‹åº¦ï¼‰")
#
#         # ä½¿ç”¨ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦ - å°±åƒé—®â€œä½ çš„ä¸“é•¿å’Œæˆ‘çš„éœ€æ±‚æœ‰å¤šåŒ¹é…ï¼â€
#         # æ•°å­¦åŸç†ï¼šç‚¹ç§¯è¶Šå¤§ = å‘é‡è¶Šç›¸ä¼¼ = æœ‹å‹è¶Šæœ‰ç”¨
#         scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2))
#         scores = scores.squeeze(2)          # [batch_size, seq_len]
#
#         print(f"   ç›¸å…³æ€§åˆ†æ•°: {scores.squeeze().detach().numpy()}")
#         print("   ğŸ’¡ åˆ†æ•°è¶Šé«˜ = æœ‹å‹è¶Šæœ‰ç”¨ï¼")
#
#         # =================== æ­¥éª¤2: åˆ†é…æ³¨æ„åŠ›æ¯”ä¾‹ ===================
#         print("\nğŸ“Š æ­¥éª¤2: è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆåˆ†é…æ±‚åŠ©æ¯”ä¾‹ï¼‰")
#
#         # ä½¿ç”¨softmaxç¡®ä¿æ‰€æœ‰æƒé‡åŠ èµ·æ¥= 100%
#         # å°±åƒæŠŠè¯„åˆ†è½¬æ¢æˆç™¾åˆ†æ¯”åˆ†é…
#
#         attention_weights = F.softmax(scores, dim=1)            # [batch_size, seq_len]
#
#         print(f"   æ³¨æ„åŠ›æƒé‡: {attention_weights.squeeze().detach().numpy()}")
#         print(f"   æƒé‡æ€»å’Œ: {attention_weights.sum().item():.4f} (åº”è¯¥=1.0)")
#         print("   ğŸ’¡ è¿™å°±æ˜¯ç»™æ¯ä¸ªæœ‹å‹åˆ†é…çš„æ³¨æ„åŠ›æ¯”ä¾‹ï¼")
#
#         # =================== æ­¥éª¤3: ç»¼åˆæ‰€æœ‰å»ºè®® ===================
#         print("\nğŸ¤ æ­¥éª¤3: è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ï¼ˆç»¼åˆæœ‹å‹å»ºè®®ï¼‰")
#
#         # åŠ æƒå¹³å‡ - æŒ‰æ¯”ä¾‹ç»„åˆæ‰€æœ‰æœ‹å‹çš„å»ºè®®
#         # æƒé‡é«˜çš„æœ‹å‹ï¼Œä»–çš„å»ºè®®å½±å“æ›´å¤§
#         context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
#         context_vector = context_vector.squeeze(1)              # [batch_size, hidden_dim]
#
#         print(f"   æœ€ç»ˆä¸Šä¸‹æ–‡å‘é‡å½¢çŠ¶: {context_vector.shape}")
#         print("   ğŸ’¡ è¿™å°±æ˜¯ç»¼åˆæ‰€æœ‰æœ‹å‹å»ºè®®åçš„æœ€ç»ˆç­”æ¡ˆï¼")
#
#         return context_vector, attention_weights
#
# def demonstrate_attention():
#     """
#     ğŸª æ³¨æ„åŠ›æœºåˆ¶ç°åœºæ¼”ç¤º
#
#     ç”¨å…·ä½“çš„æ•°å­—æ¥å±•ç¤ºæ³¨æ„åŠ›æœºåˆ¶æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œ
#     è®©æŠ½è±¡çš„æ¦‚å¿µå˜å¾—å…·ä½“å¯è§ï¼
#     """
#
#     print("ğŸ¬ æ³¨æ„åŠ›æœºåˆ¶ç°åœºæ¼”ç¤ºå¼€å§‹ï¼")
#     print("=" * 60)
#
#     # è®¾ç½®æ¼”ç¤ºå‚æ•°
#     batch_size = 1          # ä¸€æ¬¡å¤„ç†ä¸€ä¸ªå¥å­
#     seq_len = 4             # è¾“å…¥å¥å­æœ‰4ä¸ªè¯ï¼ˆæ¯”å¦‚ "I love machine learning"ï¼‰
#     hidden_dim = 8          # æ¯ä¸ªè¯ç”¨8ç»´å‘é‡è¡¨ç¤º
#
#     print(f"ğŸ“ æ¼”ç¤ºè®¾ç½®ï¼š")
#     print(f"   å¥å­é•¿åº¦: {seq_len}ä¸ªè¯")
#     print(f"   å‘é‡ç»´åº¦: {hidden_dim}ç»´")
#     print(f"   æƒ³è±¡å¥å­: 'I love machine learning'")
#     print(f"   è¦ç¿»è¯‘çš„è¯: 'machine' â†’ 'æœºå™¨'")
#
#     # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼ˆè¿™äº›é€šå¸¸æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒå‡ºæ¥çš„ï¼‰
#     print("\nğŸ² åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
#     torch.manual_seed(42)           # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
#
#     # è§£ç å™¨å½“å‰çŠ¶æ€ï¼šè¡¨ç¤ºâ€œæˆ‘ç°åœ¨è¦ç¿»è¯‘machineè¿™ä¸ªè¯â€
#     decoder_hidden = torch.randn(batch_size, hidden_dim)
#     print(f"    è§£ç å™¨çŠ¶æ€ï¼ˆè¦ç¿»è¯‘'machine'ï¼‰ï¼šå·²åˆ›å»º")
#
#     # ç¼–ç å™¨è¾“å‡ºï¼šè¡¨ç¤ºæ¯ä¸ªè¾“å…¥è¯çš„ä¿¡æ¯
#     encoder_outputs = torch.randn(batch_size, seq_len, hidden_dim)
#     print(f"    ç¼–ç å™¨è¾“å‡ºï¼ˆ4ä¸ªè¯çš„ä¿¡æ¯ï¼‰ï¼šå·²åˆ›å»º")
#
#     # åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—
#     print("\nğŸ—ï¸ åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—...")
#     attention = SimpleAttention(hidden_dim)
#
#     # ğŸ¬ å¼€å§‹æ³¨æ„åŠ›è®¡ç®—ï¼
#     print("\nğŸš€ å¼€å§‹æ³¨æ„åŠ›è®¡ç®—...")
#     context_vector, attention_weights = attention(decoder_hidden, encoder_outputs)
#
#     # ğŸ“Š ç»“æœåˆ†æ
#     print("\n" + "="*60)
#     print("ğŸ“Š è®¡ç®—ç»“æœåˆ†æï¼š")
#     print("="*60)
#
#     weights_array = attention_weights.squeeze().detach().numpy()
#
#     # åˆ›å»ºè¯æ±‡æ ‡ç­¾ï¼ˆæ¨¡æ‹Ÿï¼‰
#     word_labels = ['I', 'love', 'machine', 'learning']
#
#     print("\nğŸ¯ æ³¨æ„åŠ›æƒé‡åˆ†æï¼š")
#     for i, (word, weight) in enumerate(zip(word_labels, weights_array)):
#         percentage = weight * 100
#         bar = "â–ˆ" * int(percentage // 5)                # ç®€å•çš„æ¡å½¢å›¾
#         print(f"   {word:>8}: {weight:.3f} ({percentage:5.1f}%) {bar}")
#
#     # æ‰¾å‡ºæœ€å…³æ³¨çš„è¯
#     max_idx = weights_array.argmax()
#     max_word = word_labels[max_idx]
#     max_weight = weights_array[max_idx]
#
#     print(f"\nğŸ’¡ æ¨¡å‹æœ€å…³æ³¨: '{max_word}' (æƒé‡: {max_weight:.3f})")
#     print(f"   è¿™è¯´æ˜ç¿»è¯‘'machine'æ—¶ï¼Œæ¨¡å‹ä¸»è¦å‚è€ƒ'{max_word}'è¿™ä¸ªè¯ï¼")
#
#     # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
#     print("\nğŸ¨ ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨...")
#     plt.figure(figsize=(14, 5))
#
#     # å·¦å›¾ï¼šæ¡å½¢å›¾
#     plt.subplot(1, 3, 1)
#     colors = ['lightblue' if i != max_idx else 'red' for i in range(len(weights_array))]
#     bars = plt.bar(range(len(weights_array)), weights_array, color=colors, edgecolor='navy')
#     plt.title('æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
#     plt.xlabel('è¾“å…¥è¯')
#     plt.ylabel('æ³¨æ„åŠ›æƒé‡')
#     plt.xticks(range(len(word_labels)), word_labels, rotation=45)
#     plt.ylim(0, 1)
#     plt.grid(True, alpha=0.3)
#
#     # æ·»åŠ æ•°å€¼æ ‡ç­¾
#     for bar, weight in zip(bars, weights_array):
#         plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
#                  f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')
#
#     # ä¸­å›¾ï¼šé¥¼å›¾
#     plt.subplot(1, 3, 2)
#     colors_pie = ['lightblue', 'lightgreen', 'red', 'lightyellow']
#     plt.pie(weights_array, labels=word_labels, autopct='%1.1f%%',
#             colors=colors_pie, startangle=90)
#     plt.title('æ³¨æ„åŠ›æƒé‡æ¯”ä¾‹', fontsize=14, fontweight='bold')
#
#     # å³å›¾ï¼šçƒ­åŠ›å›¾
#     plt.subplot(1, 3, 3)
#     weights_matrix = weights_array.reshape(1, -1)
#     plt.imshow(weights_matrix, cmap='Reds', aspect='auto')
#     plt.title('æ³¨æ„åŠ›çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
#     plt.xticks(range(len(word_labels)), word_labels)
#     plt.yticks([0], ['attention'])
#
#     # æ·»åŠ æ•°å€¼
#     for i, weight in enumerate(weights_array):
#         plt.text(i, 0, f'{weight:.2f}', ha='center', va='center',
#                  color='white' if weight > 0.5 else 'black', fontweight='bold')
#
#     plt.tight_layout()
#     plt.show()
#
#     print("\n" + "=" * 60)
#     print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
#     print("âœ… è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒè®¡ç®—è¿‡ç¨‹ï¼")
#     print("ğŸ’¡ æ¨¡å‹å­¦ä¼šäº†åŠ¨æ€åœ°é€‰æ‹©æœ€ç›¸å…³çš„è¾“å…¥ä¿¡æ¯ï¼")
#     print("=" * 60)
#
# # ğŸ¬ å¼€å§‹æ¼”ç¤ºï¼
# print("ğŸª æ¬¢è¿æ¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ç°åœºæ¼”ç¤ºï¼")
# demonstrate_attention()


# åŠ¨æ‰‹å®è·µï¼šæ„å»ºå®Œæ•´çš„æ³¨æ„åŠ›ç¿»è¯‘ç³»ç»Ÿ
"""
ğŸ“Š ç¬¬ä¸€æ­¥ï¼šæ•°æ®å‡†å¤‡ - ä¸ºæˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººå‡†å¤‡"å­¦ä¹ ææ–™"

å°±åƒæ•™å­©å­å­¦è‹±è¯­éœ€è¦å‡†å¤‡è¯¾æœ¬ä¸€æ ·ï¼Œæˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººä¹Ÿéœ€è¦å¤§é‡çš„ä¸­è‹±å¯¹ç…§å¥å­æ¥å­¦ä¹ ã€‚
è¿™ä¸€æ­¥æˆ‘ä»¬å°†ä»cmn.txtæ–‡ä»¶ä¸­åŠ è½½çœŸå®çš„ç¿»è¯‘æ•°æ®é›†ï¼Œå¹¶æŠŠæ–‡å­—è½¬æ¢æˆæœºå™¨èƒ½ç†è§£çš„æ•°å­—ã€‚
"""

import re
import string
class EnhancedTranslationDatast:
    """
    å¢å¼ºç‰ˆç¿»è¯‘æ•°æ®é›† - æˆ‘ä»¬çš„"ç”µå­è¯¾æœ¬"

    è¿™ä¸ªç±»çš„ä½œç”¨å°±åƒä¸€ä¸ªæ™ºèƒ½çš„è¯­è¨€è¯¾æœ¬ï¼Œå®ƒèƒ½ï¼š
    1. ä»cmn.txtæ–‡ä»¶ä¸­åŠ è½½å¤§é‡çš„ä¸­è‹±å¯¹ç…§å¥å­
    2. æŠŠæ–‡å­—è½¬æ¢æˆæœºå™¨èƒ½ç†è§£çš„æ•°å­—
    3. ä¸ºè®­ç»ƒè¿‡ç¨‹æä¾›è§„æ•´çš„æ•°æ®
    """

    def __init__(self, data_file="cmn.txt", max_pairs=5000):
        print("ğŸ“š æ­£åœ¨å‡†å¤‡ç¿»è¯‘æ•°æ®é›†...")
        print(f"ğŸ“ ä»æ–‡ä»¶ {data_file} åŠ è½½æ•°æ®...")

        # ä»cmn.txtæ–‡ä»¶ä¸­åŠ è½½æ•°æ®
        self.pairs = self.load_data_from_file(data_file, max_pairs)

        print(f"ğŸ“ æˆåŠŸåŠ è½½äº† {len(self.pairs)} å¯¹ä¸­è‹±å¥å­")
        print("ğŸ’¡ è¿™äº›å¥å­æ¥è‡ªçœŸå®çš„ç¿»è¯‘æ•°æ®é›†")

        # å¼€å§‹æ„å»ºè¯æ±‡è¡¨
        self.prepare_vocabularies()

    def load_data_from_file(self, filename, max_pairs=5000):
        """
        ä»cmn.txtæ–‡ä»¶ä¸­åŠ è½½ç¿»è¯‘æ•°æ®

        cmn.txtæ–‡ä»¶æ ¼å¼é€šå¸¸æ˜¯ï¼š
        è‹±æ–‡å¥å­ \t ä¸­æ–‡å¥å­ \t å…¶ä»–ä¿¡æ¯
        """
        pairs = []

        try:
            with open(filename, 'r', encoding='utf-8') as file:
                print("ğŸ“– æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶...")

                for line_num, line in enumerate(file):
                    if line_num >= max_pairs:
                        break

                    # å»é™¤æ¢è¡Œç¬¦å¹¶åˆ†å‰²
                    line = line.strip()
                    if not line:
                        continue

                    # é€šå¸¸cmn.txtæ ¼å¼æ˜¯ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”çš„
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        en_sentence = parts[0].strip()
                        zh_sentence = parts[1].strip()

                        # æ•°æ®æ¸…æ´—ï¼šå»é™¤æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦
                        en_sentence = self.clean_english_sentence(en_sentence)
                        zh_sentence = self.clean_chinese_sentence(zh_sentence)

                        # è¿‡æ»¤æ‰è¿‡é•¿æˆ–è¿‡çŸ­çš„å¥å­
                        if 3 <= len(en_sentence.split()) <= 12 and 2 <= len(zh_sentence) <= 15:
                            pairs.append((en_sentence, zh_sentence))

                    # æ˜¾ç¤ºè¿›åº¦
                    if (line_num + 1) % 1000 == 0:
                        print(f"   å·²å¤„ç† {line_num + 1} è¡Œ...")

            print(f"âœ… æ–‡ä»¶è¯»å–å®Œæˆï¼å…±å¤„ç†äº† {line_num + 1} è¡Œ")
            print(f"âœ… ç­›é€‰å‡º {len(pairs)} å¯¹ç¬¦åˆæ¡ä»¶çš„å¥å­")

        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ {filename}")
            print("ğŸ’¡ ä½¿ç”¨é»˜è®¤çš„ç¤ºä¾‹æ•°æ®...")
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®
            pairs = [
                ("I love you", "æˆ‘çˆ±ä½ "),
                ("Hello world", "ä½ å¥½ä¸–ç•Œ"),
                ("Good morning", "æ—©ä¸Šå¥½"),
                ("How are you", "ä½ å¥½å—"),
                ("Thank you very much", "éå¸¸æ„Ÿè°¢ä½ "),
                ("See you later", "å†è§"),
                ("I am very happy", "æˆ‘éå¸¸å¼€å¿ƒ"),
                ("This is really good", "è¿™çœŸçš„å¾ˆå¥½"),
                ("I like eating apples", "æˆ‘å–œæ¬¢åƒè‹¹æœ"),
                ("Today is very sunny", "ä»Šå¤©éå¸¸æ™´æœ—"),
                ("I want to drink water", "æˆ‘æƒ³å–æ°´"),
                ("You are very nice", "ä½ å¾ˆå¥½"),
                ("I need your help", "æˆ‘éœ€è¦ä½ çš„å¸®åŠ©"),
                ("This book is easy", "è¿™æœ¬ä¹¦å¾ˆå®¹æ˜“"),
                ("I want to go home", "æˆ‘æƒ³å›å®¶"),
                ("The red car is fast", "çº¢è‰²æ±½è½¦å¾ˆå¿«"),
                ("She likes beautiful flowers", "å¥¹å–œæ¬¢ç¾ä¸½çš„èŠ±"),
                ("We study machine learning", "æˆ‘ä»¬å­¦ä¹ æœºå™¨å­¦ä¹ "),
                ("The weather is nice today", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"),
                ("I enjoy reading books", "æˆ‘å–œæ¬¢è¯»ä¹¦"),
            ]

        # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
        print(f"\nğŸ“‹ æ•°æ®æ ·æœ¬é¢„è§ˆ:")
        for i, (en, zh) in enumerate(pairs[:5]):
            print(f"   æ ·æœ¬{i + 1}: '{en}' â†’ '{zh}'")

        return pairs

    def clean_english_sentence(self, sentence):
        """æ¸…æ´—è‹±æ–‡å¥å­ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œç»Ÿä¸€æ ¼å¼"""
        # è½¬æ¢ä¸ºå°å†™
        sentence = sentence.lower()
        # å»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
        sentence = re.sub(r'[^\w\s\']', '', sentence)
        # å»é™¤å¤šä½™ç©ºæ ¼
        sentence = ' '.join(sentence.split())
        return sentence

    def clean_chinese_sentence(self, sentence):
        """æ¸…æ´—ä¸­æ–‡å¥å­ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œç»Ÿä¸€æ ¼å¼"""
        # å»é™¤è‹±æ–‡å­—ç¬¦å’Œæ ‡ç‚¹
        sentence = re.sub(r'[a-zA-Z\d\s.,!?;:\"\'()[\]{}]', '', sentence)
        # å»é™¤ç‰¹æ®Šæ ‡ç‚¹ç¬¦å·
        sentence = re.sub(r'[ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘{}]', '', sentence)
        return sentence.strip()

    def prepare_vocabularies(self):
        """
        æ„å»ºè¯æ±‡è¡¨ - åˆ¶ä½œ"å­—å…¸"

        å°±åƒå­¦å¤–è¯­è¦å…ˆåˆ¶ä½œå­—å…¸ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ï¼š
        1. æ‰¾å‡ºæ‰€æœ‰å‡ºç°çš„è‹±æ–‡å•è¯å’Œä¸­æ–‡è¯æ±‡
        2. ç»™æ¯ä¸ªè¯åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•°å­—ID
        3. æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆå¼€å§‹ã€ç»“æŸã€æœªçŸ¥è¯ç­‰ï¼‰
        """
        print("\nğŸ”¨ å¼€å§‹æ„å»ºè¯æ±‡è¡¨...")

        # ç‰¹æ®Šæ ‡è®° - å°±åƒæ ‡ç‚¹ç¬¦å·ä¸€æ ·é‡è¦
        # <PAD>: å¡«å……ç¬¦ï¼Œç”¨äºè®©æ‰€æœ‰å¥å­é•¿åº¦ä¸€è‡´
        # <START>: å¥å­å¼€å§‹æ ‡è®°
        # <END>: å¥å­ç»“æŸæ ‡è®°
        # <UNK>: æœªçŸ¥è¯æ ‡è®°ï¼Œç”¨äºå¤„ç†æ²¡è§è¿‡çš„è¯
        self.en_vocab = {"<PAD>": 0, "<START>": 1, "<END>": 2, "<UNK>": 3}
        self.zh_vocab = {"<PAD>": 0, "<START>": 1, "<END>": 2, "<UNK>": 3}

        print("   æ·»åŠ ç‰¹æ®Šæ ‡è®°: <PAD>, <START>, <END>, <UNK>")

        # æ”¶é›†æ‰€æœ‰è¯æ±‡
        en_words = set()  # è‹±æ–‡å•è¯é›†åˆ
        zh_words = set()  # ä¸­æ–‡è¯æ±‡é›†åˆ

        print("\nğŸ” æ‰«ææ‰€æœ‰å¥å­ï¼Œæ”¶é›†è¯æ±‡...")
        for i, (en_sentence, zh_sentence) in enumerate(self.pairs):
            # è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯
            en_words.update(en_sentence.lower().split())
            # ä¸­æ–‡ä½¿ç”¨jiebaåˆ†è¯
            zh_words.update(jieba.cut(zh_sentence))

            if i < 3:               # æ˜¾ç¤ºå‰3ä¸ªä¾‹å­
                print(f"   ä¾‹å­{i + 1}: '{en_sentence}' â†’ '{zh_sentence}'")
                print(f"           è‹±æ–‡è¯: {en_sentence.lower().split()}")
                print(f"           ä¸­æ–‡è¯: {list(jieba.cut(zh_sentence))}")

        print(f"\nğŸ“Š è¯æ±‡ç»Ÿè®¡:")
        print(f"   å‘ç°è‹±æ–‡å•è¯: {len(en_words)} ä¸ª")
        print(f"   å‘ç°ä¸­æ–‡è¯æ±‡: {len(zh_words)} ä¸ª")

        # æ„å»ºè¯æ±‡è¡¨ï¼ˆç»™æ¯ä¸ªè¯åˆ†é…IDï¼‰
        # ä»ID=4å¼€å§‹ï¼Œå› ä¸º0-3è¢«ç‰¹æ®Šæ ‡è®°å ç”¨
        for i, word in enumerate(sorted(en_words), 4):
            self.en_vocab[word] = i

        for i, word in enumerate(sorted(zh_words), 4):
            self.zh_vocab[word] = i

        # åˆ›å»ºåå‘è¯æ±‡è¡¨ï¼ˆä»IDæŸ¥æ‰¾è¯æ±‡ï¼‰
        self.en_idx2word = {idx: word for word, idx in self.en_vocab.items()}
        self.zh_idx2word = {idx: word for word, idx in self.zh_vocab.items()}

        print(f"\nâœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼")
        print(f"   è‹±æ–‡è¯æ±‡è¡¨å¤§å°: {len(self.en_vocab)}")
        print(f"   ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {len(self.zh_vocab)}")

        # æ˜¾ç¤ºä¸€äº›è¯æ±‡è¡¨å†…å®¹ä½œä¸ºä¾‹å­
        print(f"\nğŸ“– è‹±æ–‡è¯æ±‡è¡¨ç¤ºä¾‹:")
        for word, idx in list(self.en_vocab.items())[:8]:
            print(f"   '{word}' â†’ {idx}")

        print(f"\nğŸ“– ä¸­æ–‡è¯æ±‡è¡¨ç¤ºä¾‹:")
        for word, idx in list(self.zh_vocab.items())[:8]:
            print(f"   '{word}' â†’ {idx}")

    def sentence_to_indices(self, sentence, vocab, is_chinese=False):
        """
        å°†å¥å­è½¬æ¢ä¸ºæ•°å­—åºåˆ—

        æœºå™¨åªèƒ½ç†è§£æ•°å­—ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦æŠŠæ–‡å­—å¥å­è½¬æ¢æˆæ•°å­—åºåˆ—ã€‚
        å°±åƒæŠŠ "I love you" è½¬æ¢æˆ [5, 12, 8] è¿™æ ·çš„æ•°å­—åˆ—è¡¨ã€‚
        """
        if is_chinese:
            words = list(jieba.cut(sentence))           # ä¸­æ–‡åˆ†è¯
        else:
            words = sentence.lower().split()            # è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯

        # æŸ¥æ‰¾æ¯ä¸ªè¯çš„IDï¼Œå¦‚æœè¯æ±‡è¡¨ä¸­æ²¡æœ‰å°±ç”¨<UNK>
        indices = [vocab.get(word, vocab["<UNK>"]) for word in words]
        return indices

    def get_training_data(self):
        """
        è·å–è®­ç»ƒæ•°æ® - æŠŠæ‰€æœ‰å¥å­è½¬æ¢æˆæ•°å­—åºåˆ—

        è¿™ä¸€æ­¥å°†æ‰€æœ‰çš„ä¸­è‹±å¥å­å¯¹è½¬æ¢æˆæœºå™¨å­¦ä¹ éœ€è¦çš„æ•°å­—æ ¼å¼ã€‚
        """
        print("\nğŸ”„ å°†æ‰€æœ‰å¥å­è½¬æ¢ä¸ºæ•°å­—åºåˆ—...")

        en_sequences = []
        zh_sequences = []

        for i, (en_sentence, zh_sentence) in enumerate(self.pairs):
            # è½¬æ¢è‹±æ–‡å¥å­
            en_indices = self.sentence_to_indices(en_sentence, self.en_vocab, False)

            # è½¬æ¢ä¸­æ–‡å¥å­ï¼ˆæ³¨æ„ï¼šä¸­æ–‡å¥å­å‰åè¦åŠ <START>å’Œ<END>ï¼‰
            zh_indices = ([self.zh_vocab["<START>"]] + self.sentence_to_indices(zh_sentence, self.zh_vocab, True) + [self.en_vocab["<END>"]])

            en_sequences.append(en_indices)
            zh_sequences.append(zh_indices)

            # æ˜¾ç¤ºå‰3ä¸ªè½¬æ¢ä¾‹å­
            if i < 3:
                print(f"\n   ä¾‹å­{i + 1}:")
                print(f"   è‹±æ–‡: '{en_sentence}' â†’ {en_indices}")
                print(f"   ä¸­æ–‡: '{zh_sentence}' â†’ {zh_indices}")

        print(f"\nâœ… è½¬æ¢å®Œæˆï¼å…±å¤„ç† {len(en_sequences)} ä¸ªå¥å­å¯¹")
        return en_sequences, zh_sequences

# def pad_sequences(sequences, max_length=None, pad_value=0):
#     """
#     åºåˆ—å¡«å……å‡½æ•° - è®©æ‰€æœ‰å¥å­é•¿åº¦ä¸€è‡´
#
#     å°±åƒæ’é˜Ÿæ—¶è¦ç«™æ•´é½ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦è®©æ‰€æœ‰å¥å­é•¿åº¦ä¸€è‡´ï¼Œ
#     è¿™æ ·æœºå™¨æ‰èƒ½æ‰¹é‡å¤„ç†ã€‚çŸ­å¥å­ç”¨<PAD>å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦ã€‚
#     """
#     if max_length is None:
#         max_length = max(len(seq) for seq in sequences)
#
#     print(f"\nğŸ“ å¡«å……åºåˆ—åˆ°ç»Ÿä¸€é•¿åº¦ {max_length}...")
#
#     padded_sequences = []
#     for seq in sequences:
#         if len(seq) < max_length:
#             # çŸ­å¥å­ç”¨PADå¡«å……
#             padded_seq = seq + [pad_value] * (max_length - len(seq))
#         else:
#             # é•¿å¥å­æˆªæ–­ï¼ˆè™½ç„¶æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ä¸ä¼šå‡ºç°è¿™ç§æƒ…å†µ
#             padded_seq = seq[:max_length]
#         padded_sequences.append(padded_seq)
#
#     return padded_sequences
#
# # ğŸš€ å¼€å§‹æ•°æ®å‡†å¤‡è¿‡ç¨‹ï¼
# print("ğŸ¬ å¼€å§‹æ•°æ®å‡†å¤‡è¿‡ç¨‹...")
# print("="*60)
#
# # åˆ›å»ºæ•°æ®é›†
# dataset = EnhancedTranslationDatast()
#
# # è·å–è®­ç»ƒæ•°æ®
# en_sequences, zh_sequences = dataset.get_training_data()
#
# # è®¡ç®—åºåˆ—é•¿åº¦ç»Ÿè®¡
# max_en_length = max(len(seq) for seq in en_sequences)
# max_zh_length = max(len(seq) for seq in zh_sequences)
#
# print(f"\nğŸ“Š åºåˆ—é•¿åº¦ç»Ÿè®¡:")
# print(f"   æœ€é•¿è‹±æ–‡å¥å­: {max_en_length} ä¸ªè¯")
# print(f"   æœ€é•¿ä¸­æ–‡å¥å­: {max_zh_length} ä¸ªè¯")
#
# # å±•ç¤ºé•¿åº¦åˆ†å¸ƒ
# en_lengths = [len(seq) for seq in en_sequences]
# zh_lengths = [len(seq) for seq in zh_sequences]
# print(f"   è‹±æ–‡é•¿åº¦åˆ†å¸ƒ: æœ€çŸ­{min(en_lengths)}, æœ€é•¿{max(en_lengths)}, å¹³å‡{sum(en_lengths)/len(en_lengths):.1f}")
# print(f"   ä¸­æ–‡é•¿åº¦åˆ†å¸ƒ: æœ€çŸ­{min(zh_lengths)}, æœ€é•¿{max(zh_lengths)}, å¹³å‡{sum(zh_lengths)/len(zh_lengths):.1f}")
#
# # å¡«å……åºåˆ—
# en_padded = pad_sequences(en_sequences, max_en_length, dataset.en_vocab['<PAD>'])
# zh_padded = pad_sequences(zh_sequences, max_zh_length, dataset.zh_vocab['<PAD>'])
#
# # è½¬æ¢ä¸ºPyTorchå¼ é‡
# en_tensor = torch.tensor(en_padded, dtype=torch.long)
# zh_tensor = torch.tensor(zh_padded, dtype=torch.long)
#
# print(f"\nğŸ¯ æœ€ç»ˆæ•°æ®æ ¼å¼:")
# print(f"   è‹±æ–‡å¼ é‡å½¢çŠ¶: {en_tensor.shape} (å¥å­æ•° Ã— æœ€å¤§é•¿åº¦)")
# print(f"   ä¸­æ–‡å¼ é‡å½¢çŠ¶: {zh_tensor.shape} (å¥å­æ•° Ã— æœ€å¤§é•¿åº¦)")
#
# print("\n" + "="*60)
# print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼æˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººç°åœ¨æœ‰äº†å­¦ä¹ ææ–™ï¼")
# print("ğŸ’¡ æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ„å»ºæ¨¡å‹çš„ä¸‰ä¸ªæ ¸å¿ƒéƒ¨ä»¶...")
# print("="*60)


# ğŸ“š ç¬¬äºŒæ­¥ï¼šè¯æ±‡è¡¨æ„å»ºä¸æ•°æ®è½¬æ¢
print("ğŸ”§ å¼€å§‹è¯æ±‡è¡¨æ„å»ºé˜¶æ®µ...")
print("="*60)

# åˆ›å»ºæ•°æ®é›†å®ä¾‹ï¼ˆè¿™ä¼šè‡ªåŠ¨åŠ è½½æ•°æ®å¹¶æ„å»ºè¯æ±‡è¡¨ï¼‰
dataset = EnhancedTranslationDatast(data_file='cmn.txt', max_pairs=3000)

print("\n" + "="*60)
print("âœ… æ•°æ®åŠ è½½å’Œè¯æ±‡è¡¨æ„å»ºå®Œæˆï¼")
print("="*60)


# ğŸ“ ç¬¬ä¸‰æ­¥ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—
print("ğŸ”„ å¼€å§‹æ–‡æœ¬åˆ°æ•°å­—çš„è½¬æ¢...")
print("="*50)

# è·å–è®­ç»ƒæ•°æ®ï¼ˆè½¬æ¢ä¸ºæ•°å­—åºåˆ—ï¼‰
en_sequences, zh_sequences = dataset.get_training_data()

# ğŸ“Š åˆ†æåºåˆ—é•¿åº¦åˆ†å¸ƒ
max_en_length = max(len(seq) for seq in en_sequences)
max_zh_length = max(len(seq) for seq in zh_sequences)

print(f"\nğŸ“Š åºåˆ—é•¿åº¦ç»Ÿè®¡:")
print(f"  æœ€é•¿è‹±æ–‡å¥å­ï¼š{max_en_length}ä¸ªè¯")
print(f"  æœ€é•¿ä¸­æ–‡å¥å­ï¼š{max_zh_length}ä¸ªè¯")

# å±•ç¤ºé•¿åº¦åˆ†å¸ƒ
en_lengths = [len(seq) for seq in en_sequences]
zh_lengths = [len(seq) for seq in zh_sequences]
print(f"  è‹±æ–‡é•¿åº¦åˆ†å¸ƒï¼šæœ€çŸ­{min(en_lengths)}ï¼Œæœ€é•¿{max(en_lengths)}ï¼Œå¹³å‡{sum(en_lengths)/len(en_lengths):.1f}")
print(f"  ä¸­æ–‡é•¿åº¦åˆ†å¸ƒï¼šæœ€çŸ­{min(zh_lengths)}ï¼Œæœ€é•¿{max(zh_lengths)}ï¼Œå¹³å‡{sum(zh_lengths)/len(zh_lengths):.1f}")

print("\n" + "="*50)
print("âœ… æ–‡æœ¬è½¬æ¢å®Œæˆï¼")
print("="*50)


# ğŸ“ ç¬¬å››æ­¥ï¼šåºåˆ—å¡«å……ä¸å¼ é‡è½¬æ¢
print("ğŸ“ å¼€å§‹åºåˆ—å¡«å……å¤„ç†...")
print("="*50)

def pad_sequences(sequences, max_length=None, pad_value=0):
    """
    åºåˆ—å¡«å……å‡½æ•° - è®©æ‰€æœ‰å¥å­é•¿åº¦ä¸€è‡´

    å°±åƒæ’é˜Ÿæ—¶è¦ç«™æ•´é½ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦è®©æ‰€æœ‰å¥å­é•¿åº¦ä¸€è‡´ï¼Œ
    è¿™æ ·æœºå™¨æ‰èƒ½æ‰¹é‡å¤„ç†ã€‚çŸ­å¥å­ç”¨<PAD>å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦ã€‚
    """
    if max_length is None:
        max_length = max(len(seq) for seq in sequences)

    print(f"   ğŸ“ å¡«å……åºåˆ—åˆ°ç»Ÿä¸€é•¿åº¦ {max_length}...")

    padded_sequences = []
    for seq in sequences:
        if len(seq) < max_length:
            # çŸ­å¥å­ç”¨PADå¡«å……
            padded_seq = seq + [pad_value] * (max_length - len(seq))
        else:
            # é•¿å¥å­æˆªæ–­ï¼ˆè™½ç„¶æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ä¸ä¼šå‡ºç°è¿™ç§æƒ…å†µï¼‰
            padded_seq = seq[:max_length]
        padded_sequences.append(padded_seq)

    return padded_sequences

#æ‰§è¡Œå¡«å……
en_padded = pad_sequences(en_sequences, max_en_length, dataset.en_vocab["<PAD>"])
zh_padded = pad_sequences(zh_sequences, max_zh_length, dataset.zh_vocab["<PAD>"])

# è½¬æ¢ä¸ºPyTorchå¼ é‡
en_tensor = torch.tensor(en_padded, dtype=torch.long)
zh_tensor = torch.tensor(zh_padded, dtype=torch.long)

print(f"\nğŸ¯ æœ€ç»ˆæ•°æ®æ ¼å¼:")
print(f"  è‹±æ–‡å¼ é‡å½¢çŠ¶ï¼š{en_tensor.shape}ï¼ˆå¥å­æ•°Ã—æœ€å¤§é•¿åº¦ï¼‰")
print(f"  ä¸­æ–‡å¼ é‡å½¢çŠ¶ï¼š{zh_tensor.shape}ï¼ˆå¥å­æ•°Ã—æœ€å¤§é•¿åº¦ï¼‰")

# æ˜¾ç¤ºå¡«å……å‰åçš„å¯¹æ¯”ç¤ºä¾‹
print(f"\nğŸ“‹ å¡«å……æ•ˆæœç¤ºä¾‹ï¼ˆå‰3ä¸ªå¥å­ï¼‰:")
for i in range(3):
    print(f"  å¥å­{i+1}:")
    print(f"    åŸå§‹è‹±æ–‡é•¿åº¦ï¼š{len(en_sequences[i])}")
    print(f"    å¡«å……åé•¿åº¦ï¼š{len(en_padded[i])}")
    print(f"    åŸå§‹ä¸­æ–‡é•¿åº¦ï¼š{len(zh_sequences[i])}")
    print(f"    å¡«å……åé•¿åº¦ï¼š{len(zh_padded[i])}")

print("\n" + "="*50)
print("âœ… æ•°æ®é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼æˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººç°åœ¨æœ‰äº†å­¦ä¹ ææ–™ï¼")
print("ğŸ’¡ æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ„å»ºæ¨¡å‹çš„ä¸‰ä¸ªæ ¸å¿ƒéƒ¨ä»¶...")
print("="*50)


# ğŸ§  ç»„ä»¶1ï¼šç¼–ç å™¨ - "ç†è§£ä¸“å®¶"
print("ğŸ—ï¸ å¼€å§‹æ„å»ºç¼–ç å™¨...")

class AttentionEncoder(nn.Module):
    """
    ğŸ§  ç¼–ç å™¨ - "ç†è§£ä¸“å®¶"

    è¿™æ˜¯æˆ‘ä»¬ç¿»è¯‘æœºå™¨äººçš„ç¬¬ä¸€ä¸ªéƒ¨ä»¶ï¼Œå®ƒçš„å·¥ä½œæ˜¯ï¼š
    1. æ¥æ”¶è‹±æ–‡å¥å­
    2. ç†è§£æ¯ä¸ªå•è¯çš„å«ä¹‰
    3. ä¿å­˜æ‰€æœ‰å•è¯çš„ä¿¡æ¯ï¼ˆä¸å‹ç¼©ï¼ï¼‰

    ä¸ä¼ ç»ŸSeq2Seqä¸åŒï¼Œè¿™é‡Œæˆ‘ä»¬ä¿ç•™æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œ
    è€Œä¸æ˜¯åªä¿ç•™æœ€åä¸€ä¸ªçŠ¶æ€ã€‚è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„å…³é”®ï¼
    """

    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=1):
        super(AttentionEncoder, self).__init__()

        print(f"ğŸ§  æ„å»ºç¼–ç å™¨ï¼ˆç†è§£ä¸“å®¶ï¼‰:")
        print(f"  è¯æ±‡é‡ï¼š{vocab_size}")
        print(f"  è¯åµŒå…¥ç»´åº¦ï¼š{embedding_dim}")
        print(f"  éšè—å±‚ç»´åº¦ï¼š{hidden_dim}")

        # è¯åµŒå…¥å±‚ï¼šæŠŠè¯æ±‡IDè½¬æ¢æˆå¯†é›†å‘é‡
        # å°±åƒç»™æ¯ä¸ªè‹±æ–‡å•è¯åˆ¶ä½œä¸€å¼ â€œèº«ä»½è¯â€
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # LSTMå±‚ï¼šç†è§£å¥å­çš„ä¸Šä¸‹æ–‡å’Œè¯­æ³•
        # å°±åƒä¸€ä¸ªè‹±æ–‡ä¸“å®¶ï¼Œèƒ½ç†è§£å¥å­çš„æ·±å±‚å«ä¹‰
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,
                            batch_first=True, dropout=0.1 if n_layers > 1 else 0)

        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

    def forward(self, input_sequence):
        """
        ç¼–ç å™¨çš„å·¥ä½œæµç¨‹

        è¾“å…¥: è‹±æ–‡å¥å­çš„æ•°å­—åºåˆ—ï¼Œå¦‚ [5, 12, 8, 3] è¡¨ç¤º "I love machine learning"
        è¾“å‡º: æ¯ä¸ªè‹±æ–‡å•è¯çš„æ·±åº¦ç†è§£è¡¨ç¤º
        """
        # æ­¥éª¤1ï¼šè¯åµŒå…¥ - æŠŠæ•°å­—IDè½¬æ¢æˆå‘é‡
        embedded = self.embedding(input_sequence)

        # æ­¥éª¤2ï¼šLSTMå¤„ç† - ç†è§£ä¸Šä¸‹æ–‡
        # âš ï¸ å…³é”®ï¼šæˆ‘ä»¬è¿”å›æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºï¼Œä¸åªæ˜¯æœ€åä¸€ä¸ªï¼
        encoder_outputs, (hidden, cell) = self.lstm(embedded)

        return encoder_outputs, (hidden, cell)

print("âœ… ç¼–ç å™¨æ„å»ºå®Œæˆï¼")

# ğŸ‘ï¸ ç»„ä»¶2ï¼šBahdanauæ³¨æ„åŠ›æœºåˆ¶ - "å…³æ³¨ä¸“å®¶"
print("ğŸ—ï¸ å¼€å§‹æ„å»ºæ³¨æ„åŠ›æ¨¡å—...")

class BahdanauAttention(nn.Module):
    """
    ğŸ‘ï¸ Bahdanauæ³¨æ„åŠ›æœºåˆ¶ - "å…³æ³¨ä¸“å®¶"

    è¿™æ˜¯æˆ‘ä»¬ç¿»è¯‘æœºå™¨äººçš„ç¬¬äºŒä¸ªéƒ¨ä»¶ï¼Œå®ƒçš„å·¥ä½œæ˜¯ï¼š
    1. åˆ†æå½“å‰è¦ç¿»è¯‘çš„è¯éœ€è¦ä»€ä¹ˆå¸®åŠ©
    2. æ‰«ææ‰€æœ‰è‹±æ–‡å•è¯ï¼Œè¯„ä¼°æ¯ä¸ªè¯çš„æœ‰ç”¨ç¨‹åº¦
    3. è®¡ç®—æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ

    è¿™å°±æ˜¯æˆ‘ä»¬ä¹‹å‰å­¦ä¹ çš„"é€‰æœ‹å‹å¸®å¿™"ç®—æ³•çš„ä¸“ä¸šå®ç°ï¼
    """

    def __init__(self, hidden_dim, attention_type='Bahdanau'):
        super(BahdanauAttention, self).__init__()
        self.hidden_dim = hidden_dim
        self.attention_type = attention_type

        print(f"ğŸ‘ï¸ æ„å»ºæ³¨æ„åŠ›æ¨¡å—ï¼ˆå…³æ³¨ä¸“å®¶ï¼‰:")
        print(f"   ä½¿ç”¨ {attention_type} æ³¨æ„åŠ›æœºåˆ¶")
        print(f"   éšè—ç»´åº¦: {hidden_dim}")

        if attention_type == 'Bahdanau':
            # ä¸‰ä¸ªçº¿æ€§å˜æ¢å±‚ - Bahdanauæ³¨æ„åŠ›çš„æ ¸å¿ƒç»„ä»¶
            # è¿™ä¸‰ä¸ªå±‚å®ç°äº†æˆ‘ä»¬å­¦è¿‡çš„â€œç›¸å…³æ€§è®¡ç®—â€å…¬å¼

            # W_h: å¤„ç†ç¼–ç å™¨è¾“å‡ºï¼ˆæœ‹å‹ä»¬çš„ä¸“é•¿ï¼‰
            self.W_h = nn.Linear(hidden_dim, hidden_dim, bias=False)
            print("   ğŸ“Š W_h: å¤„ç†ç¼–ç å™¨è¾“å‡ºçš„å˜æ¢å±‚")

            # W_s: å¤„ç†è§£ç å™¨çŠ¶æ€ï¼ˆå½“å‰çš„å›°æƒ‘ï¼‰
            self.W_s = nn.Linear(hidden_dim, hidden_dim, bias=False)
            print("   ğŸ“Š W_s: å¤„ç†è§£ç å™¨çŠ¶æ€çš„å˜æ¢å±‚")

            # v: è®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼ˆç»¼åˆè¯„ä¼°ï¼‰
            self.v = nn.Linear(hidden_dim, 1, bias=False)
            print("   ğŸ“Š v: è®¡ç®—æœ€ç»ˆç›¸å…³æ€§åˆ†æ•°çš„å±‚")

    def forward(self, decoder_hidden, encoder_outputs):
        """
        æ³¨æ„åŠ›è®¡ç®—çš„æ ¸å¿ƒè¿‡ç¨‹

        è¿™å°±æ˜¯æˆ‘ä»¬å­¦è¿‡çš„"é€‰æœ‹å‹å¸®å¿™ä¸‰æ­¥æ³•"çš„ä»£ç å®ç°ï¼

        å‚æ•°:
            decoder_hidden: å½“å‰çš„"å›°æƒ‘çŠ¶æ€" [batch_size, hidden_dim]
            encoder_outputs: æ‰€æœ‰"æœ‹å‹çš„ä¸“é•¿" [batch_size, seq_len, hidden_dim]
        """
        seq_len = encoder_outputs.size(1)

        # ğŸ¯ æ­¥éª¤1: å‡†å¤‡æ•°æ® - è®©decoder_hiddenå’Œencoder_outputså°ºå¯¸åŒ¹é…
        # å°†decoder_hiddenä»[batch_size, hidden_dim]æ‰©å±•ä¸º[batch_size, seq_len, hidden_dim]
        # 1. unsqueeze(1)åœ¨ç¬¬1ç»´å¢åŠ ä¸€ä¸ªç»´åº¦,å˜æˆ[batch_size, 1, hidden_dim]
        # 2. repeat(1, seq_len, 1)å°†ç¬¬1ç»´é‡å¤seq_lenæ¬¡,ä¿æŒå…¶ä»–ç»´åº¦ä¸å˜
        # è¿™æ ·åšæ˜¯ä¸ºäº†è®©decoder_hiddençš„ç»´åº¦ä¸encoder_outputsåŒ¹é…,ä¾¿äºåç»­è®¡ç®—
        decoder_hidden_expanded = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)

        # ğŸ¯ æ­¥éª¤2: è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        if self.attention_type == 'dot':
            # ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—
            # å…¬å¼: e_ij = h_j Â· s_i
            # ä½¿ç”¨æ‰¹é‡çŸ©é˜µä¹˜æ³•(bmm)è®¡ç®—ç‚¹ç§¯æ³¨æ„åŠ›åˆ†æ•°
            scores = torch.bmm(encoder_outputs, decoder_hidden_expanded)

        else:
            # Bahdanauæ³¨æ„åŠ›è®¡ç®—
            # å…¬å¼ï¼še_ij = v^T * tanh(W_h * h_j + W_s * s_i)
            # 1. ä½¿ç”¨W_hå˜æ¢å±‚å¤„ç†ç¼–ç å™¨è¾“å‡º(æœ‹å‹ä¸“é•¿)
            # è¾“å…¥: encoder_outputs [batch_size, seq_len, hidden_dim]
            # è¾“å‡º: W_h_out [batch_size, seq_len, hidden_dim]
            W_h_out = self.W_h(encoder_outputs)             # å¤„ç†â€œæœ‹å‹ä¸“é•¿â€

            # 2. ä½¿ç”¨W_så˜æ¢å±‚å¤„ç†è§£ç å™¨çŠ¶æ€(å½“å‰å›°æƒ‘)
            # è¾“å…¥: decoder_hidden_expanded [batch_size, seq_len, hidden_dim]
            # è¾“å‡º: W_s_out [batch_size, seq_len, hidden_dim]
            W_s_out = self.W_s(decoder_hidden_expanded)     # å¤„ç†â€œå½“å‰å›°æƒ‘â€

            # 3. å°†ä¸¤ä¸ªå˜åŒ–ç»“æœç›¸åŠ å¹¶é€šè¿‡tanhæ¿€æ´»å‡½æ•°
            # è¾“å…¥: W_h_out + W_s_out [batch_size, seq_len, hidden_dim]
            # è¾“å‡º: energy [batch_size, seq_len, hidden_dim]
            energy = torch.tanh(W_h_out + W_s_out)          # éçº¿æ€§ç»„åˆ

            # 4. ä½¿ç”¨vå±‚è®¡ç®—æœ€ç»ˆæ³¨æ„åŠ›åˆ†æ•°å¹¶å‹ç¼©æœ€åä¸€ç»´
            # è¾“å…¥: energy [batch_size, seq_len, hidden_dim]
            # è¾“å‡º: scores [batch_size, seq_len]
            scores = self.v(energy).squeeze(2)              # è®¡ç®—æœ€ç»ˆåˆ†æ•°

        # ğŸ¯ æ­¥éª¤3: è®¡ç®—æ³¨æ„åŠ›æƒé‡ - softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=1)

        # ğŸ¯ æ­¥éª¤4: è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ - åŠ æƒå¹³å‡
        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        context_vector = context_vector.squeeze(1)

        return context_vector, attention_weights

print("âœ… æ³¨æ„åŠ›æ¨¡å—æ„å»ºå®Œæˆï¼")


# âœï¸ ç»„ä»¶3ï¼šè§£ç å™¨ - "ç¿»è¯‘ä¸“å®¶"
print("ğŸ—ï¸ å¼€å§‹æ„å»ºè§£ç å™¨...")

class AttentionDecoder(nn.Module):
    """
    âœï¸ è§£ç å™¨ - "ç¿»è¯‘ä¸“å®¶"

    è¿™æ˜¯æˆ‘ä»¬ç¿»è¯‘æœºå™¨äººçš„ç¬¬ä¸‰ä¸ªéƒ¨ä»¶ï¼Œå®ƒçš„å·¥ä½œæ˜¯ï¼š
    1. æ¥æ”¶æ³¨æ„åŠ›æ¨¡å—æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    2. ç»“åˆå½“å‰è¾“å…¥è¯çš„ä¿¡æ¯
    3. ç”Ÿæˆä¸‹ä¸€ä¸ªä¸­æ–‡è¯

    ä¸ä¼ ç»Ÿè§£ç å™¨ä¸åŒï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ³¨æ„åŠ›æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œ
    è€Œä¸æ˜¯å›ºå®šçš„ç¼–ç å™¨æœ€ç»ˆçŠ¶æ€ã€‚
    """
    def __init__(self, vocab_size, embedding_dim, hidden_dim, attention, n_layers=1):
        super(AttentionDecoder, self).__init__()

        print(f"âœï¸ æ„å»ºè§£ç å™¨ï¼ˆç¿»è¯‘ä¸“å®¶ï¼‰:")
        print(f"   ä¸­æ–‡è¯æ±‡é‡: {vocab_size}")
        print(f"   è¯åµŒå…¥ç»´åº¦: {embedding_dim}")
        print(f"   éšè—å±‚ç»´åº¦: {hidden_dim}")

        # ä¸­æ–‡è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # æ³¨æ„åŠ›æ¨¡å—ï¼ˆæˆ‘ä»¬åˆšæ‰åˆ›å»ºçš„ï¼‰
        self.attention = attention

        # âš ï¸ å…³é”®è®¾è®¡ï¼šLSTMçš„è¾“å…¥æ˜¯è¯åµŒå…¥ + ä¸Šä¸‹æ–‡å‘é‡çš„æ‹¼æ¥
        # è¿™æ ·è§£ç å™¨å°±èƒ½åŒæ—¶ä½¿ç”¨"å½“å‰è¯ä¿¡æ¯"å’Œ"æ³¨æ„åŠ›ä¸Šä¸‹æ–‡"
        lstm_input_size = embedding_dim + hidden_dim
        self.lstm = nn.LSTM(lstm_input_size, hidden_dim, n_layers,
                            batch_first=True, dropout=0.1 if n_layers > 1 else 0)
        print(f"   ğŸ”— LSTMè¾“å…¥ç»´åº¦: {lstm_input_size} (è¯åµŒå…¥ + ä¸Šä¸‹æ–‡)")

        # è¾“å‡ºæŠ•å½±å±‚ï¼šæŠŠLSTMè¾“å‡ºè½¬æ¢æˆè¯æ±‡æ¦‚ç‡åˆ†å¸ƒ
        self.output_projection = nn.Linear(hidden_dim, vocab_size)
        print(f"   ğŸ“¤ è¾“å‡ºå±‚: {hidden_dim} â†’ {vocab_size}")

    def forward(self, input_token, hidden_state, encoder_outputs):
        """
        è§£ç å™¨çš„å·¥ä½œæµç¨‹ - ç”Ÿæˆä¸€ä¸ªä¸­æ–‡è¯

        è¿™æ˜¯"è¾¹çœ‹è¾¹è¯‘"çš„æ ¸å¿ƒå®ç°ï¼š
        1. æ ¹æ®å½“å‰çŠ¶æ€è®¡ç®—æ³¨æ„åŠ›
        2. è·å¾—å½“å‰æœ€ç›¸å…³çš„è‹±æ–‡ä¿¡æ¯
        3. ç»“åˆå½“å‰è¾“å…¥ç”Ÿæˆä¸‹ä¸€ä¸ªä¸­æ–‡è¯
        """
        # æ­¥éª¤1ï¼šè¯åµŒå…¥
        embedded = self.embedding(input_token)

        # æ­¥éª¤2ï¼šè®¡ç®—æ³¨æ„åŠ› - è¿™æ˜¯å…³é”®ï¼
        # ä½¿ç”¨å½“å‰è§£ç å™¨çŠ¶æ€æ¥å†³å®šå…³æ³¨å“ªäº›è‹±æ–‡è¯
        current_hidden = hidden_state[0][-1]            # å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
        context_vector, attention_weights = self.attention(current_hidden, encoder_outputs)

        # æ­¥éª¤3ï¼šæ‹¼æ¥è¾“å…¥
        # æŠŠâ€œå½“å‰è¯ä¿¡æ¯â€å’Œâ€œæ³¨æ„åŠ›ä¸Šä¸‹æ–‡â€ç»„åˆèµ·æ¥
        lstm_input = torch.cat([embedded, context_vector.unsqueeze(1)], dim=2)

        # æ­¥éª¤4ï¼šLSTMå¤„ç†
        lstm_output, new_hidden_state = self.lstm(lstm_input, hidden_state)

        # æ­¥éª¤5ï¼šç”Ÿæˆè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
        output = self.output_projection(lstm_output)

        return output, new_hidden_state, attention_weights

print("âœ… è§£ç å™¨æ„å»ºå®Œæˆï¼")


# ğŸ¤– ç»„è£…å®Œæ•´çš„ç¿»è¯‘ç³»ç»Ÿ
print("ğŸ”§ å¼€å§‹ç»„è£…å®Œæ•´çš„ç¿»è¯‘ç³»ç»Ÿ...")

class AttentionSeq2Seq(nn.Module):
    """
    ğŸ¤– å®Œæ•´çš„å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ç¿»è¯‘ç³»ç»Ÿ

    è¿™æ˜¯æˆ‘ä»¬çš„æ€»æŒ‡æŒ¥å®˜ï¼Œå®ƒåè°ƒä¸‰ä¸ªéƒ¨ä»¶ï¼š
    ğŸ§  ç¼–ç å™¨ + ğŸ‘ï¸ æ³¨æ„åŠ›æ¨¡å— + âœï¸ è§£ç å™¨

    å®ç°äº†å®Œæ•´çš„"è¾¹çœ‹è¾¹è¯‘"ç¿»è¯‘æµç¨‹ï¼
    """

    def __init__(self, en_vocab_size, zh_vocab_size, embedding_dim, hidden_dim):
        super(AttentionSeq2Seq, self).__init__()

        print(f"ğŸ¤– ç»„è£…å®Œæ•´ç¿»è¯‘ç³»ç»Ÿ...")
        print(f"   è‹±æ–‡è¯æ±‡é‡: {en_vocab_size}")
        print(f"   ä¸­æ–‡è¯æ±‡é‡: {zh_vocab_size}")
        print(f"   åµŒå…¥ç»´åº¦: {embedding_dim}")
        print(f"   éšè—ç»´åº¦: {hidden_dim}")

        # åˆ›å»ºä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶
        print("\nğŸ”§ åˆ›å»ºç³»ç»Ÿç»„ä»¶:")

        # ç»„ä»¶1ï¼šç¼–ç å™¨
        self.encoder = AttentionEncoder(en_vocab_size, embedding_dim, hidden_dim)

        # ç»„ä»¶2ï¼šæ³¨æ„åŠ›æ¨¡å—
        self.attention = BahdanauAttention(hidden_dim)

        # ç»„ä»¶3ï¼šè§£ç å™¨ï¼ˆä½¿ç”¨ä¸Šé¢åˆ›å»ºçš„æ³¨æ„åŠ›æ¨¡å—ï¼‰
        self.decoder = AttentionDecoder(zh_vocab_size, embedding_dim, hidden_dim, self.attention)

        self.zh_vocab_size = zh_vocab_size

        print("\nâœ… ç¿»è¯‘ç³»ç»Ÿç»„è£…å®Œæˆï¼")

    def forward(self, en_sequence, zh_sequence=None, teacher_forcing_ratio=0.5):
        """
        ç¿»è¯‘ç³»ç»Ÿçš„ä¸»å·¥ä½œæµç¨‹

        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. è®­ç»ƒæ¨¡å¼ï¼šæœ‰æ­£ç¡®ç­”æ¡ˆï¼Œç”¨äºå­¦ä¹ 
        2. æ¨ç†æ¨¡å¼ï¼šæ²¡æœ‰ç­”æ¡ˆï¼ŒçœŸæ­£ç¿»è¯‘
        """
        batch_size = en_sequence.size(0)

        # ç¬¬ä¸€æ­¥ï¼šç¼–ç  - ç†è§£è‹±æ–‡
        encoder_outputs, encoder_hidden = self.encoder(en_sequence)

        if zh_sequence is not None:
            # è®­ç»ƒæ¨¡å¼
            return self._train_forward(encoder_outputs, encoder_hidden, zh_sequence, teacher_forcing_ratio)
        else:
            # æ¨ç†æ¨¡å¼
            return self._inference_forward(encoder_outputs, encoder_hidden, max_length=15)

    def _train_forward(self, encoder_outputs, encoder_hidden, target_sequence, teacher_forcing_ratio):
        """è®­ç»ƒæ¨¡å¼ï¼šæœ‰æ ‡å‡†ç­”æ¡ˆï¼Œè¾¹å­¦è¾¹ç»ƒ"""
        batch_size, target_length = target_sequence.shape
        outputs = torch.zeros(batch_size, target_length - 1, self.zh_vocab_size).to(device)
        all_attention_weights = []

        decoder_hidden = encoder_hidden
        decoder_input = target_sequence[:, 0].unsqueeze(1)          # <START>æ ‡è®°

        for t in range(1, target_length):
            # ç”Ÿæˆå½“å‰è¯
            output, decoder_hidden, attention_weights = self.decoder(
                decoder_input, decoder_hidden, encoder_outputs)

            outputs[:, t-1, :] = output.squeeze(1)
            all_attention_weights.append(attention_weights)

            # Teacher Forcingç­–ç•¥
            if random.random() < teacher_forcing_ratio:
                # ä½¿ç”¨æ­£ç¡®ç­”æ¡ˆï¼ˆè€å¸ˆæŒ‡å¯¼ï¼‰
                decoder_input = target_sequence[:, t].unsqueeze(1)
            else:
                # ä½¿ç”¨æ¨¡å‹é¢„æµ‹ï¼ˆç‹¬ç«‹æ€è€ƒï¼‰
                decoder_input = output.argmax(2)

        return outputs, torch.stack(all_attention_weights, dim=1)

    def _inference_forward(self, encoder_outputs, encoder_hidden, max_length):
        """æ¨ç†æ¨¡å¼ï¼šçœŸæ­£çš„ç¿»è¯‘ï¼Œé€è¯ç”Ÿæˆç›´åˆ°å¥å­ç»“æŸ"""
        batch_size = encoder_outputs.size(0)
        decoder_hidden = encoder_hidden
        decoder_input = torch.ones(batch_size, 1, dtype=torch.long).to(device)          # <START>

        outputs = []
        attention_weights_list = []

        for step in range(max_length):
            # ç”Ÿæˆä¸‹ä¸€ä¸ªè¯
            output, decoder_hidden, attention_weights = self.decoder(
                decoder_input, decoder_hidden, encoder_outputs)

            predicted = output.argmax(2)
            outputs.append(predicted.squeeze(1))
            attention_weights_list.append(attention_weights)

            # ä¸‹ä¸€æ­¥çš„è¾“å…¥å°±æ˜¯å½“å‰çš„é¢„æµ‹
            decoder_input = predicted

            # å¦‚æœç”Ÿæˆäº†<END>æ ‡è®°å°±åœæ­¢
            if predicted.item() == 2:
                break

        if outputs:
            return torch.stack(outputs, dim=1), torch.stack(attention_weights_list, dim=1)
        else:
            return torch.empty(batch_size, 0, dtype=torch.long).to(device), torch.empty(batch_size, 0, encoder_outputs.size(1)).to(device)

# ğŸš€ åˆ›å»ºæˆ‘ä»¬çš„ç¿»è¯‘ç³»ç»Ÿï¼
print("\n" + "="*60)
print("ğŸ¬ å¼€å§‹åˆ›å»ºå®Œæ•´çš„æ³¨æ„åŠ›ç¿»è¯‘ç³»ç»Ÿ...")
print("="*60)

attention_model = AttentionSeq2Seq(
    en_vocab_size=len(dataset.en_vocab),
    zh_vocab_size=len(dataset.zh_vocab),
    embedding_dim=128,                          # è¯åµŒå…¥ç»´åº¦
    hidden_dim=256                              # éšè—å±‚ç»´åº¦
).to(device)

# ç»Ÿè®¡æ¨¡å‹å‚æ•°
total_params = sum(p.numel() for p in attention_model.parameters())
trainable_params = sum(p.numel() for p in attention_model.parameters() if p.requires_grad)

print("\n" + "="*60)
print("âœ… å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ç¿»è¯‘ç³»ç»Ÿåˆ›å»ºæˆåŠŸï¼")
print(f"ğŸ“Š æ¨¡å‹ç»Ÿè®¡:")
print(f"   æ€»å‚æ•°é‡: {total_params:,}")
print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
print(f"   æ¨¡å‹å¤§å°: ~{total_params * 4 / 1024 / 1024:.1f} MB")
print("ğŸ’¡ æˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººå·²ç»å°±ç»ªï¼Œå‡†å¤‡å¼€å§‹å­¦ä¹ ï¼")
print("="*60)


# ğŸ¯ è®­ç»ƒå‡½æ•°ï¼šæ•™æœºå™¨äººå­¦ç¿»è¯‘
def train_attention_model(model, en_tensor, zh_tensor, epochs=300):
    """
    è®­ç»ƒå¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ç¿»è¯‘æ¨¡å‹

    å‚æ•°:
        model: æˆ‘ä»¬æ„å»ºçš„æ³¨æ„åŠ›ç¿»è¯‘æ¨¡å‹
        en_tensor: è‹±æ–‡æ•°æ®å¼ é‡
        zh_tensor: ä¸­æ–‡æ•°æ®å¼ é‡
        epochs: è®­ç»ƒè½®æ•°
    """
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒå¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ç¿»è¯‘æ¨¡å‹...")
    print("=" * 50)

    # è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(ignore_index=0)             # å¿½ç•¥<PAD>æ ‡è®°
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adamä¼˜åŒ–å™¨

    print("ğŸ”§ è®­ç»ƒé…ç½®:")
    print(f"   æŸå¤±å‡½æ•°: CrossEntropyLoss (å¿½ç•¥PAD)")
    print(f"   ä¼˜åŒ–å™¨: Adam (å­¦ä¹ ç‡=0.001)")
    print(f"   è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   æ•°æ®é‡: {len(en_tensor)} ä¸ªå¥å­å¯¹")

    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
    en_tensor = en_tensor.to(device)
    zh_tensor = zh_tensor.to(device)

    model.train()                           # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    losses = []                             # è®°å½•æŸå¤±å€¼

    print("\nğŸš€ å¼€å§‹è®­ç»ƒå¾ªç¯...")
    for epoch in range(epochs):
        optimizer.zero_grad()               # æ¸…é›¶ç»´åº¦

        # å‰å‘ä¼ æ’­ï¼šè®©æ¨¡å‹å°è¯•ç¿»è¯‘
        outputs, attention_weights = model(en_tensor, zh_tensor, teacher_forcing_ratio=0.8)

        # è®¡ç®—æŸå¤±ï¼šå¯¹æ¯”æ¨¡å‹è¾“å‡ºå’Œæ­£ç¡®ç­”æ¡ˆ
        loss = criterion(outputs.reshape(-1, outputs.size(-1)),
                         zh_tensor[:, 1:].reshape(-1))

        # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # å‚æ•°æ›´æ–°ï¼šæ ¹æ®æ¢¯åº¦è°ƒæ•´å‚æ•°
        optimizer.step()

        # è®°å½•æŸå¤±
        losses.append(loss.item())

        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if (epoch + 1) % 50 == 0:
            print(f"ğŸ“Š Epoch {epoch + 1:3d}/{epochs}, Loss: {loss.item():.4f}")

            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
            progress = (epoch + 1) / epochs
            bar_length = 30
            filled_length = int(bar_length * progress)
            bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
            print(f"    è¿›åº¦: |{bar}| {progress:.1%}")

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    return losses

# ğŸ¬ å¼€å§‹è®­ç»ƒï¼
print("ğŸ¬ å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ³¨æ„åŠ›ç¿»è¯‘æ¨¡å‹...")
print("ğŸ’¡ è¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
print("="*60)

attention_losses = train_attention_model(attention_model, en_tensor, zh_tensor, epochs=300)

print("\n" + "="*60)
print("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼æˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººå­¦ä¼šç¿»è¯‘äº†ï¼")
print("="*60)


# ğŸ“ˆ è®­ç»ƒç»“æœå¯è§†åŒ–
print("ğŸ“Š åˆ†æè®­ç»ƒç»“æœ...")

# ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
plt.figure(figsize=(15, 5))

# å·¦å›¾ï¼šå®Œæ•´è®­ç»ƒè¿‡ç¨‹
plt.subplot(1, 3, 1)
plt.plot(attention_losses, label='è®­ç»ƒæŸå¤±', color='red', linewidth=2)
plt.title('å®Œæ•´è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
plt.xlabel('è®­ç»ƒè½®æ¬¡')
plt.ylabel('æŸå¤±å€¼')
plt.legend()
plt.grid(True, alpha=0.3)

# ä¸­å›¾ï¼šè®­ç»ƒåæœŸæŸå¤±ï¼ˆæ›´æ¸…æ¥šåœ°çœ‹åˆ°æ”¶æ•›æƒ…å†µï¼‰
plt.subplot(1, 3, 2)
plt.plot(attention_losses[-100:], label='æœ€å100è½®', color='red', linewidth=2)
plt.title('è®­ç»ƒåæœŸæŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
plt.xlabel('è®­ç»ƒè½®æ¬¡ï¼ˆæœ€å100è½®ï¼‰')
plt.ylabel('æŸå¤±å€¼')
plt.legend()
plt.grid(True, alpha=0.3)

# å³å›¾ï¼šæŸå¤±ä¸‹é™ç»Ÿè®¡
plt.subplot(1, 3, 3)
initial_loss = attention_losses[0]
final_loss = attention_losses[-1]
min_loss = min(attention_losses)

stats = ['åˆå§‹æŸå¤±', 'æœ€ç»ˆæŸå¤±', 'æœ€ä½æŸå¤±']
values = [initial_loss, final_loss, min_loss]
colors = ['lightcoral', 'lightgreen', 'lightblue']

bars = plt.bar(stats, values, color=colors, edgecolor='black', linewidth=1)
plt.title('è®­ç»ƒæ•ˆæœç»Ÿè®¡', fontsize=14, fontweight='bold')
plt.ylabel('æŸå¤±å€¼')

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, value in zip(bars, values):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
            f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# è¾“å‡ºè®­ç»ƒæ•ˆæœåˆ†æ
print("\nğŸ“ˆ è®­ç»ƒæ•ˆæœåˆ†æ:")
print(f"   åˆå§‹æŸå¤±: {initial_loss:.4f}")
print(f"   æœ€ç»ˆæŸå¤±: {final_loss:.4f}")
print(f"   æœ€ä½æŸå¤±: {min_loss:.4f}")
print(f"   æŸå¤±é™å¹…: {((initial_loss - final_loss) / initial_loss * 100):.1f}%")

# åˆ¤æ–­è®­ç»ƒæ•ˆæœ
if final_loss < initial_loss * 0.3:
    print("   ğŸ‰ è®­ç»ƒæ•ˆæœ: éå¸¸å¥½ï¼æ¨¡å‹å­¦ä¹ å……åˆ†")
elif final_loss < initial_loss * 0.5:
    print("   ğŸ˜Š è®­ç»ƒæ•ˆæœ: è‰¯å¥½ï¼Œæ¨¡å‹åŸºæœ¬å­¦ä¼š")
else:
    print("   ğŸ˜ è®­ç»ƒæ•ˆæœ: ä¸€èˆ¬ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ")

print("\nâœ… è®­ç»ƒç»“æœåˆ†æå®Œæˆï¼æ¥ä¸‹æ¥æˆ‘ä»¬æµ‹è¯•ç¿»è¯‘æ•ˆæœ...")


# ğŸ”® ç¿»è¯‘æµ‹è¯•å‡½æ•°ï¼šéªŒè¯æ¨¡å‹æ•ˆæœ
def translate_with_attention(model, sentence, dataset, visualize=True):
    """
    ä½¿ç”¨å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹è¿›è¡Œç¿»è¯‘ï¼Œå¹¶å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡

    å‚æ•°:
        model: è®­ç»ƒå¥½çš„ç¿»è¯‘æ¨¡å‹
        sentence: è¦ç¿»è¯‘çš„è‹±æ–‡å¥å­
        dataset: æ•°æ®é›†ï¼ˆåŒ…å«è¯æ±‡è¡¨ï¼‰
        visualize: æ˜¯å¦å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    """
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    print(f"ğŸ”¤ æ­£åœ¨ç¿»è¯‘: '{sentence}'")

    with torch.no_grad():
        # æ­¥éª¤1: é¢„å¤„ç†è¾“å…¥å¥å­
        print("   ğŸ”§ é¢„å¤„ç†è¾“å…¥...")
        en_indices = dataset.sentence_to_indices(sentence, dataset.en_vocab, False)

        # å¡«å……åˆ°ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„é•¿åº¦
        en_padded = en_indices + [dataset.en_vocab["<PAD>"]] * (max_en_length - len(en_indices))
        en_tensor = torch.tensor([en_padded], dtype=torch.long).to(device)

        print(f"      åŸå¥é•¿åº¦: {len(en_indices)} è¯")
        print(f"      å¡«å……åé•¿åº¦: {len(en_padded)} è¯")

        # æ­¥éª¤2: ç”Ÿæˆç¿»è¯‘å’Œæ³¨æ„åŠ›æƒé‡
        print("   ğŸ¤– æ¨¡å‹æ¨ç†ä¸­...")
        output_indices, attention_weights = model(en_tensor)

        # æ­¥éª¤3: è½¬æ¢å›ä¸­æ–‡
        print("   ğŸ“ ç”Ÿæˆè¯‘æ–‡...")
        translated_words = []
        for idx in output_indices[0]:
            word = dataset.zh_idx2word.get(idx.item(), "<UNK>")
            if word == "<END>":
                break
            if word not in ["<START>", "<PAD>"]:
                translated_words.append(word)

        translation = "".join(translated_words)

        print(f"   âœ… ç¿»è¯‘å®Œæˆ: '{translation}'")

        # æ­¥éª¤4: å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
        if visualize and len(translated_words) > 0:
            print("   ğŸ¨ ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–...")
            visualize_attention_weights(
                sentence.split(),
                translated_words,
                attention_weights[0][:len(translated_words), :len(sentence.split())].cpu().numpy(),
                sentence
            )

        return translation


def visualize_attention_weights(input_words, output_words, attention_weights, original_sentence):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾"""
    plt.figure(figsize=(12, 8))

    # åˆ›å»ºçƒ­åŠ›å›¾
    im = plt.imshow(attention_weights, cmap='Blues', aspect='auto')

    # è®¾ç½®åæ ‡è½´æ ‡ç­¾
    plt.xticks(range(len(input_words)), input_words, rotation=45)
    plt.yticks(range(len(output_words)), output_words)

    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i in range(len(output_words)):
        for j in range(len(input_words)):
            plt.text(j, i, f'{attention_weights[i, j]:.2f}',
                     ha="center", va="center",
                     color="white" if attention_weights[i, j] > 0.5 else "black",
                     fontsize=10, fontweight='bold')

    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title(f'æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–\nåŸå¥: "{original_sentence}"',
              fontsize=14, fontweight='bold', pad=20)
    plt.xlabel('US è¾“å…¥è¯ï¼ˆè‹±æ–‡ï¼‰', fontsize=12, fontweight='bold')
    plt.ylabel('CN è¾“å‡ºè¯ï¼ˆä¸­æ–‡ï¼‰', fontsize=12, fontweight='bold')

    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im)
    cbar.set_label('æ³¨æ„åŠ›æƒé‡', fontsize=12, fontweight='bold')

    # æ·»åŠ è§£é‡Šæ–‡å­—
    plt.text(len(input_words) + 0.5, len(output_words) // 2,
             'é¢œè‰²è¶Šæ·±\nå…³æ³¨åº¦è¶Šé«˜\n\næ¯è¡Œæƒé‡\nä¹‹å’Œä¸º1',
             fontsize=10, ha='center', va='center',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))

    plt.tight_layout()
    plt.show()


print("ğŸ”® ç¿»è¯‘æµ‹è¯•å‡½æ•°å‡†å¤‡å®Œæˆï¼")

# ğŸ§ª ç¿»è¯‘æ•ˆæœæµ‹è¯•ï¼šè§è¯æ™ºèƒ½ç¿»è¯‘çš„é­…åŠ›
print("ğŸ§ª å¼€å§‹æµ‹è¯•ç¿»è¯‘æ•ˆæœ...")
print("=" * 60)

# å‡†å¤‡æµ‹è¯•å¥å­ï¼ˆåŒ…å«è®­ç»ƒè¿‡çš„å’Œæ²¡è®­ç»ƒè¿‡çš„ï¼‰
test_sentences = [
    "I love you",  # ç®€å•æƒ…æ„Ÿè¡¨è¾¾
    "How are you",  # æ—¥å¸¸é—®å€™
    "I want to go home",  # æ„æ„¿è¡¨è¾¾
    "The red car is fast",  # æè¿°æ€§å¥å­
    "Thank you very much",  # æ„Ÿè°¢è¡¨è¾¾
]

print(f"ğŸ“ æµ‹è¯•å¥å­æ•°é‡: {len(test_sentences)}")
print("ğŸ’¡ æ¯ä¸ªå¥å­éƒ½ä¼šæ˜¾ç¤º:")
print("   - ç¿»è¯‘è¿‡ç¨‹")
print("   - æœ€ç»ˆç»“æœ")
print("   - æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–")
print("   - ä¸è®­ç»ƒæ•°æ®çš„å¯¹æ¯”ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")

# é€ä¸ªæµ‹è¯•ç¿»è¯‘
for i, sentence in enumerate(test_sentences):
    print(f"\n{'=' * 50}")
    print(f"ğŸ¯ æµ‹è¯• {i + 1}/{len(test_sentences)}: {sentence}")
    print('=' * 50)

    # æ‰§è¡Œç¿»è¯‘
    translation = translate_with_attention(attention_model, sentence, dataset, visualize=True)

    print(f"\nğŸ“Š ç¿»è¯‘ç»“æœ:")
    print(f"   è¾“å…¥: {sentence}")
    print(f"   è¾“å‡º: {translation}")

    # æŸ¥æ‰¾è®­ç»ƒæ•°æ®ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„æ­£ç¡®ç­”æ¡ˆ
    ground_truth = None
    for en_orig, zh_orig in dataset.pairs:
        if en_orig.lower().strip() == sentence.lower().strip():
            ground_truth = zh_orig
            break

    if ground_truth:
        print(f"   å‚è€ƒ: {ground_truth}")
        # ç®€å•çš„ç›¸ä¼¼åº¦è¯„ä¼°
        if translation == ground_truth:
            print("   âœ… å®Œå…¨åŒ¹é…ï¼")
        elif len(set(translation) & set(ground_truth)) > 0:
            print("   ğŸ˜Š éƒ¨åˆ†åŒ¹é…ï¼Œæœ‰ç›¸åŒå­—ç¬¦")
        else:
            print("   ğŸ˜ ä¸å¤ªåŒ¹é…ï¼Œä½†å¯èƒ½ä»ç„¶åˆç†")
    else:
        print("   ğŸ“ æ–°å¥å­ï¼Œæ— å‚è€ƒç­”æ¡ˆ")

    print("\n" + "â”€" * 50)

print("\n" + "=" * 60)
print("ğŸ‰ ç¿»è¯‘æµ‹è¯•å®Œæˆï¼")
print("ğŸ’¡ è§‚å¯Ÿå‘ç°:")
print("   1. æ³¨æ„åŠ›æƒé‡æ¸…æ¥šæ˜¾ç¤ºäº†è¯æ±‡å¯¹åº”å…³ç³»")
print("   2. æ¨¡å‹å­¦ä¼šäº†åŸºæœ¬çš„è‹±ä¸­ç¿»è¯‘æ¨¡å¼")
print("   3. å¯¹äºè®­ç»ƒè¿‡çš„å¥å­ï¼Œç¿»è¯‘è´¨é‡è¾ƒé«˜")
print("   4. å¯¹äºæ–°å¥å­ï¼Œæ¨¡å‹ä¹Ÿèƒ½ç»™å‡ºåˆç†ç¿»è¯‘")
print("=" * 60)
