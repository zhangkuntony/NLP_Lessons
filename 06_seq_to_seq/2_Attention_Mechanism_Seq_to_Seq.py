# è§£å†³OpenMPå†²çªé—®é¢˜
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import jieba
from collections import Counter

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ä¸­æ–‡æ˜¾ç¤ºé…ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼")

#
# """
# ğŸ¨ å¯è§†åŒ–æ¼”ç¤ºï¼šåŸºç¡€Seq2Seqæ¨¡å‹çš„é—®é¢˜
#
# è¿™ä¸ªå‡½æ•°ç”¨ç”ŸåŠ¨çš„å›¾è¡¨æ¥å±•ç¤ºä¼ ç»ŸSeq2Seqæ¨¡å‹çš„ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼š
# 1. ä¿¡æ¯ç“¶é¢ˆé—®é¢˜ï¼šæ‰€æœ‰ä¿¡æ¯è¢«å‹ç¼©åˆ°ä¸€ä¸ªå°å‘é‡ä¸­
# 2. å¯¹é½é—®é¢˜ï¼šæ¨¡å‹ä¸çŸ¥é“è¾“å‡ºè¯å¯¹åº”å“ªä¸ªè¾“å…¥è¯
# """
#
# def visualize_basic_seq2seq_problem():
#     """ç”¨å›¾è¡¨å±•ç¤ºåŸºç¡€çš„seq2seqçš„é—®é¢˜ï¼Œè®©æŠ½è±¡æ¦‚å¿µå˜å¾—ç›´è§‚æ˜“æ‡‚"""
#
#     # åˆ›å»ºä¸¤ä¸ªå­å›¾ï¼šä¸Šæ–¹å±•ç¤ºä¿¡æ¯ç“¶é¢ˆï¼Œä¸‹æ–¹å±•ç¤ºå¯¹é½é—®é¢˜
#     fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))
#
#     # ===================ç¬¬ä¸€ä¸ªå›¾ï¼šä¿¡æ¯ç“¶é¢ˆé—®é¢˜===================
#     ax1.set_title('é—®é¢˜1ï¼šä¿¡æ¯ç“¶é¢ˆ - å°±åƒæŠŠæ•´æœ¬ä¹¦çš„å†…å®¹å†™åœ¨ä¾¿æ¡çº¸ä¸Š',
#                   fontsize=16, fontweight='bold', pad=20)
#
#     # ç”¨ä¸€ä¸ªé•¿å¥å­æ¥æ¼”ç¤ºä¿¡æ¯ç“¶é¢ˆé—®é¢˜
#     input_words = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']
#     print(f"ğŸ“ ç¤ºä¾‹å¥å­ï¼š{' '.join(input_words)}")
#     print("   è¿™ä¸ªå¥å­æœ‰9ä¸ªè¯ï¼ŒåŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ï¼ˆé¢œè‰²ã€åŠ¨ä½œã€å¯¹è±¡ç­‰ï¼‰")
#
#     # ç»˜åˆ¶è¾“å…¥è¯æ±‡ï¼ˆç”¨è“è‰²æ–¹å—è¡¨ç¤ºï¼‰
#     for i, word in enumerate(input_words):
#         # æ¯ä¸ªè¯ç”¨ä¸€ä¸ªå°æ–¹å—è¡¨ç¤º
#         rect = plt.Rectangle((i-0.3, 3), 0.6, 0.6, facecolor='lightblue', edgecolor='blue')
#         ax1.add_patch(rect)
#         ax1.text(i, 3.3, word, ha='center', va='center', fontsize=10)
#
#     # ç»˜åˆ¶ä¿¡æ¯æµåŠ¨ç®­å¤´ï¼ˆå±•ç¤ºåºåˆ—å¤„ç†è¿‡ç¨‹ï¼‰
#     for i in range(len(input_words) - 1):
#         ax1.arrow(i + 0.3, 3.3, 0.4, 0, head_width=0.05, head_length=0.05, fc='gray', ec='gray')
#
#     # ç»˜åˆ¶ç“¶é¢ˆå‘é‡ï¼ˆç”¨çº¢è‰²æ–¹å—è¡¨ç¤ºå‹ç¼©åçš„ä¿¡æ¯ï¼‰
#     ax1.add_patch(plt.Rectangle((4, 1.5), 1, 0.8, facecolor='red', edgecolor='darkred'))
#     ax1.text(4.5, 1.9, 'è¯­ä¹‰å‘é‡\n(ä¿¡æ¯ç“¶é¢ˆ)', ha='center', va='center',
#              fontsize=12, fontweight='bold', color='white')
#
#     # ç»˜åˆ¶å‹ç¼©ç®­å¤´ï¼ˆæ‰€æœ‰ä¿¡æ¯æ±‡èšåˆ°ä¸€ä¸ªå‘é‡ï¼‰
#     ax1.arrow(8.3, 3, -3.5, -0.7, head_width=0.08, head_length=0.1, fc='red', ec='red', linewidth=2)
#     ax1.text(6, 2.5, 'æ‰€æœ‰ä¿¡æ¯\nè¢«å‹ç¼©ï¼', ha='center', va='center',
#              fontsize=11, color='red', fontweight='bold')
#
#     # æ·»åŠ è¯´æ˜æ–‡å­—
#     ax1.text(1, 0.8, 'é—®é¢˜ï¼š9ä¸ªè¯çš„ä¸°å¯Œä¿¡æ¯ â†’ 1ä¸ªå›ºå®šå¤§å°çš„å‘é‡',
#              fontsize=12, fontweight='bold', color='red')
#     ax1.text(1, 0.5, '   å°±åƒæŠŠä¸€æ•´æœ¬å°è¯´å‹ç¼©æˆä¸€å¥è¯ï¼',
#              fontsize=10, color='darkred')
#
#     ax1.set_xlim(-1, 10)
#     ax1.set_ylim(0.3, 4)
#     ax1.axis('off')
#
#     # ===================ç¬¬äºŒä¸ªå›¾ï¼šå¯¹é½é—®é¢˜===================
#     ax2.set_title('é—®é¢˜2ï¼šæ— æ³•å¯¹é½ - ä¸çŸ¥é“å“ªä¸ªä¸­æ–‡è¯å¯¹åº”å“ªä¸ªè‹±æ–‡è¯',
#                   fontsize=16, fontweight='bold', pad=20)
#
#     # ç”¨ç®€å•çš„ç¿»è¯‘ä¾‹å­æ¼”ç¤ºå¯¹é½é—®é¢˜
#     en_words = ['I', 'love', 'machine', 'learning']
#     zh_words = ['æˆ‘', 'å–œæ¬¢', 'æœºå™¨', 'å­¦ä¹ ']
#
#     print(f"\\nç¿»è¯‘ç¤ºä¾‹ï¼š")
#     print(f"   è‹±æ–‡ï¼š{' '.join(en_words)}")
#     print(f"   ä¸­æ–‡ï¼š{''.join(zh_words)}")
#     print(f"   é—®é¢˜ï¼šæ¨¡å‹ä¸çŸ¥é“'machine'å¯¹åº”'æœºå™¨'")
#
#     # ç»˜åˆ¶è‹±æ–‡è¯æ±‡ï¼ˆç»¿è‰²æ–¹å—ï¼‰
#     for i, word in enumerate(en_words):
#         rect = plt.Rectangle((i * 2 - 0.4, 2.5), 0.8, 0.6, facecolor='lightgreen', edgecolor='green')
#         ax2.add_patch(rect)
#         ax2.text(i * 2, 2.8, word, ha='center', va='center', fontsize=12)
#
#     # ç»˜åˆ¶ä¸­æ–‡è¯æ±‡ï¼ˆé»„è‰²æ–¹å—ï¼‰
#     for i, word in enumerate(zh_words):
#         rect = plt.Rectangle((i * 2 - 0.4, 0.5), 0.8, 0.6, facecolor='lightyellow', edgecolor='orange')
#         ax2.add_patch(rect)
#         ax2.text(i * 2, 0.8, word, ha='center', va='center', fontsize=12)
#
#     # ç»˜åˆ¶æ­£ç¡®çš„å¯¹åº”å…³ç³»ï¼ˆç»¿è‰²è™šçº¿è¡¨ç¤ºç†æƒ³çš„å¯¹åº”ï¼‰
#     correct_alignments = [(0, 0), (1, 1), (2, 2), (3, 3)]
#     for en_idx, zh_idx in correct_alignments:
#         ax2.plot([en_idx * 2, zh_idx * 2], [2.5, 1.1], 'g--', linewidth=2, alpha=0.7)
#
#     # æ·»åŠ é—®å·å’Œè¯´æ˜ï¼ˆè¡¨ç¤ºæ¨¡å‹çš„å›°æƒ‘ï¼‰
#     ax2.text(3, 1.7, 'ï¼Ÿ', fontsize=30, ha='center', va='center')
#     ax2.text(5, 1.7, 'æ¨¡å‹ä¸çŸ¥é“\nå¯¹åº”å…³ç³»ï¼', ha='center', va='center',
#              fontsize=12, color='red', fontweight='bold')
#
#     # æ·»åŠ è¯´æ˜æ–‡å­—
#     ax2.text(0.5, 3.3, 'US è‹±æ–‡è¾“å…¥', fontsize=12, fontweight='bold', color='green')
#     ax2.text(0.5, 0.1, 'CN ä¸­æ–‡è¾“å‡º', fontsize=12, fontweight='bold', color='orange')
#     ax2.text(4.5, 1.3, 'è™šçº¿ = ç†æƒ³çš„å¯¹åº”å…³ç³»', fontsize=10, color='green')
#
#     ax2.set_xlim(-1, 8)
#     ax2.set_ylim(0, 3.5)
#     ax2.axis('off')
#
#     plt.tight_layout()
#     plt.show()
#
# # è¿è¡Œå¯è§†åŒ–æ¼”ç¤º
# print("ğŸ¨ å¼€å§‹å¯è§†åŒ–æ¼”ç¤ºåŸºç¡€Seq2Seqçš„é—®é¢˜...")
# print("=" * 60)
# visualize_basic_seq2seq_problem()
# print("=" * 60)
# print("ğŸ¯ çœ‹åˆ°è¿™äº›é—®é¢˜äº†å—ï¼Ÿè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ³¨æ„åŠ›æœºåˆ¶ï¼")
# print("ğŸ’¡ æ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜è€Œè¯ç”Ÿçš„ï¼")
#
#
# # å¯è§†åŒ–æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†
# def visualize_attention_mechanism():
#     fig, ax = plt.subplots(1, 1, figsize=(15, 8))
#
#     # è¾“å…¥åºåˆ—
#     input_words = ['The', 'red', 'car', 'is', 'very', 'fast']
#     output_words = ['è¿™', 'çº¢è‰²', 'æ±½è½¦', 'éå¸¸', 'å¿«']
#
#     # åˆ›å»ºæ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼ˆç¤ºä¾‹ï¼‰
#     attention_weights = np.array([
#         [0.8, 0.1, 0.05, 0.03, 0.01, 0.01],  # ç¿»è¯‘"è¿™"æ—¶ä¸»è¦å…³æ³¨"The"
#         [0.1, 0.8, 0.05, 0.02, 0.02, 0.01],  # ç¿»è¯‘"çº¢è‰²"æ—¶ä¸»è¦å…³æ³¨"red"
#         [0.05, 0.1, 0.8, 0.03, 0.01, 0.01],  # ç¿»è¯‘"æ±½è½¦"æ—¶ä¸»è¦å…³æ³¨"car"
#         [0.02, 0.02, 0.03, 0.1, 0.8, 0.03],  # ç¿»è¯‘"éå¸¸"æ—¶ä¸»è¦å…³æ³¨"very"
#         [0.01, 0.01, 0.02, 0.06, 0.1, 0.8],  # ç¿»è¯‘"å¿«"æ—¶ä¸»è¦å…³æ³¨"fast"
#     ])
#
#     # ç»˜åˆ¶çƒ­åŠ›å›¾
#     im = ax.imshow(attention_weights, cmap='Reds', aspect='auto')
#
#     # è®¾ç½®æ ‡ç­¾
#     ax.set_xticks(range(len(input_words)))
#     ax.set_yticks(range(len(output_words)))
#     ax.set_xticklabels(input_words, fontsize=14)
#     ax.set_yticklabels(output_words, fontsize=14)
#
#     # æ·»åŠ æ•°å€¼æ ‡æ³¨
#     for i in range(len(output_words)):
#         for j in range(len(input_words)):
#             text = ax.text(j, i, f'{attention_weights[i, j]:.2f}',
#                            ha="center", va="center", color="black" if attention_weights[i, j] < 0.5 else "white",
#                            fontsize=10, fontweight='bold')
#
#     # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
#     ax.set_title('æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼šæ¨¡å‹å…³æ³¨çš„ç„¦ç‚¹', fontsize=16, fontweight='bold', pad=20)
#     ax.set_xlabel('è¾“å…¥è¯ï¼ˆè‹±æ–‡ï¼‰', fontsize=14, fontweight='bold')
#     ax.set_ylabel('è¾“å‡ºè¯ï¼ˆä¸­æ–‡ï¼‰', fontsize=14, fontweight='bold')
#
#     # æ·»åŠ é¢œè‰²æ¡
#     cbar = plt.colorbar(im, ax=ax)
#     cbar.set_label('æ³¨æ„åŠ›æƒé‡', fontsize=12, fontweight='bold')
#
#     # æ·»åŠ è§£é‡Šæ–‡å­—
#     ax.text(len(input_words) + 0.5, len(output_words) // 2,
#             'é¢œè‰²è¶Šæ·±\n= å…³æ³¨åº¦è¶Šé«˜\n\næ¯è¡Œæƒé‡\nä¹‹å’Œä¸º1',
#             fontsize=12, ha='center', va='center',
#             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
#
#     plt.tight_layout()
#     plt.show()
#
# visualize_attention_mechanism()
# print("ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹çŸ¥é“è¯¥å…³æ³¨ä»€ä¹ˆï¼")
#
#
# """
# ğŸ”§ åŠ¨æ‰‹å®ç°ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒç®—æ³•
#
# è®©æˆ‘ä»¬æŠŠåˆšæ‰å­¦åˆ°çš„"é€‰æœ‹å‹å¸®å¿™"ç†è®ºè½¬æ¢æˆå®é™…çš„ä»£ç ï¼
# è¿™ä¸ªç®€å•çš„æ³¨æ„åŠ›æ¨¡å—å®Œç¾æ¼”ç¤ºäº†ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤ã€‚
# """
#
# class SimpleAttention(nn.Module):
#     """
#     ç®€å•æ³¨æ„åŠ›æ¨¡å— - æŠŠç†è®ºå˜æˆä»£ç ï¼
#
#     è¿™å°±æ˜¯æˆ‘ä»¬åˆšæ‰è®¨è®ºçš„"é€‰æœ‹å‹å¸®å¿™"ç®—æ³•çš„ä»£ç å®ç°ï¼š
#     1. è¯„ä¼°æ¯ä¸ªæœ‹å‹çš„æœ‰ç”¨ç¨‹åº¦
#     2. å†³å®šå‘æ¯ä¸ªæœ‹å‹æ±‚åŠ©çš„æ¯”ä¾‹
#     3. ç»¼åˆæ‰€æœ‰æœ‹å‹çš„å»ºè®®
#     """
#
#     def __init__(self, hidden_dim):
#         super(SimpleAttention, self).__init__()
#         self.hidden_dim = hidden_dim
#         print(f"ğŸ—ï¸ åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—ï¼Œéšè—ç»´åº¦: {hidden_dim}")
#
#     def forward(self, decoder_hidden, encoder_outputs):
#         """
#         æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒè®¡ç®—è¿‡ç¨‹
#
#         å‚æ•°è§£é‡Šï¼ˆç”¨æˆ‘ä»¬çš„æ¯”å–»ï¼‰:
#             decoder_hidden: ä½ å½“å‰çš„"å›°æƒ‘çŠ¶æ€" [batch_size, hidden_dim]
#                           ï¼ˆä½ è¦ç¿»è¯‘ä»€ä¹ˆè¯ï¼Ÿä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿï¼‰
#             encoder_outputs: æ‰€æœ‰æœ‹å‹çš„"ä¸“ä¸šçŸ¥è¯†" [batch_size, seq_len, hidden_dim]
#                            ï¼ˆæ¯ä¸ªè¾“å…¥è¯èƒ½æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿï¼‰
#
#         è¿”å›ç»“æœ:
#             context_vector: "ç»¼åˆæ‰€æœ‰å»ºè®®çš„æœ€ç»ˆç­”æ¡ˆ" [batch_size, hidden_dim]
#             attention_weights: "å‘æ¯ä¸ªæœ‹å‹æ±‚åŠ©çš„æ¯”ä¾‹" [batch_size, seq_len]
#         """
#
#         print(f"\\nğŸ§  å¼€å§‹æ³¨æ„åŠ›è®¡ç®—...")
#         print(f"   å½“å‰çŠ¶æ€å½¢çŠ¶: {decoder_hidden.shape}")
#         print(f"   è¾“å…¥ä¿¡æ¯å½¢çŠ¶: {encoder_outputs.shape}")
#
#         # =================== æ­¥éª¤1: è¯„ä¼°æœ‹å‹çš„æœ‰ç”¨ç¨‹åº¦ ===================
#         print("\\nğŸ¯ æ­¥éª¤1: è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆè¯„ä¼°æœ‹å‹æœ‰ç”¨ç¨‹åº¦ï¼‰")
#
#         # ä½¿ç”¨ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦ - å°±åƒé—®â€œä½ çš„ä¸“é•¿å’Œæˆ‘çš„éœ€æ±‚æœ‰å¤šåŒ¹é…ï¼â€
#         # æ•°å­¦åŸç†ï¼šç‚¹ç§¯è¶Šå¤§ = å‘é‡è¶Šç›¸ä¼¼ = æœ‹å‹è¶Šæœ‰ç”¨
#         scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2))
#         scores = scores.squeeze(2)          # [batch_size, seq_len]
#
#         print(f"   ç›¸å…³æ€§åˆ†æ•°: {scores.squeeze().detach().numpy()}")
#         print("   ğŸ’¡ åˆ†æ•°è¶Šé«˜ = æœ‹å‹è¶Šæœ‰ç”¨ï¼")
#
#         # =================== æ­¥éª¤2: åˆ†é…æ³¨æ„åŠ›æ¯”ä¾‹ ===================
#         print("\\nğŸ“Š æ­¥éª¤2: è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆåˆ†é…æ±‚åŠ©æ¯”ä¾‹ï¼‰")
#
#         # ä½¿ç”¨softmaxç¡®ä¿æ‰€æœ‰æƒé‡åŠ èµ·æ¥= 100%
#         # å°±åƒæŠŠè¯„åˆ†è½¬æ¢æˆç™¾åˆ†æ¯”åˆ†é…
#
#         attention_weights = F.softmax(scores, dim=1)            # [batch_size, seq_len]
#
#         print(f"   æ³¨æ„åŠ›æƒé‡: {attention_weights.squeeze().detach().numpy()}")
#         print(f"   æƒé‡æ€»å’Œ: {attention_weights.sum().item():.4f} (åº”è¯¥=1.0)")
#         print("   ğŸ’¡ è¿™å°±æ˜¯ç»™æ¯ä¸ªæœ‹å‹åˆ†é…çš„æ³¨æ„åŠ›æ¯”ä¾‹ï¼")
#
#         # =================== æ­¥éª¤3: ç»¼åˆæ‰€æœ‰å»ºè®® ===================
#         print("\\nğŸ¤ æ­¥éª¤3: è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ï¼ˆç»¼åˆæœ‹å‹å»ºè®®ï¼‰")
#
#         # åŠ æƒå¹³å‡ - æŒ‰æ¯”ä¾‹ç»„åˆæ‰€æœ‰æœ‹å‹çš„å»ºè®®
#         # æƒé‡é«˜çš„æœ‹å‹ï¼Œä»–çš„å»ºè®®å½±å“æ›´å¤§
#         context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
#         context_vector = context_vector.squeeze(1)              # [batch_size, hidden_dim]
#
#         print(f"   æœ€ç»ˆä¸Šä¸‹æ–‡å‘é‡å½¢çŠ¶: {context_vector.shape}")
#         print("   ğŸ’¡ è¿™å°±æ˜¯ç»¼åˆæ‰€æœ‰æœ‹å‹å»ºè®®åçš„æœ€ç»ˆç­”æ¡ˆï¼")
#
#         return context_vector, attention_weights
#
# def demonstrate_attention():
#     """
#     ğŸª æ³¨æ„åŠ›æœºåˆ¶ç°åœºæ¼”ç¤º
#
#     ç”¨å…·ä½“çš„æ•°å­—æ¥å±•ç¤ºæ³¨æ„åŠ›æœºåˆ¶æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œ
#     è®©æŠ½è±¡çš„æ¦‚å¿µå˜å¾—å…·ä½“å¯è§ï¼
#     """
#
#     print("ğŸ¬ æ³¨æ„åŠ›æœºåˆ¶ç°åœºæ¼”ç¤ºå¼€å§‹ï¼")
#     print("=" * 60)
#
#     # è®¾ç½®æ¼”ç¤ºå‚æ•°
#     batch_size = 1          # ä¸€æ¬¡å¤„ç†ä¸€ä¸ªå¥å­
#     seq_len = 4             # è¾“å…¥å¥å­æœ‰4ä¸ªè¯ï¼ˆæ¯”å¦‚ "I love machine learning"ï¼‰
#     hidden_dim = 8          # æ¯ä¸ªè¯ç”¨8ç»´å‘é‡è¡¨ç¤º
#
#     print(f"ğŸ“ æ¼”ç¤ºè®¾ç½®ï¼š")
#     print(f"   å¥å­é•¿åº¦: {seq_len}ä¸ªè¯")
#     print(f"   å‘é‡ç»´åº¦: {hidden_dim}ç»´")
#     print(f"   æƒ³è±¡å¥å­: 'I love machine learning'")
#     print(f"   è¦ç¿»è¯‘çš„è¯: 'machine' â†’ 'æœºå™¨'")
#
#     # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼ˆè¿™äº›é€šå¸¸æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒå‡ºæ¥çš„ï¼‰
#     print("\\nğŸ² åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
#     torch.manual_seed(42)           # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
#
#     # è§£ç å™¨å½“å‰çŠ¶æ€ï¼šè¡¨ç¤ºâ€œæˆ‘ç°åœ¨è¦ç¿»è¯‘machineè¿™ä¸ªè¯â€
#     decoder_hidden = torch.randn(batch_size, hidden_dim)
#     print(f"    è§£ç å™¨çŠ¶æ€ï¼ˆè¦ç¿»è¯‘'machine'ï¼‰ï¼šå·²åˆ›å»º")
#
#     # ç¼–ç å™¨è¾“å‡ºï¼šè¡¨ç¤ºæ¯ä¸ªè¾“å…¥è¯çš„ä¿¡æ¯
#     encoder_outputs = torch.randn(batch_size, seq_len, hidden_dim)
#     print(f"    ç¼–ç å™¨è¾“å‡ºï¼ˆ4ä¸ªè¯çš„ä¿¡æ¯ï¼‰ï¼šå·²åˆ›å»º")
#
#     # åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—
#     print("\\nğŸ—ï¸ åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—...")
#     attention = SimpleAttention(hidden_dim)
#
#     # ğŸ¬ å¼€å§‹æ³¨æ„åŠ›è®¡ç®—ï¼
#     print("\\nğŸš€ å¼€å§‹æ³¨æ„åŠ›è®¡ç®—...")
#     context_vector, attention_weights = attention(decoder_hidden, encoder_outputs)
#
#     # ğŸ“Š ç»“æœåˆ†æ
#     print("\\n" + "="*60)
#     print("ğŸ“Š è®¡ç®—ç»“æœåˆ†æï¼š")
#     print("="*60)
#
#     weights_array = attention_weights.squeeze().detach().numpy()
#
#     # åˆ›å»ºè¯æ±‡æ ‡ç­¾ï¼ˆæ¨¡æ‹Ÿï¼‰
#     word_labels = ['I', 'love', 'machine', 'learning']
#
#     print("\\nğŸ¯ æ³¨æ„åŠ›æƒé‡åˆ†æï¼š")
#     for i, (word, weight) in enumerate(zip(word_labels, weights_array)):
#         percentage = weight * 100
#         bar = "â–ˆ" * int(percentage // 5)                # ç®€å•çš„æ¡å½¢å›¾
#         print(f"   {word:>8}: {weight:.3f} ({percentage:5.1f}%) {bar}")
#
#     # æ‰¾å‡ºæœ€å…³æ³¨çš„è¯
#     max_idx = weights_array.argmax()
#     max_word = word_labels[max_idx]
#     max_weight = weights_array[max_idx]
#
#     print(f"\\nğŸ’¡ æ¨¡å‹æœ€å…³æ³¨: '{max_word}' (æƒé‡: {max_weight:.3f})")
#     print(f"   è¿™è¯´æ˜ç¿»è¯‘'machine'æ—¶ï¼Œæ¨¡å‹ä¸»è¦å‚è€ƒ'{max_word}'è¿™ä¸ªè¯ï¼")
#
#     # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
#     print("\\nğŸ¨ ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨...")
#     plt.figure(figsize=(14, 5))
#
#     # å·¦å›¾ï¼šæ¡å½¢å›¾
#     plt.subplot(1, 3, 1)
#     colors = ['lightblue' if i != max_idx else 'red' for i in range(len(weights_array))]
#     bars = plt.bar(range(len(weights_array)), weights_array, color=colors, edgecolor='navy')
#     plt.title('æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
#     plt.xlabel('è¾“å…¥è¯')
#     plt.ylabel('æ³¨æ„åŠ›æƒé‡')
#     plt.xticks(range(len(word_labels)), word_labels, rotation=45)
#     plt.ylim(0, 1)
#     plt.grid(True, alpha=0.3)
#
#     # æ·»åŠ æ•°å€¼æ ‡ç­¾
#     for bar, weight in zip(bars, weights_array):
#         plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
#                  f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')
#
#     # ä¸­å›¾ï¼šé¥¼å›¾
#     plt.subplot(1, 3, 2)
#     colors_pie = ['lightblue', 'lightgreen', 'red', 'lightyellow']
#     plt.pie(weights_array, labels=word_labels, autopct='%1.1f%%',
#             colors=colors_pie, startangle=90)
#     plt.title('æ³¨æ„åŠ›æƒé‡æ¯”ä¾‹', fontsize=14, fontweight='bold')
#
#     # å³å›¾ï¼šçƒ­åŠ›å›¾
#     plt.subplot(1, 3, 3)
#     weights_matrix = weights_array.reshape(1, -1)
#     plt.imshow(weights_matrix, cmap='Reds', aspect='auto')
#     plt.title('æ³¨æ„åŠ›çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
#     plt.xticks(range(len(word_labels)), word_labels)
#     plt.yticks([0], ['attention'])
#
#     # æ·»åŠ æ•°å€¼
#     for i, weight in enumerate(weights_array):
#         plt.text(i, 0, f'{weight:.2f}', ha='center', va='center',
#                  color='white' if weight > 0.5 else 'black', fontweight='bold')
#
#     plt.tight_layout()
#     plt.show()
#
#     print("\\n" + "=" * 60)
#     print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
#     print("âœ… è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒè®¡ç®—è¿‡ç¨‹ï¼")
#     print("ğŸ’¡ æ¨¡å‹å­¦ä¼šäº†åŠ¨æ€åœ°é€‰æ‹©æœ€ç›¸å…³çš„è¾“å…¥ä¿¡æ¯ï¼")
#     print("=" * 60)
#
# # ğŸ¬ å¼€å§‹æ¼”ç¤ºï¼
# print("ğŸª æ¬¢è¿æ¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ç°åœºæ¼”ç¤ºï¼")
# demonstrate_attention()


# åŠ¨æ‰‹å®è·µï¼šæ„å»ºå®Œæ•´çš„æ³¨æ„åŠ›ç¿»è¯‘ç³»ç»Ÿ
"""
ğŸ“Š ç¬¬ä¸€æ­¥ï¼šæ•°æ®å‡†å¤‡ - ä¸ºæˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººå‡†å¤‡"å­¦ä¹ ææ–™"

å°±åƒæ•™å­©å­å­¦è‹±è¯­éœ€è¦å‡†å¤‡è¯¾æœ¬ä¸€æ ·ï¼Œæˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººä¹Ÿéœ€è¦å¤§é‡çš„ä¸­è‹±å¯¹ç…§å¥å­æ¥å­¦ä¹ ã€‚
è¿™ä¸€æ­¥æˆ‘ä»¬å°†ä»cmn.txtæ–‡ä»¶ä¸­åŠ è½½çœŸå®çš„ç¿»è¯‘æ•°æ®é›†ï¼Œå¹¶æŠŠæ–‡å­—è½¬æ¢æˆæœºå™¨èƒ½ç†è§£çš„æ•°å­—ã€‚
"""

import re
import string
class EnhancedTranslationDatast:
    """
    å¢å¼ºç‰ˆç¿»è¯‘æ•°æ®é›† - æˆ‘ä»¬çš„"ç”µå­è¯¾æœ¬"

    è¿™ä¸ªç±»çš„ä½œç”¨å°±åƒä¸€ä¸ªæ™ºèƒ½çš„è¯­è¨€è¯¾æœ¬ï¼Œå®ƒèƒ½ï¼š
    1. ä»cmn.txtæ–‡ä»¶ä¸­åŠ è½½å¤§é‡çš„ä¸­è‹±å¯¹ç…§å¥å­
    2. æŠŠæ–‡å­—è½¬æ¢æˆæœºå™¨èƒ½ç†è§£çš„æ•°å­—
    3. ä¸ºè®­ç»ƒè¿‡ç¨‹æä¾›è§„æ•´çš„æ•°æ®
    """

    def __init__(self, data_file="cmn.txt", max_pairs=5000):
        print("ğŸ“š æ­£åœ¨å‡†å¤‡ç¿»è¯‘æ•°æ®é›†...")
        print(f"ğŸ“ ä»æ–‡ä»¶ {data_file} åŠ è½½æ•°æ®...")

        # ä»cmn.txtæ–‡ä»¶ä¸­åŠ è½½æ•°æ®
        self.pairs = self.load_data_from_file(data_file, max_pairs)

        print(f"ğŸ“ æˆåŠŸåŠ è½½äº† {len(self.pairs)} å¯¹ä¸­è‹±å¥å­")
        print("ğŸ’¡ è¿™äº›å¥å­æ¥è‡ªçœŸå®çš„ç¿»è¯‘æ•°æ®é›†")

        # å¼€å§‹æ„å»ºè¯æ±‡è¡¨
        self.prepare_vocabularies()

    def load_data_from_file(self, filename, max_pairs=5000):
        """
        ä»cmn.txtæ–‡ä»¶ä¸­åŠ è½½ç¿»è¯‘æ•°æ®

        cmn.txtæ–‡ä»¶æ ¼å¼é€šå¸¸æ˜¯ï¼š
        è‹±æ–‡å¥å­ \t ä¸­æ–‡å¥å­ \t å…¶ä»–ä¿¡æ¯
        """
        pairs = []

        try:
            with open(filename, 'r', encoding='utf-8') as file:
                print("ğŸ“– æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶...")

                for line_num, line in enumerate(file):
                    if line_num >= max_pairs:
                        break

                    # å»é™¤æ¢è¡Œç¬¦å¹¶åˆ†å‰²
                    line = line.strip()
                    if not line:
                        continue

                    # é€šå¸¸cmn.txtæ ¼å¼æ˜¯ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”çš„
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        en_sentence = parts[0].strip()
                        zh_sentence = parts[1].strip()

                        # æ•°æ®æ¸…æ´—ï¼šå»é™¤æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦
                        en_sentence = self.clean_english_sentence(en_sentence)
                        zh_sentence = self.clean_chinese_sentence(zh_sentence)

                        # è¿‡æ»¤æ‰è¿‡é•¿æˆ–è¿‡çŸ­çš„å¥å­
                        if 3 <= len(en_sentence.split()) <= 12 and 2 <= len(zh_sentence) <= 15:
                            pairs.append((en_sentence, zh_sentence))

                    # æ˜¾ç¤ºè¿›åº¦
                    if (line_num + 1) % 1000 == 0:
                        print(f"   å·²å¤„ç† {line_num + 1} è¡Œ...")

            print(f"âœ… æ–‡ä»¶è¯»å–å®Œæˆï¼å…±å¤„ç†äº† {line_num + 1} è¡Œ")
            print(f"âœ… ç­›é€‰å‡º {len(pairs)} å¯¹ç¬¦åˆæ¡ä»¶çš„å¥å­")

        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ {filename}")
            print("ğŸ’¡ ä½¿ç”¨é»˜è®¤çš„ç¤ºä¾‹æ•°æ®...")
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®
            pairs = [
                ("I love you", "æˆ‘çˆ±ä½ "),
                ("Hello world", "ä½ å¥½ä¸–ç•Œ"),
                ("Good morning", "æ—©ä¸Šå¥½"),
                ("How are you", "ä½ å¥½å—"),
                ("Thank you very much", "éå¸¸æ„Ÿè°¢ä½ "),
                ("See you later", "å†è§"),
                ("I am very happy", "æˆ‘éå¸¸å¼€å¿ƒ"),
                ("This is really good", "è¿™çœŸçš„å¾ˆå¥½"),
                ("I like eating apples", "æˆ‘å–œæ¬¢åƒè‹¹æœ"),
                ("Today is very sunny", "ä»Šå¤©éå¸¸æ™´æœ—"),
                ("I want to drink water", "æˆ‘æƒ³å–æ°´"),
                ("You are very nice", "ä½ å¾ˆå¥½"),
                ("I need your help", "æˆ‘éœ€è¦ä½ çš„å¸®åŠ©"),
                ("This book is easy", "è¿™æœ¬ä¹¦å¾ˆå®¹æ˜“"),
                ("I want to go home", "æˆ‘æƒ³å›å®¶"),
                ("The red car is fast", "çº¢è‰²æ±½è½¦å¾ˆå¿«"),
                ("She likes beautiful flowers", "å¥¹å–œæ¬¢ç¾ä¸½çš„èŠ±"),
                ("We study machine learning", "æˆ‘ä»¬å­¦ä¹ æœºå™¨å­¦ä¹ "),
                ("The weather is nice today", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"),
                ("I enjoy reading books", "æˆ‘å–œæ¬¢è¯»ä¹¦"),
            ]

        # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
        print(f"\nğŸ“‹ æ•°æ®æ ·æœ¬é¢„è§ˆ:")
        for i, (en, zh) in enumerate(pairs[:5]):
            print(f"   æ ·æœ¬{i + 1}: '{en}' â†’ '{zh}'")

        return pairs

    def clean_english_sentence(self, sentence):
        """æ¸…æ´—è‹±æ–‡å¥å­ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œç»Ÿä¸€æ ¼å¼"""
        # è½¬æ¢ä¸ºå°å†™
        sentence = sentence.lower()
        # å»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
        sentence = re.sub(r'[^\w\s\']', '', sentence)
        # å»é™¤å¤šä½™ç©ºæ ¼
        sentence = ' '.join(sentence.split())
        return sentence

    def clean_chinese_sentence(self, sentence):
        """æ¸…æ´—ä¸­æ–‡å¥å­ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œç»Ÿä¸€æ ¼å¼"""
        # å»é™¤è‹±æ–‡å­—ç¬¦å’Œæ ‡ç‚¹
        sentence = re.sub(r'[a-zA-Z\d\s.,!?;:\"\'()[\]{}]', '', sentence)
        # å»é™¤ç‰¹æ®Šæ ‡ç‚¹ç¬¦å·
        sentence = re.sub(r'[ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘{}]', '', sentence)
        return sentence.strip()

    def prepare_vocabularies(self):
        """
        æ„å»ºè¯æ±‡è¡¨ - åˆ¶ä½œ"å­—å…¸"

        å°±åƒå­¦å¤–è¯­è¦å…ˆåˆ¶ä½œå­—å…¸ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ï¼š
        1. æ‰¾å‡ºæ‰€æœ‰å‡ºç°çš„è‹±æ–‡å•è¯å’Œä¸­æ–‡è¯æ±‡
        2. ç»™æ¯ä¸ªè¯åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•°å­—ID
        3. æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆå¼€å§‹ã€ç»“æŸã€æœªçŸ¥è¯ç­‰ï¼‰
        """
        print("\\nğŸ”¨ å¼€å§‹æ„å»ºè¯æ±‡è¡¨...")

        # ç‰¹æ®Šæ ‡è®° - å°±åƒæ ‡ç‚¹ç¬¦å·ä¸€æ ·é‡è¦
        # <PAD>: å¡«å……ç¬¦ï¼Œç”¨äºè®©æ‰€æœ‰å¥å­é•¿åº¦ä¸€è‡´
        # <START>: å¥å­å¼€å§‹æ ‡è®°
        # <END>: å¥å­ç»“æŸæ ‡è®°
        # <UNK>: æœªçŸ¥è¯æ ‡è®°ï¼Œç”¨äºå¤„ç†æ²¡è§è¿‡çš„è¯
        self.en_vocab = {"<PAD>": 0, "<START>": 1, "<END>": 2, "<UNK>": 3}
        self.zh_vocab = {"<PAD>": 0, "<START>": 1, "<END>": 2, "<UNK>": 3}

        print("   æ·»åŠ ç‰¹æ®Šæ ‡è®°: <PAD>, <START>, <END>, <UNK>")

        # æ”¶é›†æ‰€æœ‰è¯æ±‡
        en_words = set()  # è‹±æ–‡å•è¯é›†åˆ
        zh_words = set()  # ä¸­æ–‡è¯æ±‡é›†åˆ

        print("\\nğŸ” æ‰«ææ‰€æœ‰å¥å­ï¼Œæ”¶é›†è¯æ±‡...")
        for i, (en_sentence, zh_sentence) in enumerate(self.pairs):
            # è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯
            en_words.update(en_sentence.lower().split())
            # ä¸­æ–‡ä½¿ç”¨jiebaåˆ†è¯
            zh_words.update(jieba.cut(zh_sentence))

            if i < 3:               # æ˜¾ç¤ºå‰3ä¸ªä¾‹å­
                print(f"   ä¾‹å­{i + 1}: '{en_sentence}' â†’ '{zh_sentence}'")
                print(f"           è‹±æ–‡è¯: {en_sentence.lower().split()}")
                print(f"           ä¸­æ–‡è¯: {list(jieba.cut(zh_sentence))}")

        print(f"\\nğŸ“Š è¯æ±‡ç»Ÿè®¡:")
        print(f"   å‘ç°è‹±æ–‡å•è¯: {len(en_words)} ä¸ª")
        print(f"   å‘ç°ä¸­æ–‡è¯æ±‡: {len(zh_words)} ä¸ª")

        # æ„å»ºè¯æ±‡è¡¨ï¼ˆç»™æ¯ä¸ªè¯åˆ†é…IDï¼‰
        # ä»ID=4å¼€å§‹ï¼Œå› ä¸º0-3è¢«ç‰¹æ®Šæ ‡è®°å ç”¨
        for i, word in enumerate(sorted(en_words), 4):
            self.en_vocab[word] = i

        for i, word in enumerate(sorted(zh_words), 4):
            self.zh_vocab[word] = i

        # åˆ›å»ºåå‘è¯æ±‡è¡¨ï¼ˆä»IDæŸ¥æ‰¾è¯æ±‡ï¼‰
        self.en_idx2word = {idx: word for word, idx in self.en_vocab.items()}
        self.zh_idx2word = {idx: word for word, idx in self.zh_vocab.items()}

        print(f"\\nâœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼")
        print(f"   è‹±æ–‡è¯æ±‡è¡¨å¤§å°: {len(self.en_vocab)}")
        print(f"   ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {len(self.zh_vocab)}")

        # æ˜¾ç¤ºä¸€äº›è¯æ±‡è¡¨å†…å®¹ä½œä¸ºä¾‹å­
        print(f"\\nğŸ“– è‹±æ–‡è¯æ±‡è¡¨ç¤ºä¾‹:")
        for word, idx in list(self.en_vocab.items())[:8]:
            print(f"   '{word}' â†’ {idx}")

        print(f"\\nğŸ“– ä¸­æ–‡è¯æ±‡è¡¨ç¤ºä¾‹:")
        for word, idx in list(self.zh_vocab.items())[:8]:
            print(f"   '{word}' â†’ {idx}")

    def sentence_to_indices(self, sentence, vocab, is_chinese=False):
        """
        å°†å¥å­è½¬æ¢ä¸ºæ•°å­—åºåˆ—

        æœºå™¨åªèƒ½ç†è§£æ•°å­—ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦æŠŠæ–‡å­—å¥å­è½¬æ¢æˆæ•°å­—åºåˆ—ã€‚
        å°±åƒæŠŠ "I love you" è½¬æ¢æˆ [5, 12, 8] è¿™æ ·çš„æ•°å­—åˆ—è¡¨ã€‚
        """
        if is_chinese:
            words = list(jieba.cut(sentence))           # ä¸­æ–‡åˆ†è¯
        else:
            words = sentence.lower().split()            # è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯

        # æŸ¥æ‰¾æ¯ä¸ªè¯çš„IDï¼Œå¦‚æœè¯æ±‡è¡¨ä¸­æ²¡æœ‰å°±ç”¨<UNK>
        indices = [vocab.get(word, vocab["<UNK>"]) for word in words]
        return indices

    def get_training_data(self):
        """
        è·å–è®­ç»ƒæ•°æ® - æŠŠæ‰€æœ‰å¥å­è½¬æ¢æˆæ•°å­—åºåˆ—

        è¿™ä¸€æ­¥å°†æ‰€æœ‰çš„ä¸­è‹±å¥å­å¯¹è½¬æ¢æˆæœºå™¨å­¦ä¹ éœ€è¦çš„æ•°å­—æ ¼å¼ã€‚
        """
        print("\\nğŸ”„ å°†æ‰€æœ‰å¥å­è½¬æ¢ä¸ºæ•°å­—åºåˆ—...")

        en_sequences = []
        zh_sequences = []

        for i, (en_sentence, zh_sentence) in enumerate(self.pairs):
            # è½¬æ¢è‹±æ–‡å¥å­
            en_indices = self.sentence_to_indices(en_sentence, self.en_vocab, False)

            # è½¬æ¢ä¸­æ–‡å¥å­ï¼ˆæ³¨æ„ï¼šä¸­æ–‡å¥å­å‰åè¦åŠ <START>å’Œ<END>ï¼‰
            zh_indices = ([self.zh_vocab["<START>"]] + self.sentence_to_indices(zh_sentence, self.zh_vocab, True) + [self.en_vocab["<END>"]])

            en_sequences.append(en_indices)
            zh_sequences.append(zh_indices)

            # æ˜¾ç¤ºå‰3ä¸ªè½¬æ¢ä¾‹å­
            if i < 3:
                print(f"\\n   ä¾‹å­{i + 1}:")
                print(f"   è‹±æ–‡: '{en_sentence}' â†’ {en_indices}")
                print(f"   ä¸­æ–‡: '{zh_sentence}' â†’ {zh_indices}")

        print(f"\\nâœ… è½¬æ¢å®Œæˆï¼å…±å¤„ç† {len(en_sequences)} ä¸ªå¥å­å¯¹")
        return en_sequences, zh_sequences

def pad_sequences(sequences, max_length=None, pad_value=0):
    """
    åºåˆ—å¡«å……å‡½æ•° - è®©æ‰€æœ‰å¥å­é•¿åº¦ä¸€è‡´

    å°±åƒæ’é˜Ÿæ—¶è¦ç«™æ•´é½ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦è®©æ‰€æœ‰å¥å­é•¿åº¦ä¸€è‡´ï¼Œ
    è¿™æ ·æœºå™¨æ‰èƒ½æ‰¹é‡å¤„ç†ã€‚çŸ­å¥å­ç”¨<PAD>å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦ã€‚
    """
    if max_length is None:
        max_length = max(len(seq) for seq in sequences)

    print(f"\\nğŸ“ å¡«å……åºåˆ—åˆ°ç»Ÿä¸€é•¿åº¦ {max_length}...")

    padded_sequences = []
    for seq in sequences:
        if len(seq) < max_length:
            # çŸ­å¥å­ç”¨PADå¡«å……
            padded_seq = seq + [pad_value] * (max_length - len(seq))
        else:
            # é•¿å¥å­æˆªæ–­ï¼ˆè™½ç„¶æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ä¸ä¼šå‡ºç°è¿™ç§æƒ…å†µ
            padded_seq = seq[:max_length]
        padded_sequences.append(padded_seq)

    return padded_sequences

# ğŸš€ å¼€å§‹æ•°æ®å‡†å¤‡è¿‡ç¨‹ï¼
print("ğŸ¬ å¼€å§‹æ•°æ®å‡†å¤‡è¿‡ç¨‹...")
print("="*60)

# åˆ›å»ºæ•°æ®é›†
dataset = EnhancedTranslationDatast()

# è·å–è®­ç»ƒæ•°æ®
en_sequences, zh_sequences = dataset.get_training_data()

# è®¡ç®—åºåˆ—é•¿åº¦ç»Ÿè®¡
max_en_length = max(len(seq) for seq in en_sequences)
max_zh_length = max(len(seq) for seq in zh_sequences)

print(f"\\nğŸ“Š åºåˆ—é•¿åº¦ç»Ÿè®¡:")
print(f"   æœ€é•¿è‹±æ–‡å¥å­: {max_en_length} ä¸ªè¯")
print(f"   æœ€é•¿ä¸­æ–‡å¥å­: {max_zh_length} ä¸ªè¯")

# å±•ç¤ºé•¿åº¦åˆ†å¸ƒ
en_lengths = [len(seq) for seq in en_sequences]
zh_lengths = [len(seq) for seq in zh_sequences]
print(f"   è‹±æ–‡é•¿åº¦åˆ†å¸ƒ: æœ€çŸ­{min(en_lengths)}, æœ€é•¿{max(en_lengths)}, å¹³å‡{sum(en_lengths)/len(en_lengths):.1f}")
print(f"   ä¸­æ–‡é•¿åº¦åˆ†å¸ƒ: æœ€çŸ­{min(zh_lengths)}, æœ€é•¿{max(zh_lengths)}, å¹³å‡{sum(zh_lengths)/len(zh_lengths):.1f}")

# å¡«å……åºåˆ—
en_padded = pad_sequences(en_sequences, max_en_length, dataset.en_vocab['<PAD>'])
zh_padded = pad_sequences(zh_sequences, max_zh_length, dataset.zh_vocab['<PAD>'])

# è½¬æ¢ä¸ºPyTorchå¼ é‡
en_tensor = torch.tensor(en_padded, dtype=torch.long)
zh_tensor = torch.tensor(zh_padded, dtype=torch.long)

print(f"\\nğŸ¯ æœ€ç»ˆæ•°æ®æ ¼å¼:")
print(f"   è‹±æ–‡å¼ é‡å½¢çŠ¶: {en_tensor.shape} (å¥å­æ•° Ã— æœ€å¤§é•¿åº¦)")
print(f"   ä¸­æ–‡å¼ é‡å½¢çŠ¶: {zh_tensor.shape} (å¥å­æ•° Ã— æœ€å¤§é•¿åº¦)")

print("\\n" + "="*60)
print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼æˆ‘ä»¬çš„ç¿»è¯‘æœºå™¨äººç°åœ¨æœ‰äº†å­¦ä¹ ææ–™ï¼")
print("ğŸ’¡ æ¥ä¸‹æ¥æˆ‘ä»¬å°†æ„å»ºæ¨¡å‹çš„ä¸‰ä¸ªæ ¸å¿ƒéƒ¨ä»¶...")
print("="*60)
