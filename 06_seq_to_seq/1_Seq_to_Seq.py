# å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import random
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“ä»¥ä¾¿matplotlibæ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

print("âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼")
print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"ğŸ¯ è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}")

# åŠ¨æ‰‹å®ç°ï¼šæ„å»ºä¸€ä¸ªSeq2Seqæ¨¡å‹
# 4.1 å‡†å¤‡æ•°æ®ï¼šä½¿ç”¨è‹±ä¸­ç¿»è¯‘æ•°æ®é›†
# ä»cmn.txtæ–‡ä»¶è¯»å–è‹±ä¸­ç¿»è¯‘æ•°æ®é›†
def load_cmn_data(file_path, max_samples=1000):
    """
    ä»cmn.txtæ–‡ä»¶åŠ è½½è‹±ä¸­ç¿»è¯‘æ•°æ®
    Args:
        file_path: cmn.txtæ–‡ä»¶è·¯å¾„
        max_samples: æœ€å¤§æ ·æœ¬æ•°é‡ï¼ˆä¸ºäº†æ¼”ç¤ºï¼Œé™åˆ¶æ•°æ®é‡ï¼‰
    Returns:
        list: è‹±ä¸­å¥å­å¯¹åˆ—è¡¨
    """
    data_pairs = []

    print(f"ğŸ“ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        print(f"ğŸ“Š æ–‡ä»¶æ€»è¡Œæ•°: {len(lines):,}")

        for i, line in enumerate(lines[:max_samples]):
            if i % 10000 == 0:
                print(f"   å¤„ç†è¿›åº¦: {i:,}/{min(max_samples, len(lines)):,}")

            # è§£ææ¯ä¸€è¡Œï¼šè‹±æ–‡\tä¸­æ–‡\tç‰ˆæƒä¿¡æ¯
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                en_sentence = parts[0].strip().lower()              # è‹±æ–‡å¥å­ï¼Œè½¬å°å†™
                zh_sentence = parts[1].strip()                      # ä¸­æ–‡å¥å­

                # ç®€å•è¿‡æ»¤ï¼šåªä¿ç•™é•¿åº¦é€‚ä¸­çš„å¥å­
                if len(en_sentence.split()) <= 10 and len(zh_sentence.split()) <= 20:
                    data_pairs.append((en_sentence, zh_sentence))

    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(data_pairs):,} ä¸ªå¥å­å¯¹")
    return data_pairs

# åŠ è½½cmn.txtæ•°æ®
cmn_file_path = "cmn.txt"
raw_data = load_cmn_data(cmn_file_path, max_samples=2000)

print(f"\nğŸ“ æ•°æ®æ ·ä¾‹:")
if raw_data:
    for i, (en, zh) in enumerate(raw_data[:10]):
        print(f"{i + 1:2d}. è‹±æ–‡: '{en}' â†’ ä¸­æ–‡: '{zh}'")

    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    en_lengths = [len(sent.split()) for sent, _ in raw_data]
    zh_lengths = [len(sent) for _, sent in raw_data]

    print(f"   è‹±æ–‡å¥å­é•¿åº¦: æœ€çŸ­ {min(en_lengths)}, æœ€é•¿ {max(en_lengths)}, å¹³å‡ {sum(en_lengths)/len(en_lengths):.1f}")
    print(f"   ä¸­æ–‡å¥å­é•¿åº¦: æœ€çŸ­ {min(zh_lengths)}, æœ€é•¿ {max(zh_lengths)}, å¹³å‡ {sum(zh_lengths)/len(zh_lengths):.1f}")
else:
    print("âŒ æœªèƒ½åŠ è½½æ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®é›†")
    # å¤‡ç”¨æ•°æ®é›†ï¼šåŸºç¡€è‹±ä¸­ç¿»è¯‘å¥å­
    raw_data = [
        ("hello", "ä½ å¥½"),
        ("goodbye", "å†è§"),
        ("thank you", "è°¢è°¢"),
        ("how are you", "ä½ å¥½å—"),
        ("good morning", "æ—©ä¸Šå¥½"),
        ("good night", "æ™šå®‰"),
        ("i love you", "æˆ‘çˆ±ä½ "),
        ("what is your name", "ä½ å«ä»€ä¹ˆåå­—"),
        ("nice to meet you", "å¾ˆé«˜å…´è§åˆ°ä½ "),
        ("see you later", "å†è§"),
        ("excuse me", "ä¸å¥½æ„æ€"),
        ("i am sorry", "å¯¹ä¸èµ·"),
        ("yes", "æ˜¯çš„"),
        ("no", "ä¸æ˜¯"),
        ("please", "è¯·"),
        ("where are you from", "ä½ æ¥è‡ªå“ªé‡Œ"),
        ("i am from china", "æˆ‘æ¥è‡ªä¸­å›½"),
        ("do you speak english", "ä½ ä¼šè¯´è‹±è¯­å—"),
        ("i don't understand", "æˆ‘ä¸æ˜ç™½"),
        ("can you help me", "ä½ èƒ½å¸®åŠ©æˆ‘å—"),
        ("i am hungry", "æˆ‘é¥¿äº†"),
        ("i am thirsty", "æˆ‘æ¸´äº†"),
        ("how much is it", "å¤šå°‘é’±"),
        ("where is the bathroom", "æ´—æ‰‹é—´åœ¨å“ªé‡Œ"),
    ]
    print(f"ğŸ“Š å¤‡ç”¨æ•°æ®é›†å¤§å°: {len(raw_data)} ä¸ªå¥å­å¯¹")

# æ„å»ºè¯æ±‡è¡¨ç±» - è¿™æ˜¯NLPä»»åŠ¡çš„åŸºç¡€å·¥å…·
class Vocabulary:
    def __init__(self):
        self.PAD_TOKEN = 'PAD'                  # å¡«å……ç¬¦å·
        self.SOS_TOKEN = 'SOS'                  # å¥å­å¼€å§‹ç¬¦å·
        self.EOS_TOKEN = 'EOS'                  # å¥å­ç»“æŸç¬¦å·
        self.UNK_TOKEN = 'UNK'                  # æœªçŸ¥è¯ç¬¦å·

        # è¯æ±‡è¡¨å­—å…¸ï¼š word -> index
        self.word2idx = {
            self.PAD_TOKEN: 0,
            self.SOS_TOKEN: 1,
            self.EOS_TOKEN: 2,
            self.UNK_TOKEN: 3
        }

        # åå‘å­—å…¸ï¼š index -> word
        self.idx2word = {idx: word for word, idx in self.word2idx.items()}

    def add_word(self, word):
        """å‘è¯æ±‡è¡¨æ·»åŠ æ–°è¯"""
        if word not in self.word2idx:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word

    def add_sentence(self, sentence):
        """å‘è¯æ±‡è¡¨æ·»åŠ æ•´ä¸ªå¥å­çš„è¯æ±‡"""
        for word in sentence.split():
            self.add_word(word)

    def __len__(self):
        return len(self.word2idx)

    def encode_sentence(self, sentence, add_eos=True):
        """å°†å¥å­è½¬æ¢ä¸ºç´¢å¼•åºåˆ—"""
        indices = []
        for word in sentence.split():
            if word in self.word2idx:
                indices.append(self.word2idx[word])
            else:
                indices.append(self.word2idx[self.UNK_TOKEN])

        if add_eos:
            indices.append(self.word2idx[self.EOS_TOKEN])

        return indices

    def decode_sentence(self, indices):
        """å°†ç´¢å¼•åºåˆ—è½¬æ¢å›å¥å­"""
        words = []
        for idx in indices:
            if idx == self.word2idx[self.EOS_TOKEN]:
                break
            if idx == self.word2idx[self.PAD_TOKEN]:
                continue
            words.append(self.idx2word[idx])
        return ' '.join(words)

# åˆ›å»ºè‹±æ–‡å’Œä¸­æ–‡è¯æ±‡è¡¨
en_vocab = Vocabulary()
zh_vocab = Vocabulary()

# æ„å»ºè¯æ±‡è¡¨
for en_sentence, zh_sentence in raw_data:
    en_vocab.add_sentence(en_sentence)
    # ä¸­æ–‡æŒ‰å­—ç¬¦åˆ†å‰²ï¼ˆæ¯ä¸ªæ±‰å­—ä½œä¸ºä¸€ä¸ªè¯ï¼‰
    zh_words = ' '.join(list(zh_sentence))
    zh_vocab.add_sentence(zh_words)

print(f"ğŸ“š è‹±æ–‡è¯æ±‡è¡¨å¤§å°: {len(en_vocab)}")
print(f"ğŸ“š ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {len(zh_vocab)}")

# å±•ç¤ºä¸€äº›è¯æ±‡
print(f"\nğŸ”¤ è‹±æ–‡è¯æ±‡ç¤ºä¾‹: {list(en_vocab.word2idx.keys())[:15]}")
print(f"ğŸ”¤ ä¸­æ–‡è¯æ±‡ç¤ºä¾‹: {list(zh_vocab.word2idx.keys())[:15]}")

# ç‰¹æ®Šå¤„ç†ï¼šä¸ºä¸­æ–‡è¯æ±‡è¡¨æ·»åŠ å­—ç¬¦çº§åˆ«çš„ç¼–ç è§£ç æ–¹æ³•
class ChineseVocabulary(Vocabulary):
    def add_sentence(self, sentence):
        """ä¸­æ–‡å¥å­æŒ‰å­—ç¬¦æ·»åŠ åˆ°è¯æ±‡è¡¨"""
        for char in sentence:
            if char.strip():                # å¿½ç•¥ç©ºæ ¼
                self.add_word(char)

    def encode_sentence(self, sentence, add_eos=True):
        """å°†ä¸­æ–‡å¥å­è½¬æ¢ä¸ºå­—ç¬¦ç´¢å¼•åºåˆ—"""
        indices = []
        for char in sentence:
            if char.strip():                # å¿½ç•¥ç©ºæ ¼
                if char in self.word2idx:
                    indices.append(self.word2idx[char])
                else:
                    indices.append(self.word2idx[self.UNK_TOKEN])

        if add_eos:
            indices.append(self.word2idx[self.EOS_TOKEN])

        return indices

    def decode_sentence(self, indices):
        """å°†å­—ç¬¦ç´¢å¼•åºåˆ—è½¬æ¢å›ä¸­æ–‡å¥å­"""
        chars = []
        for idx in indices:
            if idx == self.word2idx[self.EOS_TOKEN]:
                break
            if idx == self.word2idx[self.PAD_TOKEN]:
                continue
            chars.append(self.idx2word[idx])

        return ''.join(chars)

# é‡æ–°åˆ›å»ºä¸­æ–‡è¯æ±‡è¡¨
zh_vocab = ChineseVocabulary()
for en_sentence, zh_sentence in raw_data:
    zh_vocab.add_sentence(zh_sentence)

print(f"\nğŸ“š æ›´æ–°åçš„ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {len(zh_vocab)}")
print(f"ğŸ”¤ ä¸­æ–‡å­—ç¬¦ç¤ºä¾‹: {list(zh_vocab.word2idx.keys())[:20]}")


# tokenize (åˆ†è¯)
# åˆ›å»ºæ•°æ®é›†ç±»
class TranslationDataset(Dataset):
    def __init__(self, data_pairs, src_vocab, tgt_vocab, max_length=20):
        """
        ç¿»è¯‘æ•°æ®é›†
        Args:
            data_pairs: å¥å­å¯¹åˆ—è¡¨ [(src_sentence, tgt_sentence), ...]
            src_vocab: æºè¯­è¨€è¯æ±‡è¡¨
            tgt_vocab: ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.data_pairs = data_pairs
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_length = max_length

    def __len__(self):
        return len(self.data_pairs)

    def __getitem__(self, idx):
        src_sentence, tgt_sentence = self.data_pairs[idx]

        # ç¼–ç æºå¥å­
        src_indices = self.src_vocab.encode_sentence(src_sentence, add_eos=True)

        # ç¼–ç ç›®æ ‡å¥å­ï¼ˆç”¨äºè®­ç»ƒçš„è¾“å…¥ï¼Œéœ€è¦æ·»åŠ SOSï¼‰
        tgt_input_indices = [self.tgt_vocab.word2idx[self.tgt_vocab.SOS_TOKEN]] + \
            self.tgt_vocab.encode_sentence(tgt_sentence, add_eos=False)

        # ç¼–ç ç›®æ ‡å¥å­ï¼ˆç”¨äºè®¡ç®—æŸå¤±çš„æ ‡ç­¾ï¼Œéœ€è¦æ·»åŠ EOSï¼‰
        tgt_output_indices = self.tgt_vocab.encode_sentence(tgt_sentence, add_eos=True)

        return {
            'src': src_indices,
            'tgt_input': tgt_input_indices,
            'tgt_output': tgt_output_indices,
            'src_text': src_sentence,
            'tgt_text': tgt_sentence
        }

def collate_fn(batch):
    """è‡ªå®šä¹‰çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œç”¨äºå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—"""

    # è·å–æ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬çš„æ•°æ®
    src_sequences = [item['src'] for item in batch]
    tgt_input_sequences = [item['tgt_input'] for item in batch]
    tgt_output_sequences = [item['tgt_output'] for item in batch]

    # å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦
    src_padded = pad_sequences(src_sequences, en_vocab.word2idx[en_vocab.PAD_TOKEN])
    tgt_input_padded = pad_sequences(tgt_input_sequences, zh_vocab.word2idx[zh_vocab.PAD_TOKEN])
    tgt_output_padded = pad_sequences(tgt_output_sequences, zh_vocab.word2idx[zh_vocab.PAD_TOKEN])

    return {
        'src': torch.tensor(src_padded, dtype=torch.long),
        'tgt_input': torch.tensor(tgt_input_padded, dtype=torch.long),
        'tgt_output': torch.tensor(tgt_output_padded, dtype=torch.long),
        'src_text': [item['src_text'] for item in batch],
        'tgt_text': [item['tgt_text'] for item in batch]
    }

def pad_sequences(sequences, pad_token):
    """å°†åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦"""
    max_length = max(len(seq) for seq in sequences)
    padded_sequences = []

    for seq in sequences:
        padded_seq = seq + [pad_token] * (max_length - len(seq))
        padded_sequences.append(padded_seq)

    return padded_sequences

# åˆ›å»ºæ•°æ®é›†
dataset = TranslationDataset(raw_data, en_vocab, zh_vocab)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
batch_size = 4
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

print(f"ğŸ“¦ æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"ğŸ”„ æ‰¹æ¬¡å¤§å°: {batch_size}")

# æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
sample_batch = next(iter(dataloader))
print(f"\nğŸ” æ ·æœ¬æ‰¹æ¬¡å½¢çŠ¶:")
print(f"   æºåºåˆ—: {sample_batch['src'].shape}")
print(f"   æºåºåˆ—: {sample_batch['src']}")
print(f"   ç›®æ ‡è¾“å…¥: {sample_batch['tgt_input'].shape}")
print(f"   ç›®æ ‡è¾“å…¥: {sample_batch['tgt_input']}")
print(f"   ç›®æ ‡è¾“å‡º: {sample_batch['tgt_output'].shape}")
print(f"   ç›®æ ‡è¾“å‡º: {sample_batch['tgt_output']}")
print(f"     è‹±æ–‡: '{sample_batch['src_text']}'")
print(f"     ä¸­æ–‡: '{sample_batch['tgt_text']}'")

# æ˜¾ç¤ºä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
print(f"\nğŸ“ æ ·æœ¬è¯¦æƒ…:")
for i in range(min(2, len(sample_batch['src_text']))):
    print(f"   æ ·æœ¬ {i+1}:")
    print(f"     è‹±æ–‡: '{sample_batch['src_text'][i]}'")
    print(f"     ä¸­æ–‡: '{sample_batch['tgt_text'][i]}'")
    print(f"     è‹±æ–‡ç¼–ç : {sample_batch['src'][i].tolist()}")
    print(f"     ä¸­æ–‡è¾“å…¥ç¼–ç : {sample_batch['tgt_input'][i].tolist()}")
    print(f"     ä¸­æ–‡è¾“å‡ºç¼–ç : {sample_batch['tgt_output'][i].tolist()}")


# 4.2 ç¼–ç å™¨å®ç°ï¼šç†è§£è¾“å…¥åºåˆ—
# è¯æ±‡è¡¨æ„å»ºè¿‡ç¨‹å¯è§†åŒ–å’Œæ•°æ®æµæ¼”ç¤º

