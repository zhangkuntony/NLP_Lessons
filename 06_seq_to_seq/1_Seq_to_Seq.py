# è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³OpenMPå†²çª
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import random
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“ä»¥ä¾¿matplotlibæ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
plt.rcParams['font.family'] = ['sans-serif']  # è®¾ç½®å­—ä½“æ—

# è§£å†³ç‰¹å®šUnicodeå­—ç¬¦æ˜¾ç¤ºé—®é¢˜
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")

# é¢å¤–çš„å­—ä½“è®¾ç½®ï¼Œè§£å†³Unicodeå­—ç¬¦é—®é¢˜
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.autolayout'] = True

# å¿½ç•¥æ‰€æœ‰matplotlibå­—ä½“ç›¸å…³è­¦å‘Š
warnings.filterwarnings("ignore", message=".*glyph.*")
warnings.filterwarnings("ignore", message=".*Font.*")
warnings.filterwarnings("ignore", message=".*fallback.*")

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

print("âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼")
print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"ğŸ¯ è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}")

# åŠ¨æ‰‹å®ç°ï¼šæ„å»ºä¸€ä¸ªSeq2Seqæ¨¡å‹
# 4.1 å‡†å¤‡æ•°æ®ï¼šä½¿ç”¨è‹±ä¸­ç¿»è¯‘æ•°æ®é›†
# ä»cmn.txtæ–‡ä»¶è¯»å–è‹±ä¸­ç¿»è¯‘æ•°æ®é›†
def load_cmn_data(file_path, max_samples=1000):
    """
    ä»cmn.txtæ–‡ä»¶åŠ è½½è‹±ä¸­ç¿»è¯‘æ•°æ®
    Args:
        file_path: cmn.txtæ–‡ä»¶è·¯å¾„
        max_samples: æœ€å¤§æ ·æœ¬æ•°é‡ï¼ˆä¸ºäº†æ¼”ç¤ºï¼Œé™åˆ¶æ•°æ®é‡ï¼‰
    Returns:
        list: è‹±ä¸­å¥å­å¯¹åˆ—è¡¨
    """
    data_pairs = []

    print(f"ğŸ“ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        print(f"ğŸ“Š æ–‡ä»¶æ€»è¡Œæ•°: {len(lines):,}")

        for i, line in enumerate(lines[:max_samples]):
            if i % 10000 == 0:
                print(f"   å¤„ç†è¿›åº¦: {i:,}/{min(max_samples, len(lines)):,}")

            # è§£ææ¯ä¸€è¡Œï¼šè‹±æ–‡\tä¸­æ–‡\tç‰ˆæƒä¿¡æ¯
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                en_sentence = parts[0].strip().lower()              # è‹±æ–‡å¥å­ï¼Œè½¬å°å†™
                zh_sentence = parts[1].strip()                      # ä¸­æ–‡å¥å­

                # ç®€å•è¿‡æ»¤ï¼šåªä¿ç•™é•¿åº¦é€‚ä¸­çš„å¥å­
                if len(en_sentence.split()) <= 10 and len(zh_sentence.split()) <= 20:
                    data_pairs.append((en_sentence, zh_sentence))

    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(data_pairs):,} ä¸ªå¥å­å¯¹")
    return data_pairs

# åŠ è½½cmn.txtæ•°æ®
cmn_file_path = "cmn.txt"
raw_data = load_cmn_data(cmn_file_path, max_samples=2000)

print(f"\nğŸ“ æ•°æ®æ ·ä¾‹:")
if raw_data:
    for i, (en, zh) in enumerate(raw_data[:10]):
        print(f"{i + 1:2d}. è‹±æ–‡: '{en}' â†’ ä¸­æ–‡: '{zh}'")

    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    en_lengths = [len(sent.split()) for sent, _ in raw_data]
    zh_lengths = [len(sent) for _, sent in raw_data]

    print(f"   è‹±æ–‡å¥å­é•¿åº¦: æœ€çŸ­ {min(en_lengths)}, æœ€é•¿ {max(en_lengths)}, å¹³å‡ {sum(en_lengths)/len(en_lengths):.1f}")
    print(f"   ä¸­æ–‡å¥å­é•¿åº¦: æœ€çŸ­ {min(zh_lengths)}, æœ€é•¿ {max(zh_lengths)}, å¹³å‡ {sum(zh_lengths)/len(zh_lengths):.1f}")
else:
    print("âŒ æœªèƒ½åŠ è½½æ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®é›†")
    # å¤‡ç”¨æ•°æ®é›†ï¼šåŸºç¡€è‹±ä¸­ç¿»è¯‘å¥å­
    raw_data = [
        ("hello", "ä½ å¥½"),
        ("goodbye", "å†è§"),
        ("thank you", "è°¢è°¢"),
        ("how are you", "ä½ å¥½å—"),
        ("good morning", "æ—©ä¸Šå¥½"),
        ("good night", "æ™šå®‰"),
        ("i love you", "æˆ‘çˆ±ä½ "),
        ("what is your name", "ä½ å«ä»€ä¹ˆåå­—"),
        ("nice to meet you", "å¾ˆé«˜å…´è§åˆ°ä½ "),
        ("see you later", "å†è§"),
        ("excuse me", "ä¸å¥½æ„æ€"),
        ("i am sorry", "å¯¹ä¸èµ·"),
        ("yes", "æ˜¯çš„"),
        ("no", "ä¸æ˜¯"),
        ("please", "è¯·"),
        ("where are you from", "ä½ æ¥è‡ªå“ªé‡Œ"),
        ("i am from china", "æˆ‘æ¥è‡ªä¸­å›½"),
        ("do you speak english", "ä½ ä¼šè¯´è‹±è¯­å—"),
        ("i don't understand", "æˆ‘ä¸æ˜ç™½"),
        ("can you help me", "ä½ èƒ½å¸®åŠ©æˆ‘å—"),
        ("i am hungry", "æˆ‘é¥¿äº†"),
        ("i am thirsty", "æˆ‘æ¸´äº†"),
        ("how much is it", "å¤šå°‘é’±"),
        ("where is the bathroom", "æ´—æ‰‹é—´åœ¨å“ªé‡Œ"),
    ]
    print(f"ğŸ“Š å¤‡ç”¨æ•°æ®é›†å¤§å°: {len(raw_data)} ä¸ªå¥å­å¯¹")

# æ„å»ºè¯æ±‡è¡¨ç±» - è¿™æ˜¯NLPä»»åŠ¡çš„åŸºç¡€å·¥å…·
class Vocabulary:
    def __init__(self):
        self.PAD_TOKEN = 'PAD'                  # å¡«å……ç¬¦å·
        self.SOS_TOKEN = 'SOS'                  # å¥å­å¼€å§‹ç¬¦å·
        self.EOS_TOKEN = 'EOS'                  # å¥å­ç»“æŸç¬¦å·
        self.UNK_TOKEN = 'UNK'                  # æœªçŸ¥è¯ç¬¦å·

        # è¯æ±‡è¡¨å­—å…¸ï¼š word -> index
        self.word2idx = {
            self.PAD_TOKEN: 0,
            self.SOS_TOKEN: 1,
            self.EOS_TOKEN: 2,
            self.UNK_TOKEN: 3
        }

        # åå‘å­—å…¸ï¼š index -> word
        self.idx2word = {idx: word for word, idx in self.word2idx.items()}

    def add_word(self, word):
        """å‘è¯æ±‡è¡¨æ·»åŠ æ–°è¯"""
        if word not in self.word2idx:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word

    def add_sentence(self, sentence):
        """å‘è¯æ±‡è¡¨æ·»åŠ æ•´ä¸ªå¥å­çš„è¯æ±‡"""
        for word in sentence.split():
            self.add_word(word)

    def __len__(self):
        return len(self.word2idx)

    def encode_sentence(self, sentence, add_eos=True):
        """å°†å¥å­è½¬æ¢ä¸ºç´¢å¼•åºåˆ—"""
        indices = []
        for word in sentence.split():
            if word in self.word2idx:
                indices.append(self.word2idx[word])
            else:
                indices.append(self.word2idx[self.UNK_TOKEN])

        if add_eos:
            indices.append(self.word2idx[self.EOS_TOKEN])

        return indices

    def decode_sentence(self, indices):
        """å°†ç´¢å¼•åºåˆ—è½¬æ¢å›å¥å­"""
        words = []
        for idx in indices:
            if idx == self.word2idx[self.EOS_TOKEN]:
                break
            if idx == self.word2idx[self.PAD_TOKEN]:
                continue
            words.append(self.idx2word[idx])
        return ' '.join(words)

# åˆ›å»ºè‹±æ–‡å’Œä¸­æ–‡è¯æ±‡è¡¨
en_vocab = Vocabulary()
zh_vocab = Vocabulary()

# æ„å»ºè¯æ±‡è¡¨
for en_sentence, zh_sentence in raw_data:
    en_vocab.add_sentence(en_sentence)
    # ä¸­æ–‡æŒ‰å­—ç¬¦åˆ†å‰²ï¼ˆæ¯ä¸ªæ±‰å­—ä½œä¸ºä¸€ä¸ªè¯ï¼‰
    zh_words = ' '.join(list(zh_sentence))
    zh_vocab.add_sentence(zh_words)

print(f"ğŸ“š è‹±æ–‡è¯æ±‡è¡¨å¤§å°: {len(en_vocab)}")
print(f"ğŸ“š ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {len(zh_vocab)}")

# å±•ç¤ºä¸€äº›è¯æ±‡
print(f"\nğŸ”¤ è‹±æ–‡è¯æ±‡ç¤ºä¾‹: {list(en_vocab.word2idx.keys())[:15]}")
print(f"ğŸ”¤ ä¸­æ–‡è¯æ±‡ç¤ºä¾‹: {list(zh_vocab.word2idx.keys())[:15]}")

# ç‰¹æ®Šå¤„ç†ï¼šä¸ºä¸­æ–‡è¯æ±‡è¡¨æ·»åŠ å­—ç¬¦çº§åˆ«çš„ç¼–ç è§£ç æ–¹æ³•
class ChineseVocabulary(Vocabulary):
    def add_sentence(self, sentence):
        """ä¸­æ–‡å¥å­æŒ‰å­—ç¬¦æ·»åŠ åˆ°è¯æ±‡è¡¨"""
        for char in sentence:
            if char.strip():                # å¿½ç•¥ç©ºæ ¼
                self.add_word(char)

    def encode_sentence(self, sentence, add_eos=True):
        """å°†ä¸­æ–‡å¥å­è½¬æ¢ä¸ºå­—ç¬¦ç´¢å¼•åºåˆ—"""
        indices = []
        for char in sentence:
            if char.strip():                # å¿½ç•¥ç©ºæ ¼
                if char in self.word2idx:
                    indices.append(self.word2idx[char])
                else:
                    indices.append(self.word2idx[self.UNK_TOKEN])

        if add_eos:
            indices.append(self.word2idx[self.EOS_TOKEN])

        return indices

    def decode_sentence(self, indices):
        """å°†å­—ç¬¦ç´¢å¼•åºåˆ—è½¬æ¢å›ä¸­æ–‡å¥å­"""
        chars = []
        for idx in indices:
            if idx == self.word2idx[self.EOS_TOKEN]:
                break
            if idx == self.word2idx[self.PAD_TOKEN]:
                continue
            chars.append(self.idx2word[idx])

        return ''.join(chars)

# é‡æ–°åˆ›å»ºä¸­æ–‡è¯æ±‡è¡¨
zh_vocab = ChineseVocabulary()
for en_sentence, zh_sentence in raw_data:
    zh_vocab.add_sentence(zh_sentence)

print(f"\nğŸ“š æ›´æ–°åçš„ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {len(zh_vocab)}")
print(f"ğŸ”¤ ä¸­æ–‡å­—ç¬¦ç¤ºä¾‹: {list(zh_vocab.word2idx.keys())[:20]}")


# tokenize (åˆ†è¯)
# åˆ›å»ºæ•°æ®é›†ç±»
class TranslationDataset(Dataset):
    def __init__(self, data_pairs, src_vocab, tgt_vocab, max_length=20):
        """
        ç¿»è¯‘æ•°æ®é›†
        Args:
            data_pairs: å¥å­å¯¹åˆ—è¡¨ [(src_sentence, tgt_sentence), ...]
            src_vocab: æºè¯­è¨€è¯æ±‡è¡¨
            tgt_vocab: ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.data_pairs = data_pairs
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_length = max_length

    def __len__(self):
        return len(self.data_pairs)

    def __getitem__(self, idx):
        src_sentence, tgt_sentence = self.data_pairs[idx]

        # ç¼–ç æºå¥å­
        src_indices = self.src_vocab.encode_sentence(src_sentence, add_eos=True)

        # ç¼–ç ç›®æ ‡å¥å­ï¼ˆç”¨äºè®­ç»ƒçš„è¾“å…¥ï¼Œéœ€è¦æ·»åŠ SOSï¼‰
        tgt_input_indices = [self.tgt_vocab.word2idx[self.tgt_vocab.SOS_TOKEN]] + \
            self.tgt_vocab.encode_sentence(tgt_sentence, add_eos=False)

        # ç¼–ç ç›®æ ‡å¥å­ï¼ˆç”¨äºè®¡ç®—æŸå¤±çš„æ ‡ç­¾ï¼Œéœ€è¦æ·»åŠ EOSï¼‰
        tgt_output_indices = self.tgt_vocab.encode_sentence(tgt_sentence, add_eos=True)

        return {
            'src': src_indices,
            'tgt_input': tgt_input_indices,
            'tgt_output': tgt_output_indices,
            'src_text': src_sentence,
            'tgt_text': tgt_sentence
        }

def collate_fn(batch):
    """è‡ªå®šä¹‰çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œç”¨äºå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—"""

    # è·å–æ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬çš„æ•°æ®
    src_sequences = [item['src'] for item in batch]
    tgt_input_sequences = [item['tgt_input'] for item in batch]
    tgt_output_sequences = [item['tgt_output'] for item in batch]

    # å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦
    src_padded = pad_sequences(src_sequences, en_vocab.word2idx[en_vocab.PAD_TOKEN])
    tgt_input_padded = pad_sequences(tgt_input_sequences, zh_vocab.word2idx[zh_vocab.PAD_TOKEN])
    tgt_output_padded = pad_sequences(tgt_output_sequences, zh_vocab.word2idx[zh_vocab.PAD_TOKEN])

    return {
        'src': torch.tensor(src_padded, dtype=torch.long),
        'tgt_input': torch.tensor(tgt_input_padded, dtype=torch.long),
        'tgt_output': torch.tensor(tgt_output_padded, dtype=torch.long),
        'src_text': [item['src_text'] for item in batch],
        'tgt_text': [item['tgt_text'] for item in batch]
    }

def pad_sequences(sequences, pad_token):
    """å°†åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦"""
    max_length = max(len(seq) for seq in sequences)
    padded_sequences = []

    for seq in sequences:
        padded_seq = seq + [pad_token] * (max_length - len(seq))
        padded_sequences.append(padded_seq)

    return padded_sequences

# åˆ›å»ºæ•°æ®é›†
dataset = TranslationDataset(raw_data, en_vocab, zh_vocab)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
batch_size = 4
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

print(f"ğŸ“¦ æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"ğŸ”„ æ‰¹æ¬¡å¤§å°: {batch_size}")

# æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
sample_batch = next(iter(dataloader))
print(f"\nğŸ” æ ·æœ¬æ‰¹æ¬¡å½¢çŠ¶:")
print(f"   æºåºåˆ—: {sample_batch['src'].shape}")
print(f"   æºåºåˆ—: {sample_batch['src']}")
print(f"   ç›®æ ‡è¾“å…¥: {sample_batch['tgt_input'].shape}")
print(f"   ç›®æ ‡è¾“å…¥: {sample_batch['tgt_input']}")
print(f"   ç›®æ ‡è¾“å‡º: {sample_batch['tgt_output'].shape}")
print(f"   ç›®æ ‡è¾“å‡º: {sample_batch['tgt_output']}")
print(f"     è‹±æ–‡: '{sample_batch['src_text']}'")
print(f"     ä¸­æ–‡: '{sample_batch['tgt_text']}'")

# æ˜¾ç¤ºä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
print(f"\nğŸ“ æ ·æœ¬è¯¦æƒ…:")
for i in range(min(2, len(sample_batch['src_text']))):
    print(f"   æ ·æœ¬ {i+1}:")
    print(f"     è‹±æ–‡: '{sample_batch['src_text'][i]}'")
    print(f"     ä¸­æ–‡: '{sample_batch['tgt_text'][i]}'")
    print(f"     è‹±æ–‡ç¼–ç : {sample_batch['src'][i].tolist()}")
    print(f"     ä¸­æ–‡è¾“å…¥ç¼–ç : {sample_batch['tgt_input'][i].tolist()}")
    print(f"     ä¸­æ–‡è¾“å‡ºç¼–ç : {sample_batch['tgt_output'][i].tolist()}")


# 4.2 ç¼–ç å™¨å®ç°ï¼šç†è§£è¾“å…¥åºåˆ—
# è¯æ±‡è¡¨æ„å»ºè¿‡ç¨‹å¯è§†åŒ–å’Œæ•°æ®æµæ¼”ç¤º

print("ğŸ” è¯æ±‡è¡¨æ„å»ºè¿‡ç¨‹è¯¦è§£")
print("=" * 60)

# æ¼”ç¤ºè¯æ±‡è¡¨æ„å»ºè¿‡ç¨‹
print("\nğŸ“š è¯æ±‡è¡¨æ„å»ºæ­¥éª¤æ¼”ç¤º:")
sample_sentences = ["hello world", "good morning", "thank you very much"]

demo_vocab = Vocabulary()
print(f"1. åˆå§‹è¯æ±‡è¡¨ï¼š{list(demo_vocab.idx2word.keys())}")

for i, sentence in enumerate(sample_sentences):
    print(f"\n2.{i+1} æ·»åŠ å¥å­ï¼š'{sentence}'")
    demo_vocab.add_sentence(sentence)
    print(f"     å½“å‰è¯æ±‡è¡¨: {list(demo_vocab.word2idx.keys())}")
    print(f"     è¯æ±‡è¡¨å¤§å°: {len(demo_vocab)}")

print(f"\nğŸ“Š æœ€ç»ˆè¯æ±‡è¡¨ç»Ÿè®¡:")
print(f"   æ€»è¯æ±‡æ•°: {len(demo_vocab)}")
print(f"   ç‰¹æ®Šç¬¦å·æ•°: 4 (PAD, SOS, EOS, UNK)")
print(f"   å®é™…å•è¯æ•°: {len(demo_vocab) - 4}")

# æ¼”ç¤ºç¼–ç å’Œè§£ç è¿‡ç¨‹
print(f"\nğŸ”„ ç¼–ç è§£ç è¿‡ç¨‹æ¼”ç¤º:")
test_sentence = "hello world"
print(f"åŸå§‹å¥å­: '{test_sentence}'")

# ç¼–ç è¿‡ç¨‹
encoded = demo_vocab.encode_sentence(test_sentence)
print(f"ç¼–ç ç»“æœï¼š{encoded}")
print(f"å¯¹åº”è¯æ±‡ï¼š{[demo_vocab.idx2word[idx] for idx in encoded]}")

# è§£ç è¿‡ç¨‹
decoded = demo_vocab.decode_sentence(encoded)
print(f"è§£ç ç»“æœï¼š{decoded}")

# å±•ç¤ºå®é™…æ•°æ®é›†çš„è¯æ±‡åˆ†å¸ƒ
print(f"\nğŸ“ˆ æ•°æ®é›†è¯æ±‡åˆ†å¸ƒåˆ†æ:")
en_words = []
zh_chars = []

for en_sent, zh_sent in raw_data:
    en_words.extend(en_sent.split())
    zh_chars.extend(list(zh_sent))

en_word_freq = Counter(en_words)
zh_char_freq = Counter(zh_chars)

print(f"\nğŸ‡¬ğŸ‡§ è‹±æ–‡è¯æ±‡ç»Ÿè®¡:")
print(f"   æ€»è¯æ±‡æ•°: {len(en_words)} (åŒ…å«é‡å¤)")
print(f"   å”¯ä¸€è¯æ±‡æ•°: {len(en_word_freq)}")
print(f"   æœ€é«˜é¢‘è¯æ±‡: {en_word_freq.most_common(5)}")

print(f"\nğŸ‡¨ğŸ‡³ ä¸­æ–‡å­—ç¬¦ç»Ÿè®¡:")
print(f"   æ€»å­—ç¬¦æ•°: {len(zh_chars)} (åŒ…å«é‡å¤)")
print(f"   å”¯ä¸€å­—ç¬¦æ•°: {len(zh_char_freq)}")
print(f"   æœ€é«˜é¢‘å­—ç¬¦: {zh_char_freq.most_common(5)}")

# æ£€æŸ¥æ•°æ®é›†çš„åºåˆ—é•¿åº¦åˆ†å¸ƒ
en_lengths = [len(sent.split()) for sent, _ in raw_data]
zh_lengths = [len(sent) for _, sent in raw_data]

print(f"\nğŸ“ åºåˆ—é•¿åº¦åˆ†æ:")
print(f"   è‹±æ–‡å¥å­é•¿åº¦: æœ€çŸ­ {min(en_lengths)}, æœ€é•¿ {max(en_lengths)}, å¹³å‡ {sum(en_lengths)/len(en_lengths):.1f} ä¸ªå•è¯")
print(f"   ä¸­æ–‡å¥å­é•¿åº¦: æœ€çŸ­ {min(zh_lengths)}, æœ€é•¿ {max(zh_lengths)}, å¹³å‡ {sum(zh_lengths)/len(zh_lengths):.1f} ä¸ªå­—ç¬¦")

# æ‰¾å‡ºæœ€é•¿å’Œæœ€çŸ­çš„å¥å­
max_en_idx = en_lengths.index(max(en_lengths))
min_en_idx = en_lengths.index(min(en_lengths))

print(f"\nğŸ“ é•¿åº¦ç¤ºä¾‹:")
print(f"   æœ€é•¿è‹±æ–‡å¥å­: '{raw_data[max_en_idx][0]}' (é•¿åº¦: {max(en_lengths)} ä¸ªå•è¯)")
print(f"   æœ€çŸ­è‹±æ–‡å¥å­: '{raw_data[min_en_idx][0]}' (é•¿åº¦: {min(en_lengths)} ä¸ªå•è¯)")

max_zh_idx = zh_lengths.index(max(zh_lengths))
min_zh_idx = zh_lengths.index(min(zh_lengths))

print(f"   æœ€é•¿ä¸­æ–‡å¥å­: '{raw_data[max_zh_idx][1]}' (é•¿åº¦: {max(zh_lengths)} ä¸ªå­—ç¬¦)")
print(f"   æœ€çŸ­ä¸­æ–‡å¥å­: '{raw_data[min_zh_idx][1]}' (é•¿åº¦: {min(zh_lengths)} ä¸ªå­—ç¬¦)")


# ç¼–ç å™¨å®ç°
class Encoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):
        """
        ç¼–ç å™¨
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            embedding_dim: è¯åµŒå…¥ç»´åº¦
            hidden_dim: LSTMéšè—å±‚ç»´åº¦
            num_layers: LSTMå±‚æ•°
        """
        super(Encoder, self).__init__()

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # è¯åµŒå…¥å±‚ï¼šå°†è¯ç´¢å¼•è½¬æ¢ä¸ºç¨ å¯†å‘é‡
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # LSTMå±‚ï¼šå¤„ç†åºåˆ—ä¿¡æ¯
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,
                            batch_first=True, bidirectional=False)

    def forward(self, input_seq):
        """
        å‰å‘ä¼ æ’­
        Args:
            input_seq: è¾“å…¥åºåˆ— [batch_size, seq_len]
        Returns:
            outputs: æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡º [batch_size, seq_len, hidden_dim]
            (hidden, cell): æœ€ç»ˆçš„éšçŠ¶æ€å’Œç»†èƒçŠ¶æ€
        """
        # 1. è¯åµŒå…¥ï¼š[batch_size, seq_len] -> [batch_size, seq_len, embedding_dim]
        embedded = self.embedding(input_seq)

        # 2. LSTMå¤„ç†ï¼šè·å–æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡ºå’Œæœ€ç»ˆéšçŠ¶æ€
        outputs, (hidden, cell) = self.lstm(embedded)

        # è¿”å›æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€ä½œä¸ºå¥å­è¡¨ç¤º
        return outputs, (hidden, cell)

# æµ‹è¯•ç¼–ç å™¨
vocab_size = len(en_vocab)
embedding_dim = 64
hidden_dim = 128

encoder = Encoder(vocab_size, embedding_dim, hidden_dim)

print(f"ğŸ—ï¸ ç¼–ç å™¨åˆ›å»ºå®Œæˆï¼")
print(f"ğŸ“ å‚æ•°æ•°é‡: {sum(p.numel() for p in encoder.parameters()):,}")

# æµ‹è¯•ç¼–ç å™¨
test_input = sample_batch['src'][:2]            # å–å‰2ä¸ªæ ·æœ¬æµ‹è¯•
print(f"\nğŸ§ª æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}")

with torch.no_grad():
    outputs, (hidden, cell) = encoder(test_input)
    print(f"âœ… ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
    print(f"âœ… æœ€ç»ˆéšçŠ¶æ€å½¢çŠ¶: {hidden.shape}")
    print(f"âœ… æœ€ç»ˆç»†èƒçŠ¶æ€å½¢çŠ¶: {cell.shape}")


# æ•°æ®æµåŠ¨å¯è§†åŒ–ï¼šä»åŸå§‹æ•°æ®åˆ°æ¨¡å‹è¾“å…¥
print("ğŸŒŠ æ•°æ®æµåŠ¨å…¨è¿‡ç¨‹å¯è§†åŒ–")
print("=" * 70)

# é€‰æ‹©ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè¯¦ç»†æ¼”ç¤º
sample_en, sample_zh = raw_data[0]
print(f"ğŸ“ æ¼”ç¤ºæ ·æœ¬: '{sample_en}' â†’ '{sample_zh}'")
print("-" * 50)

# æ­¥éª¤1: åŸå§‹æ•°æ®
print("ğŸ æ­¥éª¤1: åŸå§‹æ•°æ®")
print(f"   è‹±æ–‡: '{sample_en}'")
print(f"   ä¸­æ–‡: '{sample_zh}'")

# æ­¥éª¤2: è¯æ±‡è¡¨ç¼–ç 
print(f"\nğŸ”¤ æ­¥éª¤2: è¯æ±‡è¡¨ç¼–ç ")
en_encoded = en_vocab.encode_sentence(sample_en, add_eos=True)
zh_encoded_input =[zh_vocab.word2idx[zh_vocab.SOS_TOKEN]] + zh_vocab.encode_sentence(sample_zh, add_eos=False)
zh_encoded_target = zh_vocab.encode_sentence(sample_zh, add_eos=True)

print(f"    è‹±æ–‡ç¼–ç ï¼š{en_encoded}")
print(f"      -> å¯¹åº”è¯æ±‡ï¼š{[en_vocab.idx2word[idx] for idx in en_encoded]}")
print(f"    ä¸­æ–‡è¾“å…¥ç¼–ç ï¼š{zh_encoded_input}")
print(f"      -> å¯¹åº”å­—ç¬¦ï¼š{[zh_vocab.idx2word[idx] for idx in zh_encoded_input]}")
print(f"    ä¸­æ–‡ç›®æ ‡ç¼–ç ï¼š{zh_encoded_target}")
print(f"      -> å¯¹åº”å­—ç¬¦ï¼š{[zh_vocab.idx2word[idx] for idx in zh_encoded_target]}")

# æ­¥éª¤3: æ‰¹å¤„ç†å’Œå¡«å……
print(f"\nğŸ“¦ æ­¥éª¤3: æ‰¹å¤„ç†å’Œå¡«å……æ¼”ç¤º")
# æ¨¡æ‹Ÿä¸€ä¸ªå°æ‰¹æ¬¡
mini_batch_indices = [0, 1, 2]
mini_batch_data = [raw_data[i] for i in mini_batch_indices]

print(f"    å°æ‰¹æ¬¡åŸå§‹æ•°æ®ï¼š")
for i, (en, zh) in enumerate(mini_batch_data):
    print(f"    æ ·æœ¬{i}: '{en}' -> â€˜{zh}'")

# ç¼–ç æ‰€æœ‰æ ·æœ¬
batch_en_encoded = []
batch_zh_input_encoded = []
batch_zh_target_encoded = []

for en, zh in mini_batch_data:
    batch_en_encoded.append(en_vocab.encode_sentence(en, add_eos=True))
    batch_zh_input_encoded.append([zh_vocab.word2idx[zh_vocab.SOS_TOKEN]] + zh_vocab.encode_sentence(zh, add_eos=False))
    batch_zh_target_encoded.append(zh_vocab.encode_sentence(zh, add_eos=True))

print(f"\n    ç¼–ç åé•¿åº¦ï¼š")
for i, (en, zh_inp, zh_tgt) in enumerate(zip(batch_en_encoded, batch_zh_input_encoded, batch_zh_target_encoded)):
    print(f"    æ ·æœ¬{i}: en={len(en)}, zh_input={len(zh_inp)}, zh_target={len(zh_tgt)}")

# å¡«å……åˆ°ç›¸åŒé•¿åº¦
batch_en_padded = pad_sequences(batch_en_encoded, en_vocab.word2idx[en_vocab.PAD_TOKEN])
batch_zh_input_padded = pad_sequences(batch_zh_input_encoded, zh_vocab.word2idx[zh_vocab.PAD_TOKEN])
batch_zh_target_padded = pad_sequences(batch_zh_target_encoded, zh_vocab.word2idx[zh_vocab.PAD_TOKEN])

print(f"\n    å¡«å……åï¼š")
for i, (en, zh_inp, zh_tgt) in enumerate(zip(batch_en_padded, batch_zh_input_padded, batch_zh_target_padded)):
    print(f"    æ ·æœ¬{i}: {en}")
    print(f"      -> {[en_vocab.idx2word[idx] for idx in en]}")
    print(f"    æ ·æœ¬{i}: {zh_inp}")
    print(f"      -> {[zh_vocab.idx2word[idx] for idx in zh_inp]}")
    print(f"    æ ·æœ¬{i}: {zh_tgt}")
    print(f"      -> {[zh_vocab.idx2word[idx] for idx in zh_tgt]}")

# æ­¥éª¤4: è½¬æ¢ä¸ºå¼ é‡
print(f"\nğŸ”¢ æ­¥éª¤4: è½¬æ¢ä¸ºPyTorchå¼ é‡")
batch_en_tensor = torch.tensor(batch_en_padded, dtype=torch.long)
batch_zh_input_tensor = torch.tensor(batch_zh_input_padded, dtype=torch.long)
batch_zh_target_tensor = torch.tensor(batch_zh_target_padded, dtype=torch.long)

print(f"   è‹±æ–‡å¼ é‡å½¢çŠ¶: {batch_en_tensor.shape}")
print(f"   ä¸­æ–‡è¾“å…¥å¼ é‡å½¢çŠ¶: {batch_zh_input_tensor.shape}")
print(f"   ä¸­æ–‡ç›®æ ‡å¼ é‡å½¢çŠ¶: {batch_zh_target_tensor.shape}")

print(f"\n  ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å¼ é‡å€¼ï¼š")
print(f"    è‹±æ–‡ï¼š{batch_en_tensor[0]}")
print(f"    ä¸­æ–‡è¾“å…¥ï¼š{batch_zh_input_tensor[0]}")
print(f"    ä¸­æ–‡ç›®æ ‡ï¼š{batch_zh_target_tensor[0]}")

# æ­¥éª¤5: æŸå¤±è®¡ç®—çš„è§£é‡Š
print(f"\nğŸ’¡ æ­¥éª¤5: è®­ç»ƒæ—¶çš„æŸå¤±è®¡ç®—")
print(f"   æ¨¡å‹é¢„æµ‹: åŸºäºè‹±æ–‡å’Œä¸­æ–‡è¾“å…¥ï¼Œé¢„æµ‹ä¸­æ–‡çš„ä¸‹ä¸€ä¸ªå­—ç¬¦")
print(f"   æŸå¤±è®¡ç®—: é¢„æµ‹ç»“æœä¸ä¸­æ–‡ç›®æ ‡æ¯”è¾ƒ")
print(f"   ğŸ’¡ ä¸ºä»€ä¹ˆä¸­æ–‡è¾“å…¥å’Œç›®æ ‡ä¸åŒï¼Ÿ")
print(f"      - è¾“å…¥: [SOS, ä½ ] â†’ æ¨¡å‹çœ‹åˆ°å¼€å§‹æ ‡è®°å’Œå‰é¢çš„å­—ç¬¦")
print(f"      - ç›®æ ‡: [ä½ , EOS] â†’ æ¨¡å‹åº”è¯¥é¢„æµ‹çš„ä¸‹ä¸€ä¸ªå­—ç¬¦")
print(f"      - è¿™æ ·åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæ¨¡å‹éƒ½çŸ¥é“åº”è¯¥é¢„æµ‹ä»€ä¹ˆï¼")

print(f"\nğŸ¯ æ•°æ®æµåŠ¨æ€»ç»“:")
print(f"   åŸå§‹æ–‡æœ¬ â†’ åˆ†è¯ â†’ ç¼–ç  â†’ å¡«å…… â†’ å¼ é‡ â†’ æ¨¡å‹ â†’ æŸå¤± â†’ æ¢¯åº¦ â†’ æ›´æ–°")


# 4.3 è§£ç å™¨å®ç°ï¼šç”Ÿæˆè¾“å‡ºåºåˆ—
# è§£ç å™¨å®ç°
class Decoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):
        """
        è§£ç å™¨
        Args:
            vocab_size: ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°
            embedding_dim: è¯åµŒå…¥ç»´åº¦
            hidden_dim: LSTMéšè—å±‚ç»´åº¦
            num_layers: LSTMå±‚æ•°
        """
        super(Decoder, self).__init__()

        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # LSTMå±‚ï¼šç”¨äºç”Ÿæˆåºåˆ—
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,
                            batch_first=True, bidirectional=False)

        # è¾“å‡ºå±‚ï¼šå°†éšçŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°
        self.output_projection = nn.Linear(hidden_dim, vocab_size)

    def forward(self, input_seq, hidden_state):
        """
        å‰å‘ä¼ æ’­
        Args:
            input_seq: è¾“å…¥åºåˆ— [batch_size, seq_len]
            hidden_state: ç¼–ç å™¨ä¼ æ¥çš„éšçŠ¶æ€ (hidden, cell)
        Returns:
            outputs: è¾“å‡ºåºåˆ—çš„è¯æ±‡åˆ†å¸ƒ [batch_size, seq_len, vocab_size]
            hidden_state: æ›´æ–°åçš„éšçŠ¶æ€
        """
        # 1. è¯åµŒå…¥
        embedded = self.embedding(input_seq)

        # 2. LSTMå¤„ç†
        outputs, hidden_state = self.lstm(embedded, hidden_state)

        # 3. æŠ•å½±åˆ°è¯æ±‡è¡¨
        outputs = self.output_projection(outputs)

        return outputs, hidden_state

    def generate(self, hidden_state, max_length=20, start_token=1, end_token=2):
        """
        ç”Ÿæˆåºåˆ—ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰
        Args:
            hidden_state: ç¼–ç å™¨çš„éšçŠ¶æ€
            max_length: ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
            start_token: å¼€å§‹æ ‡è®°çš„ç´¢å¼•
            end_token: ç»“æŸæ ‡è®°çš„ç´¢å¼•
        Returns:
            generated_sequence: ç”Ÿæˆçš„è¯æ±‡ç´¢å¼•åºåˆ—
        """
        batch_size = hidden_state[0].size(1)

        # åˆå§‹åŒ–è¾“å…¥ä¸ºå¼€å§‹æ ‡è®°
        current_input = torch.tensor([[start_token]] * batch_size)

        generated_sequence = []

        for _ in range(max_length):
            # è·å–å½“å‰æ¬¡çš„è¾“å‡º
            output, hidden_state = self.forward(current_input, hidden_state)

            # è´ªå¿ƒé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯
            predicted_word = output.argmax(dim=-1)
            generated_sequence.append(predicted_word.item())

            # å¦‚æœç”Ÿæˆäº†ç»“æŸæ ‡è®°ï¼Œåœæ­¢ç”Ÿæˆ
            if predicted_word.item() == end_token:
                break

            # æ›´æ–°ä¸‹ä¸€æ­¥çš„è¾“å…¥
            current_input = predicted_word

        return generated_sequence

# åˆ›å»ºè§£ç å™¨
zh_vocab_size = len(zh_vocab)
decoder = Decoder(zh_vocab_size, embedding_dim, hidden_dim)

print(f"ğŸ—ï¸ è§£ç å™¨åˆ›å»ºå®Œæˆï¼")
print(f"ğŸ“ å‚æ•°æ•°é‡: {sum(p.numel() for p in decoder.parameters()):,}")

# æµ‹è¯•è§£ç å™¨
test_tgt_input = sample_batch['tgt_input'][:2]
print(f"\nğŸ§ª æµ‹è¯•ç›®æ ‡è¾“å…¥å½¢çŠ¶: {test_tgt_input.shape}")

with torch.no_grad():
    # ä½¿ç”¨ç¼–ç å™¨çš„éšçŠ¶æ€ä½œä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€
    decoder_outputs, _ = decoder(test_tgt_input, (hidden, cell))
    print(f"âœ… è§£ç å™¨è¾“å‡ºå½¢çŠ¶: {decoder_outputs.shape}")
    print(f"âœ… è¾“å‡ºè¯æ±‡åˆ†å¸ƒç»´åº¦: {decoder_outputs.size(-1)} (åº”è¯¥ç­‰äºä¸­æ–‡è¯æ±‡è¡¨å¤§å° {zh_vocab_size})")


# æ¨¡å‹å‚æ•°å’Œè®¡ç®—å¤æ‚åº¦åˆ†æ
print("ğŸ“Š æ¨¡å‹å‚æ•°åˆ†æ")
print("=" * 50)

# æ˜¾ç¤ºç¼–ç å™¨å‚æ•°è¯¦æƒ…
print(f"\nğŸ” ç¼–ç å™¨å‚æ•°è¯¦ç»†åˆ†æ:")
total_params = 0
for name, param in encoder.named_parameters():
    param_count = param.numel()
    total_params += param_count
    print(f"    {name:25s}: {param.shape} -> {param_count:,} å‚æ•°")

print(f"    {'æ€»è®¡':25s}: {total_params:,} å‚æ•°")

# è®¡ç®—å‚æ•°ç»„æˆ
vocab_size = len(en_vocab)
embedding_dim = 64
hidden_dim = 128

print(f"\nğŸ§® å‚æ•°è®¡ç®—éªŒè¯:")
embedding_params = vocab_size * embedding_dim
lstm_params = 4 * (embedding_dim * hidden_dim + hidden_dim * hidden_dim + hidden_dim)           # LSTMå…¬å¼
print(f"  è¯åµŒå…¥å±‚ï¼š{vocab_size} Ã— {embedding_dim} = {embedding_params:,}")
print(f"  LSTMå±‚ï¼š4 Ã— ({embedding_dim} Ã— {hidden_dim} + {hidden_dim} Ã— {hidden_dim} + {hidden_dim} = {lstm_params:,}")
print(f"  æ€»è®¡: {embedding_params + lstm_params:,}")

# å†…å­˜å ç”¨ä¼°è®¡
print(f"\nğŸ’¾ å†…å­˜å ç”¨ä¼°è®¡:")
bytes_per_param = 4         # float32
model_memory_mb = total_params * bytes_per_param / (1024**2)
print(f"  æ¨¡å‹å‚æ•°: {model_memory_mb:.2f} MB")

# è®¡ç®—å¤æ‚åº¦åˆ†æ
print(f"\nâš¡ æ—¶é—´å¤æ‚åº¦åˆ†æ:")
print(f"  ç¼–ç å™¨å‰å‘ä¼ æ’­: O(seq_len Ã— embedding_dim Ã— hidden_dim)")
print(f"  å…¶ä¸­ seq_len â‰ˆ {max([len(sent.split()) for sent, _ in raw_data])}")
print(f"      embedding_dim = {embedding_dim}")
print(f"      hidden_dim = {hidden_dim}")

# å®é™…æµ‹è¯•ç¼–ç å™¨é€Ÿåº¦
import time
test_times = []
test_input = sample_batch['src'][:2]

print(f"\nğŸ•’ å®é™…æ€§èƒ½æµ‹è¯•:")
for i in range(5):
    start_time = time.time()
    with torch.no_grad():
        outputs, (hidden, cell) = encoder(test_input)
    end_time = time.time()
    test_times.append(end_time - start_time)

avg_time = sum(test_times) / len(test_times)
print(f"  ç¼–ç å™¨å‰å‘ä¼ æ’­æ—¶é—´: {avg_time*1000:.2f} ms (å¹³å‡)")
print(f"  å¤„ç†é€Ÿåº¦: {test_input.shape[0]/avg_time:.1f} å¥å­/ç§’")


# 4.4 å®Œæ•´çš„Seq2Seqæ¨¡å‹: å°†ç¼–ç å™¨å’Œè§£ç å™¨ç»„åˆ
# å®Œæ•´çš„Seq2Seqæ¨¡å‹
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        """
        Seq2Seqæ¨¡å‹
        Args:
            encoder: ç¼–ç å™¨
            decoder: è§£ç å™¨
            device: è®¡ç®—è®¾å¤‡ (cpu/gpu)
        """
        super(Seq2Seq, self).__init__()

        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src_seq, tgt_seq, teacher_forcing_ratio=1):
        """
        è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­
        Args:
            src_seq: æºåºåˆ— [batch_size, src_len]
            tgt_seq: ç›®æ ‡åºåˆ— [batch_size, tgt_len]
            teacher_forcing_ratio: æ•™å¸ˆå¼ºåˆ¶æ¯”ä¾‹
        Returns:
            outputs: è§£ç å™¨è¾“å‡º [batch_size, tgt_len, vocab_size]
        """
        batch_size = src_seq.size(0)
        tgt_len = tgt_seq.size(1)
        vocab_size = self.decoder.vocab_size

        # å­˜å‚¨è§£ç å™¨çš„æ‰€æœ‰è¾“å‡º
        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(self.device)

        # 1. ç¼–ç é˜¶æ®µ: è·å–æºåºåˆ—çš„è¡¨ç¤º
        _, hidden_state = self.encoder(src_seq)

        # 2. è§£ç é˜¶æ®µ: é€æ­¥ç”Ÿæˆç›®æ ‡åºåˆ—
        # è§£ç å™¨çš„ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯SOSæ ‡è®°
        decoder_input = tgt_seq[:, :1]          # ç¬¬ä¸€ä¸ªtoken (SOS)

        # ä»ç¬¬0ä¸ªæ—¶é—´æ­¥å¼€å§‹è®­ç»ƒï¼Œè€Œä¸æ˜¯ä»ç¬¬1ä¸ªæ—¶é—´æ­¥
        for t in range(tgt_len):
            # è§£ç å™¨å‰å‘ä¼ æ’­
            output, hidden_state = self.decoder(decoder_input, hidden_state)
            outputs[:, t:t+1, :] = output

            # æ•™å¸ˆå¼ºåˆ¶ï¼šå†³å®šä¸‹ä¸€ä¸ªè¾“å…¥æ˜¯çœŸå®æ ‡ç­¾è¿˜æ˜¯æ¨¡å‹é¢„æµ‹
            use_teacher_forcing = random.random() < teacher_forcing_ratio

            if use_teacher_forcing and t < tgt_len - 1:
                # ä½¿ç”¨çœŸå®çš„ä¸‹ä¸€ä¸ªè¯ä½œä¸ºè¾“å…¥ï¼ˆä½†ä¸è¦è¶…å‡ºåºåˆ—é•¿åº¦ï¼‰
                decoder_input = tgt_seq[:, t+1:t+2]
            else:
                # ä½¿ç”¨æ¨¡å‹é¢„æµ‹çš„è¯ä½œä¸ºè¾“å…¥
                decoder_input = output.argmax(dim=-1)

        return outputs

    def translate(self, src_seq, max_length=20):
        """
        æ¨ç†æ—¶çš„ç¿»è¯‘åŠŸèƒ½
        Args:
            src_seq: æºåºåˆ— [1, src_len]
            max_length: ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
        Returns:
            generated_indices: ç”Ÿæˆçš„è¯æ±‡ç´¢å¼•åˆ—è¡¨
        """
        self.eval()         # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        with torch.no_grad():
            # ç¼–ç æºåºåˆ—
            _, hidden_state = self.encoder(src_seq)

            # ç”Ÿæˆç›®æ ‡åºåˆ—
            generated_indices = []
            decoder_input = torch.tensor([[zh_vocab.word2idx[zh_vocab.SOS_TOKEN]]]).to(self.device)

            for _ in range(max_length):
                output, hidden_state = self.decoder(decoder_input, hidden_state)
                predicted_id = output.argmax(dim=-1).item()

                generated_indices.append(predicted_id)

                # å¦‚æœé¢„æµ‹åˆ°ç»“æŸæ ‡è®°ï¼Œåœæ­¢ç”Ÿæˆ
                if predicted_id == zh_vocab.word2idx[zh_vocab.EOS_TOKEN]:
                    break

                # ä¸‹ä¸€æ­¥çš„è¾“å…¥æ˜¯å½“å‰é¢„æµ‹çš„è¯
                decoder_input = torch.tensor([[predicted_id]]).to(self.device)

        return generated_indices

# åˆ›å»ºè®¾å¤‡å¯¹è±¡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åˆ›å»ºå®Œæ•´çš„Seq2Seqæ¨¡å‹
model = Seq2Seq(encoder, decoder, device).to(device)

print(f"ğŸ¯ Seq2Seqæ¨¡å‹åˆ›å»ºå®Œæˆï¼")
print(f"ğŸ“± è¿è¡Œè®¾å¤‡: {device}")
print(f"ğŸ“ æ€»å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# æ˜¾ç¤ºæ¨¡å‹ç»“æ„
print(f"\nğŸ—ï¸ æ¨¡å‹ç»“æ„:")
print(f"  ç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in model.encoder.parameters()):,}")
print(f"  è§£ç å™¨å‚æ•°: {sum(p.numel() for p in model.decoder.parameters()):,}")

# æµ‹è¯•æ¨¡å‹
test_src = sample_batch['src'][:1].to(device)               # å–ä¸€ä¸ªæ ·æœ¬
test_tgt = sample_batch['tgt_input'][:1].to(device)

print(f"\nğŸ§ª æ¨¡å‹æµ‹è¯•:")
print(f"  è¾“å…¥å½¢çŠ¶: {test_src.shape}")
print(f"  ç›®æ ‡å½¢çŠ¶: {test_tgt.shape}")

with torch.no_grad():
    outputs = model(test_src, test_tgt, teacher_forcing_ratio=1.0)
    print(f"âœ… æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {outputs.shape}")


# è®­ç»ƒæˆ‘ä»¬çš„ç¿»è¯‘æ¨¡å‹
# è®­ç»ƒ vs æ¨ç†è¯¦ç»†å¯¹æ¯”æ¼”ç¤º
print("ğŸ­ è®­ç»ƒæ¨¡å¼ vs æ¨ç†æ¨¡å¼è¯¦ç»†å¯¹æ¯”")
print("=" * 70)

# ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥æ¼”ç¤º
demo_en = "hello"
demo_zh = "ä½ å¥½"

print(f"ğŸ“ æ¼”ç¤ºå¥å­: '{demo_en}' â†’ '{demo_zh}'")
print("-" * 50)

# å‡†å¤‡è¾“å…¥æ•°æ®
src_tensor = torch.tensor([en_vocab.encode_sentence(demo_en)]).to(device)
tgt_input = [zh_vocab.word2idx[zh_vocab.SOS_TOKEN]] + zh_vocab.encode_sentence(demo_zh, add_eos=False)
tgt_input_tensor = torch.tensor([tgt_input]).to(device)
tgt_output = zh_vocab.encode_sentence(demo_zh, add_eos=True)

print(f"ğŸ“ è®­ç»ƒæ¨¡å¼æ¼”ç¤º:")
print(f"  è¾“å…¥ç¼–ç : {src_tensor.tolist()[0]} -> {[en_vocab.idx2word[i] for i in src_tensor.tolist()[0]]}")
print(f"  ç›®æ ‡è¾“å…¥: {tgt_input_tensor.tolist()[0]} -> {[zh_vocab.idx2word[i] for i in tgt_input_tensor.tolist()[0]]}")
print(f"  ç›®æ ‡è¾“å‡º: {tgt_output} -> {[zh_vocab.idx2word[i] for i in tgt_output]}")

# è®­ç»ƒæ¨¡å¼çš„è¯¦ç»†æ­¥éª¤
model.train()
print(f"\n  ğŸ”„ è®­ç»ƒæ­¥éª¤è¯¦è§£:")

# ç¼–ç é˜¶æ®µ
with torch.no_grad():
    _, (encoder_hidden, encoder_cell) = model.encoder(src_tensor)
    print(f"  1. ç¼–ç å™¨å¤„ç†: '{demo_en}' -> éšçŠ¶æ€å½¢çŠ¶ {encoder_hidden.shape}")

    # è§£ç é˜¶æ®µï¼ˆæ¨¡æ‹Ÿï¼‰
    decoder_hidden = encoder_hidden
    decoder_cell = encoder_cell

    print(f"  2. è§£ç å™¨æ­¥éª¤:")
    for t in range(len(tgt_input)):
        current_input = tgt_input_tensor[:, t:t+1]          # å½“å‰æ—¶é—´æ­¥è¾“å…¥

        # è§£ç å™¨å‰å‘ä¼ æ’­
        decoder_output, (decoder_hidden, decoder_cell) = model.decoder(current_input, (decoder_hidden, decoder_cell))
        predicted_id = decoder_output.argmax(dim=-1).item()
        predicted_word = zh_vocab.idx2word[predicted_id]

        if t < len(tgt_input):
            true_word = zh_vocab.idx2word[tgt_output[t]]
            print(f"    æ­¥éª¤{t+1}: è¾“å…¥'{zh_vocab.idx2word[current_input.item()]}' -> é¢„æµ‹'{predicted_word}' (çœŸå®: '{true_word}')")
        else:
            print(f"    æ­¥éª¤{t+1}: è¾“å…¥'{zh_vocab.idx2word[current_input.item()]}' -> é¢„æµ‹'{predicted_word}'")

print(f"\nğŸ”® æ¨ç†æ¨¡å¼æ¼”ç¤º:")
model.eval()

# æ¨ç†æ¨¡å¼çš„è¯¦ç»†æ­¥éª¤
print(f"    è¾“å…¥ç¼–ç : {src_tensor.tolist()[0]} -> {[en_vocab.idx2word[i] for i in src_tensor.tolist()[0]]}")
print(f"    ç›®æ ‡è¾“å‡º: æœªçŸ¥ï¼éœ€è¦é€æ­¥ç”Ÿæˆ")

print(f"\n   ğŸ”„ æ¨ç†æ­¥éª¤è¯¦è§£:")
with torch.no_grad():
    # ç¼–ç é˜¶æ®µ
    _, (encoder_hidden, encoder_cell) = model.encoder(src_tensor)
    print(f"    1. ç¼–ç å™¨å¤„ç†: '{demo_en}' -> éšçŠ¶æ€å½¢çŠ¶ {encoder_hidden.shape}")

    # è§£ç é˜¶æ®µ
    decoder_hidden = encoder_hidden
    decoder_cell = encoder_cell

    current_input = torch.tensor([[zh_vocab.word2idx[zh_vocab.SOS_TOKEN]]]).to(device)
    generated_sequence = []

    print(f"    2. è§£ç å™¨æ­¥éª¤:")
    for t in range(5):              # æœ€å¤šç”Ÿæˆ5ä¸ªè¯
        # è§£ç å™¨å‰å‘ä¼ æ’­
        decoder_output, (decoder_hidden, decoder_cell) = model.decoder(current_input, (decoder_hidden, decoder_cell))
        predicted_id = decoder_output.argmax(dim=-1).item()
        predicted_word = zh_vocab.idx2word[predicted_id]

        print(f"    æ­¥éª¤{t+1}: è¾“å…¥'{zh_vocab.idx2word[current_input.item()]}' -> é¢„æµ‹'{predicted_word}'")

        generated_sequence.append(predicted_id)

        # åœæ­¢æ¡ä»¶
        if predicted_id == zh_vocab.word2idx[zh_vocab.EOS_TOKEN]:
            print(f"        é‡åˆ°ç»“æŸç¬¦ï¼Œåœæ­¢ç”Ÿæˆ")
            break

        # ä¸‹ä¸€æ­¥çš„è¾“å…¥æ˜¯å½“å‰çš„é¢„æµ‹ï¼ˆå…³é”®åŒºåˆ«ï¼ï¼‰
        current_input = torch.tensor([[predicted_id]]).to(device)

    generated_text = zh_vocab.decode_sentence(generated_sequence)
    print(f"    3. æœ€ç»ˆç”Ÿæˆ: '{generated_text}'")

print(f"\nğŸ’¡ å…³é”®åŒºåˆ«æ€»ç»“:")
print(f"   ğŸ“ è®­ç»ƒæ¨¡å¼:")
print(f"      - è§£ç å™¨è¾“å…¥: ä½¿ç”¨çœŸå®çš„ç›®æ ‡åºåˆ— (Teacher Forcing)")
print(f"      - ä¼˜ç‚¹: è®­ç»ƒç¨³å®šã€å¿«é€Ÿ")
print(f"      - ç¼ºç‚¹: ä¸æ¨ç†ä¸ä¸€è‡´")
print(f"   ğŸ”® æ¨ç†æ¨¡å¼:")
print(f"      - è§£ç å™¨è¾“å…¥: ä½¿ç”¨è‡ªå·±çš„é¢„æµ‹ç»“æœ")
print(f"      - ä¼˜ç‚¹: çœŸå®çš„ä½¿ç”¨åœºæ™¯")
print(f"      - ç¼ºç‚¹: é”™è¯¯ä¼šç´¯ç§¯ä¼ æ’­")

print(f"\nâš ï¸  æ›å…‰åå·® (Exposure Bias):")
print(f"   é—®é¢˜: è®­ç»ƒæ—¶æ¨¡å‹ä»æœªè§è¿‡è‡ªå·±çš„é”™è¯¯é¢„æµ‹")
print(f"   åæœ: æ¨ç†æ—¶ä¸€æ—¦å‡ºé”™ï¼Œå¯èƒ½ä¸€é”™åˆ°åº•")
print(f"   è§£å†³æ–¹æ¡ˆ: è°ƒåº¦é‡‡æ ·ã€å¼ºåŒ–å­¦ä¹ ç­‰é«˜çº§æŠ€æœ¯")


# è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
def train_epoch(model, dataloader, optimizer, criterion, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0

    for batch_idx, batch in enumerate(dataloader):
        # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
        src = batch['src'].to(device)
        tgt_input = batch['tgt_input'].to(device)
        tgt_output = batch['tgt_output'].to(device)

        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        output = model(src, tgt_input, teacher_forcing_ratio=0.5)

        # è®¡ç®—æŸå¤±ï¼ˆå¿½ç•¥å¡«å……ç¬¦å·ï¼‰
        # ä¸å†å»æ‰ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå› ä¸ºç°åœ¨æ¨¡å‹ä¼šå­¦ä¹ é¢„æµ‹ç¬¬ä¸€ä¸ªå­—ç¬¦
        output = output.reshape(-1, output.size(-1))                # å±•å¹³æ‰€æœ‰æ—¶é—´æ­¥
        tgt_output = tgt_output.reshape(-1)                         # å±•å¹³æ‰€æœ‰æ—¶é—´æ­¥

        loss = criterion(output, tgt_output)

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # æ›´æ–°å‚æ•°
        optimizer.step()

        total_loss += loss.item()

        if batch_idx % 5 == 0:              # æ¯5ä¸ªbatchæ‰“å°ä¸€æ¬¡
            print(f'  Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}')

    return total_loss / len(dataloader)

def evaluate_model(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in dataloader:
            src = batch['src'].to(device)
            tgt_input = batch['tgt_input'].to(device)
            tgt_output = batch['tgt_output'].to(device)

            # å‰å‘ä¼ æ’­ï¼ˆä¸ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶ï¼‰
            output = model(src, tgt_input, teacher_forcing_ratio=0)

            # è®¡ç®—æŸå¤±ï¼Œä¸å†å»æ‰ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥
            output = output.reshape(-1, output.size(-1))
            tgt_output = tgt_output.reshape(-1)

            loss = criterion(output, tgt_output)
            total_loss += loss.item()

    return total_loss / len(dataloader)

def translate_sentence(model, sentence, src_vocab, tgt_vocab, device, max_length=20):
    """ç¿»è¯‘å•ä¸ªå¥å­"""
    model.eval()

    # é¢„å¤„ç†å¥å­
    tokens = sentence.lower().split()
    indices = [src_vocab.word2idx.get(token, src_vocab.word2idx[src_vocab.UNK_TOKEN]) for token in tokens]
    indices.append(src_vocab.word2idx[src_vocab.EOS_TOKEN])

    # è½¬æ¢ä¸ºtensor
    src_tensor = torch.tensor([indices]).to(device)

    # ç¿»è¯‘
    with torch.no_grad():
        generated_indices = model.translate(src_tensor, max_length)

    # è§£ç ä¸ºæ–‡æœ¬
    translation = tgt_vocab.decode_sentence(generated_indices)

    return translation

# è®¾ç½®è®­ç»ƒå‚æ•°
criterion = nn.CrossEntropyLoss(ignore_index=zh_vocab.word2idx[zh_vocab.PAD_TOKEN])
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("ğŸ‹ï¸ è®­ç»ƒè®¾ç½®å®Œæˆï¼")
print(f"ğŸ“‰ æŸå¤±å‡½æ•°: CrossEntropyLoss (å¿½ç•¥å¡«å……ç¬¦å·)")
print(f"ğŸ”§ ä¼˜åŒ–å™¨: Adam (å­¦ä¹ ç‡: 0.001)")
print(f"ğŸ¯ è®¾å¤‡: {device}")


# éªŒè¯ä¿®å¤æ•ˆæœ
print("ğŸ”§ éªŒè¯ä¿®å¤æ•ˆæœ")
print("=" * 50)

# é‡æ–°åˆ›å»ºæ¨¡å‹ä»¥åº”ç”¨ä¿®å¤
model = Seq2Seq(encoder, decoder, device).to(device)

# ç®€å•æµ‹è¯•è®­ç»ƒæ˜¯å¦æ­£å¸¸
print("\nğŸ§ª æµ‹è¯•ä¿®å¤åçš„è®­ç»ƒæµç¨‹:")
test_batch = next(iter(dataloader))
src = test_batch['src'][:2].to(device)
tgt_input = test_batch['tgt_input'][:2].to(device)
tgt_output = test_batch['tgt_output'][:2].to(device)

print(f"   æºåºåˆ—å½¢çŠ¶: {src.shape}")
print(f"   ç›®æ ‡è¾“å…¥å½¢çŠ¶: {tgt_input.shape}")
print(f"   ç›®æ ‡è¾“å‡ºå½¢çŠ¶: {tgt_output.shape}")

# æµ‹è¯•å‰å‘ä¼ æ’­
model.train()
with torch.no_grad():
    output = model(src, tgt_input, teacher_forcing_ratio=1.0)
    print(f"    æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")

    # æµ‹è¯•æŸå¤±è®¡ç®—
    criterion = nn.CrossEntropyLoss(ignore_index=zh_vocab.word2idx[zh_vocab.PAD_TOKEN])
    output_flat = output.reshape(-1, output.size(-1))
    tgt_output_flat = tgt_output.reshape(-1)
    loss = criterion(output_flat, tgt_output_flat)
    print(f"    æŸå¤±è®¡ç®—æˆåŠŸï¼ŒæŸå¤±å€¼: {loss.item():.4f}")

# æµ‹è¯•æ¨ç†
print(f"\nğŸ”® æµ‹è¯•ä¿®å¤åçš„æ¨ç†:")
test_sentences = ["hi.", "hello!", "good morning"]
for sentence in test_sentences:
    translation = translate_sentence(model, sentence, en_vocab, zh_vocab, device)
    print(f"    '{sentence}' -> '{translation}'")

print(f"\nâœ… ä¿®å¤éªŒè¯å®Œæˆï¼")
print(f"æ³¨æ„ï¼šç”±äºæ¨¡å‹å°šæœªé‡æ–°è®­ç»ƒï¼Œé¢„æµ‹ç»“æœå¯èƒ½ä»ç„¶ä¸å‡†ç¡®ã€‚")
print(f"ä½†ç°åœ¨æ¨¡å‹çš„æ¶æ„å·²ç»ä¿®å¤ï¼Œé‡æ–°è®­ç»ƒååº”è¯¥èƒ½æ­£ç¡®é¢„æµ‹ç¬¬ä¸€ä¸ªå­—ç¬¦ã€‚")

# å¼€å§‹è®­ç»ƒ
print("ğŸš€ å¼€å§‹è®­ç»ƒSeq2Seqæ¨¡å‹...")
print("=" * 50)

num_epochs = 50         # ç”±äºæ•°æ®é›†å¾ˆå°ï¼Œæˆ‘ä»¬å¤šè®­ç»ƒå‡ è½®
train_losses = []
best_loss = float('inf')

for epoch in range(num_epochs):
    print(f"\nğŸ“š Epoch {epoch + 1}/{num_epochs}")

    # è®­ç»ƒ
    train_loss = train_epoch(model, dataloader, optimizer, criterion, device)
    train_losses.append(train_loss)

    print(f"âœ… å¹³å‡è®­ç»ƒæŸå¤±: {train_loss:.4f}")

    # æ¯10ä¸ªepochæµ‹è¯•ç¿»è¯‘æ•ˆæœ
    if (epoch + 1) % 10 == 0:
        print("\nğŸ” ç¿»è¯‘æµ‹è¯•:")
        test_sentences = ["hello", "thank you", "i love you", "good morning"]

        for sentence in test_sentences:
            translation = translate_sentence(model, sentence, en_vocab, zh_vocab, device)
            print(f"    '{sentence}' -> '{translation}'")

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if train_loss < best_loss:
        best_loss = train_loss
        torch.save(model.state_dict(), "best_seq2seq_model.pt")

print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.4f}")
print("æ¨¡å‹å·²ä¿å­˜ä¸º 'best_seq2seq_model.pth'")


# 6. ç»“æœåˆ†æä¸å¯è§†åŒ–
# å®ç°æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡

import math
import re

class MTEvaluator:
    """æœºå™¨ç¿»è¯‘è¯„ä¼°å·¥å…·ç±»"""

    @staticmethod
    def calculate_bleu_score(candidate, reference, max_n=4):
        """
        è®¡ç®—BLEUè¯„åˆ† - æ”¯æŒä¸­è‹±æ–‡
        Args:
            candidate: å€™é€‰è¯‘æ–‡ï¼ˆå­—ç¬¦ä¸²ï¼‰
            reference: å‚è€ƒè¯‘æ–‡ï¼ˆå­—ç¬¦ä¸²ï¼‰
            max_n: æœ€å¤§n-gramé•¿åº¦
        Returns:
            bleu_score: BLEUè¯„åˆ†
        """
        # å¤„ç†ç©ºå­—ç¬¦ä¸²æƒ…å†µ
        if not candidate or not candidate.strip():
            return 0.0

        if not reference or not reference.strip():
            return 0.0

        # æ™ºèƒ½åˆ†è¯ï¼šæ ¹æ®è¯­è¨€ç±»å‹å•åˆ†è¯ç­–ç•¥
        candidate_tokens = MTEvaluator._smart_tokenize(candidate)
        reference_tokens = MTEvaluator._smart_tokenize(reference)

        # å†æ¬¡æ£€æŸ¥åˆ†è¯åæ˜¯å¦ä¸ºç©º
        if len(candidate_tokens) == 0 or len(reference_tokens) == 0:
            return 0.0

        # è®¡ç®—å„ä¸ªn-gramçš„ç²¾ç¡®ç‡
        precisions = []

        for n in range(1, max_n + 1):
            # è·å–n-gram
            candidate_ngrams = MTEvaluator._get_ngrams(candidate_tokens, n)
            reference_ngrams = MTEvaluator._get_ngrams(reference_tokens, n)

            if len(candidate_ngrams) == 0:
                precisions.append(0.0)
                continue

            # è®¡ç®—ç²¾ç¡®ç‡
            overlap = 0
            for ngram in candidate_ngrams:
                if ngram in reference_ngrams:
                    overlap += min(candidate_ngrams[ngram], reference_ngrams[ngram])

            precision = overlap / sum(candidate_ngrams.values()) if sum(candidate_ngrams.values()) > 0 else 0.0
            precisions.append(precision)

        # è®¡ç®—ç®€æ´æ€§æƒ©ç½š
        bp = MTEvaluator._brevity_penalty(candidate_tokens, reference_tokens)

        # è®¡ç®—BLEUè¯„åˆ†ï¼ˆå‡ ä½•è¯„åˆ†ï¼‰
        # ä¿®å¤ï¼šåªè¦æœ‰ä»»ä½•ä¸€ä¸ªprecisionä¸º0ï¼Œä½¿ç”¨å¹³æ»‘ç­–ç•¥
        valid_precisions = [p for p in precisions if p > 0]
        if len(valid_precisions) == 0:
            return 0.0

        # ä½¿ç”¨å¹³æ»‘ç­–ç•¥ï¼šå¯¹äº0å€¼precisionï¼Œä½¿ç”¨å¾ˆå°çš„å€¼æ›¿ä»£
        smoothed_precision = [max(p, 1e-10) for p in precisions]

        log_sum = sum(math.log(p) for p in smoothed_precision) / len(smoothed_precision)
        bleu = bp * math.exp(log_sum)

        return bleu

    @staticmethod
    def _smart_tokenize(text):
        """æ™ºèƒ½åˆ†è¯ï¼šæ ¹æ®æ–‡æœ¬ç±»å‹é€‰æ‹©åˆé€‚çš„åˆ†è¯ç­–ç•¥"""
        # ç§»é™¤é¦–ä½ç©ºæ ¼
        text = text.strip()

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        if MTEvaluator._contains_chinese(text):
            # ä¸­æ–‡ï¼šå­—ç¬¦çº§åˆ†è¯
            return MTEvaluator._chinese_tokenize(text)
        else:
            # è‹±æ–‡ç­‰ï¼šç©ºæ ¼åˆ†è¯
            return text.lower().split()

    @staticmethod
    def _contains_chinese(text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
        return bool(chinese_pattern.search(text))

    @staticmethod
    def _chinese_tokenize(text):
        """
        ä¸­æ–‡å­—ç¬¦çº§åˆ†è¯
        ä¿ç•™æ±‰å­—ã€æ•°å­—ã€å­—æ¯ï¼Œè¿‡æ»¤æ ‡ç‚¹ç¬¦å·
        """
        tokens = []
        for char in text:
            # ä¿ç•™æ±‰å­—ã€æ•°å­—ã€å­—æ¯
            if char.isalnum() or '\u4e00' <= char <= '\u9fff':
                tokens.append(char)
            # ä¿ç•™ä¸€äº›é‡è¦æ ‡ç‚¹ï¼ˆå¯é€‰ï¼‰
            elif char in 'ã€‚ï¼ï¼Ÿ!?.':
                tokens.append(char)
        return tokens

    @staticmethod
    def _get_ngrams(tokens, n):
        """è·å–n-gramè®¡æ•°"""
        ngrams = Counter()
        for i in range(len(tokens) - n + 1):
            ngram = tuple(tokens[i:i + n])
            ngrams[ngram] += 1
        return ngrams

    @staticmethod
    def _brevity_penalty(candiate_tokens, reference_tokens):
        """è®¡ç®—ç®€æ´æ€§æƒ©ç½š"""
        c = len(candiate_tokens)            # å€™é€‰è¯‘æ–‡é•¿åº¦
        r = len(reference_tokens)           # å‚è€ƒè¯‘æ–‡é•¿åº¦

        # å¤„ç†å€™é€‰è¯‘æ–‡ä¸ºç©ºçš„æƒ…å†µ
        if c == 0:
            return 0.0

        if c > r:
            return 1.0
        else:
            return math.exp(1 - r / c)

    @staticmethod
    def calculate_word_overlap(candidate, reference):
        """è®¡ç®—è¯æ±‡é‡å ç‡ï¼ˆç®€åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼‰ - æ”¯æŒä¸­è‹±æ–‡"""
        if not candidate or not reference:
            return 0.0

        # ä½¿ç”¨æ™ºèƒ½åˆ†è¯
        candidate_tokens = set(MTEvaluator._smart_tokenize(candidate))
        reference_tokens = set(MTEvaluator._smart_tokenize(reference))

        if len(reference_tokens) == 0:
            return 0.0

        overlap = len(candidate_tokens.intersection(reference_tokens))
        return overlap / len(reference_tokens)

# æµ‹è¯•è¯„ä¼°æŒ‡æ ‡
print("ğŸ¯ æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡æµ‹è¯•")
print("=" * 50)

# æµ‹è¯•æ ·ä¾‹
test_cases = [
    {
        'candidate': 'bonjour comment allez vous',
        'reference': 'bonjour comment allez vous',
        'description': 'å®Œå…¨åŒ¹é…'
    },
    {
        'candidate': 'bonjour comment vous allez',
        'reference': 'bonjour comment allez vous',
        'description': 'è¯åºä¸åŒ'
    },
    {
        'candidate': 'bonjour comment',
        'reference': 'bonjour comment allez vous',
        'description': 'ç¿»è¯‘ä¸å®Œæ•´'
    },
    {
        'candidate': 'salut comment ca va bien merci',
        'reference': 'bonjour comment allez vous',
        'description': 'å®Œå…¨ä¸åŒçš„è¡¨è¾¾'
    }
]

evaluator = MTEvaluator()

for i, case in enumerate(test_cases):
    print(f"\nğŸ“ æµ‹è¯•æ¡ˆä¾‹ {i+1}: {case['description']}")
    print(f"   å€™é€‰è¯‘æ–‡: '{case['candidate']}'")
    print(f"   å‚è€ƒè¯‘æ–‡: '{case['reference']}'")

    # è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
    bleu = evaluator.calculate_bleu_score(case['candidate'], case['reference'])
    word_overlap = evaluator.calculate_word_overlap(case['candidate'], case['reference'])

    print(f"   ğŸ“Š BLEUè¯„åˆ†: {bleu}")
    print(f"   ğŸ“Š è¯æ±‡é‡å ç‡: {word_overlap}")

print(f"\nğŸ’¡ è¯„ä¼°æŒ‡æ ‡è¯´æ˜:")
print(f"   - BLEUè¯„åˆ†èŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½")
print(f"   - è¯æ±‡é‡å ç‡: 0-1ï¼Œè¶Šé«˜è¡¨ç¤ºè¯æ±‡åŒ¹é…åº¦è¶Šå¥½")
print(f"   - BLEUè€ƒè™‘è¯åºï¼Œè¯æ±‡é‡å ç‡ä¸è€ƒè™‘è¯åº")

# ğŸ”§ æµ‹è¯•ä¿®å¤åçš„ä¸­æ–‡BLEUè®¡ç®—
print(f"\n" + "="*70)
print(f"ğŸ”§ æµ‹è¯•ä¿®å¤åçš„ä¸­æ–‡BLEUè®¡ç®—")
print(f"=" * 70)

# ä¸­æ–‡æµ‹è¯•æ ·ä¾‹
chinese_test_cases = [
    {
        'candidate': 'ä½ å¥½ã€‚',
        'reference': 'ä½ å¥½ã€‚',
        'description': 'ä¸­æ–‡å®Œå…¨åŒ¹é…'
    },
    {
        'candidate': 'ä½ ç”¨è·‘çš„ã€‚',
        'reference': 'ä½ ç”¨è·‘çš„ã€‚',
        'description': 'ä¸­æ–‡é•¿å¥å®Œå…¨åŒ¹é…'
    },
    {
        'candidate': 'æˆ‘èµ¢äº†ã€‚',
        'reference': 'æˆ‘èµ¢äº†ã€‚',
        'description': 'ä¸­æ–‡åŠ¨è¯å¥å®Œå…¨åŒ¹é…'
    },
    {
        'candidate': 'å¥½ã€‚',
        'reference': 'ä½ å¥½ã€‚',
        'description': 'ä¸­æ–‡éƒ¨åˆ†åŒ¹é…'
    },
    {
        'candidate': 'å¥½ä½ ã€‚',
        'reference': 'ä½ å¥½ã€‚',
        'description': 'ä¸­æ–‡è¯åºé”™è¯¯'
    },
    {
        'candidate': 'ç­‰ä¸€ä¸‹ï¼',
        'reference': 'ç­‰ç­‰ï¼',
        'description': 'ä¸­æ–‡åŒä¹‰ä¸åŒè¯'
    }
]

for i, case in enumerate(chinese_test_cases):
    print(f"\nğŸ“ ä¸­æ–‡æµ‹è¯•æ¡ˆä¾‹ {i+1}: {case['description']}")
    print(f"   å€™é€‰è¯‘æ–‡: '{case['candidate']}'")
    print(f"   å‚è€ƒè¯‘æ–‡: '{case['reference']}'")

    # æ˜¾ç¤ºåˆ†è¯ç»“æœ
    candidate_tokens = evaluator._smart_tokenize(case['candidate'])
    reference_tokens = evaluator._smart_tokenize(case['reference'])
    print(f"   å€™é€‰åˆ†è¯: {candidate_tokens}")
    print(f"   å‚è€ƒåˆ†è¯: {reference_tokens}")

    # è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
    bleu = evaluator.calculate_bleu_score(case['candidate'], case['reference'])
    word_overlap = evaluator.calculate_word_overlap(case['candidate'], case['reference'])

    print(f"   ğŸ“Š BLEUè¯„åˆ†: {bleu}")
    print(f"   ğŸ“Š è¯æ±‡é‡å ç‡: {word_overlap}")

    # è¯¦ç»†æ˜¾ç¤ºn-gramåˆ†æï¼ˆä»…å¯¹å®Œå…¨åŒ¹é…çš„æ¡ˆä¾‹ï¼‰
    if case['candidate'] == case['reference']:
        print(f"   ğŸ” N-gramåˆ†æ:")
        for n in range(1, 5):
            candidate_ngrams = evaluator._get_ngrams(candidate_tokens, n)
            reference_ngrams = evaluator._get_ngrams(reference_tokens, n)

            if len(candidate_ngrams) > 0:
                overlap = 0
                for ngram in candidate_ngrams:
                    if ngram in reference_ngrams:
                        overlap += min(candidate_ngrams[ngram], reference_ngrams[ngram])
                precision = overlap / sum(candidate_ngrams.values())
                print(f"    {n}-gram: {overlap} / {sum(candidate_ngrams.values())} = {precision:.4f}")

print(f"\nâœ… ä¿®å¤éªŒè¯:")
print(f"   - ç°åœ¨å®Œå…¨åŒ¹é…çš„ä¸­æ–‡å¥å­BLEUè¯„åˆ†åº”è¯¥æ˜¯1.0000")
print(f"   - ä¸­æ–‡å­—ç¬¦çº§åˆ†è¯å·¥ä½œæ­£å¸¸")
print(f"   - N-gramé‡å è®¡ç®—æ­£ç¡®")


# ğŸ”„ é‡æ–°è®¡ç®—ä¿®å¤åçš„BLEUè¯„åˆ†
print("ğŸ”„ é‡æ–°è®¡ç®—ä¿®å¤åçš„ç¿»è¯‘æ•ˆæœ")
print("=" * 70)

# # åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
# print("ğŸ“‚ åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹...")
# model_path = "best_seq2seq_model.pt"
#
# try:
#     # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
#     import os
#     if os.path.exists(model_path):
#         # åŠ è½½æ¨¡å‹å‚æ•°
#         model.load_state_dict(torch.load(model_path, map_location=device))
#         model.eval()
#         print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
#         print(f"ğŸ“± è¿è¡Œè®¾å¤‡: {device}")
#     else:
#         print(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨")
#         print(f"ğŸ“ å°†ä½¿ç”¨å½“å‰æœªè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¼”ç¤º")
# except Exception as e:
#     print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
#     print(f"ğŸ“ å°†ä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œæ¼”ç¤º")

# ä½¿ç”¨ä¿®å¤åçš„è¯„ä¼°å™¨é‡æ–°è®¡ç®—
print("\nğŸ“Š ä¿®å¤åçš„è¯¦ç»†è¯„ä¼°ç»“æœ:")
print("-" * 90)
print(f"{'åºå·':<4} {'è‹±æ–‡åŸå¥':<18} {'çœŸå®ä¸­æ–‡':<20} {'é¢„æµ‹ä¸­æ–‡':<20} {'BLEU':<8} {'è¯æ±‡é‡å ':<8}")
print("-" * 90)

# é‡æ–°åˆ›å»ºè¯„ä¼°æŒ‡æ ‡æ•°ç»„
fixed_bleu_scores = []
fixed_word_overlaps = []
fixed_perfect_matches = 0  # ä¿®å¤ï¼šæ”¹ä¸ºæ•´æ•°è®¡æ•°å™¨

# åªæ˜¾ç¤ºå‰20ä¸ªç»“æœä»¥èŠ‚çœç©ºé—´
for i, (en, true_zh) in enumerate(raw_data[:20]):
    pred_zh = translate_sentence(model, en, en_vocab, zh_vocab, device)

    #ä½¿ç”¨ä¿®å¤åçš„BLEUè®¡ç®—
    bleu = evaluator.calculate_bleu_score(pred_zh, true_zh)
    word_overlap = evaluator.calculate_word_overlap(pred_zh, true_zh)

    fixed_bleu_scores.append(bleu)
    fixed_word_overlaps.append(word_overlap)

    # æ£€æŸ¥å®Œå…¨åŒ¹é…
    if pred_zh.strip() == true_zh.strip():
        fixed_perfect_matches += 1

    print(f"{i+1:<4} {en:<18} {true_zh:<20} {pred_zh:<20} {bleu:<8} {word_overlap:<8.3f}")

print("-" * 90)

# å¯¹æ¯”ä¿®å¤å‰åçš„ç»“æœ
print(f"\nğŸ“ˆ ä¿®å¤å‰åå¯¹æ¯”:")
print(f"   ğŸ”´ ä¿®å¤å‰å¹³å‡BLEU: 0.000 (å®Œå…¨é”™è¯¯)")
print(f"   ğŸŸ¢ ä¿®å¤åå¹³å‡BLEU: {sum(fixed_bleu_scores)/len(fixed_bleu_scores):.4f}")

# ç»Ÿè®¡æ”¹è¿›æƒ…å†µ
perfect_bleu_count = sum(1 for score in fixed_bleu_scores if score == 1.0)
good_bleu_count = sum(1 for score in fixed_bleu_scores if score >= 0.5)
bad_bleu_count = sum(1 for score in fixed_bleu_scores if score == 0.0)

print(f"\nğŸ“Š BLEUè¯„åˆ†åˆ†å¸ƒæ”¹è¿›:")
print(f"   ğŸ† å®Œç¾åŒ¹é… (BLEU=1.0): {perfect_bleu_count}æ¡")
print(f"   ğŸ‘ è‰¯å¥½ç¿»è¯‘ (BLEUâ‰¥0.5): {good_bleu_count}æ¡")
print(f"   ğŸ‘ ä»éœ€æ”¹è¿› (BLEU=0.0): {bad_bleu_count}æ¡")

# å±•ç¤ºå‡ ä¸ªå…·ä½“çš„æ”¹è¿›æ¡ˆä¾‹
print(f"\nğŸŒŸ ä¿®å¤æ•ˆæœå±•ç¤º:")
improvement_cases = [
    ("ä½ ç”¨è·‘çš„ã€‚", "ä½ ç”¨è·‘çš„ã€‚"),
    ("ç­‰ä¸€ä¸‹ï¼", "ç­‰ä¸€ä¸‹ï¼"),
    ("ä½ å¥½ã€‚", "ä½ å¥½ã€‚"),
    ("æˆ‘èµ¢äº†ã€‚", "æˆ‘èµ¢äº†ã€‚")
]

for i, (true_zh, pred_zh) in enumerate(improvement_cases):
    if i < len(fixed_bleu_scores):
        old_bleu = 0.000            # ä¿®å¤å‰çš„å€¼
        new_bleu = evaluator.calculate_bleu_score(pred_zh, true_zh)
        print(f"   æ¡ˆä¾‹{i + 1}: '{true_zh}' â†’ '{pred_zh}'")
        print(f"     ä¿®å¤å‰BLEU: {old_bleu:.3f} â†’ ä¿®å¤åBLEU: {new_bleu:.3f} âœ…")

print(f"\nğŸ‰ ä¿®å¤æˆåŠŸï¼")
print(f"   ç°åœ¨BLEUè¯„åˆ†èƒ½å¤Ÿæ­£ç¡®åæ˜ ä¸­æ–‡ç¿»è¯‘è´¨é‡")
print(f"   å®Œå…¨åŒ¹é…çš„å¥å­BLEUè¯„åˆ†ä¸º1.000")
print(f"   è¯åºé”™è¯¯æˆ–éƒ¨åˆ†åŒ¹é…çš„å¥å­æœ‰åˆç†çš„BLEUè¯„åˆ†")


# ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
plt.figure(figsize=(12, 5))

# å­å›¾1ï¼šè®­ç»ƒæŸå¤±
plt.subplot(1, 2, 1)
plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')
plt.title('æ¨¡å‹è®­ç»ƒæŸå¤±å˜åŒ–', fontsize=14, fontweight='bold')
plt.xlabel('è½®æ¬¡ (Epoch)', fontsize=12)
plt.ylabel('æŸå¤±å€¼', fontsize=12)
plt.grid(True, alpha=0.3)
plt.legend(fontsize=10)

# å­å›¾2ï¼šæŸå¤±çš„å¹³æ»‘è¶‹åŠ¿
plt.subplot(1, 2, 2)
# è®¡ç®—ç§»åŠ¨å¹³å‡ä»¥æ˜¾ç¤ºå¹³æ»‘è¶‹åŠ¿
window_size = 5
if len(train_losses) >= window_size:
    smoothed_losses = []
    for i in range(len(train_losses) - window_size + 1):
        smoothed_losses.append(sum(train_losses[i:i + window_size]) / window_size)

    plt.plot(range(window_size, len(train_losses) + 1), smoothed_losses,
             'r-', linewidth=2, label=f'{window_size}è½®ç§»åŠ¨å¹³å‡')
    plt.title('å¹³æ»‘åçš„æŸå¤±è¶‹åŠ¿', fontsize=14, fontweight='bold')
    plt.xlabel('è½®æ¬¡ (Epoch)', fontsize=12)
    plt.ylabel('å¹³æ»‘æŸå¤±å€¼', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=10)

plt.tight_layout()
plt.show()

print(f"ğŸ“Š è®­ç»ƒåˆ†æ:")
print(f"   åˆå§‹æŸå¤±: {train_losses[0]:.4f}")
print(f"   æœ€ç»ˆæŸå¤±: {train_losses[-1]:.4f}")
print(f"   æŸå¤±é™ä½: {train_losses[0] - train_losses[-1]:.4f} ({((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%)")
print(f"   æœ€ä½³æŸå¤±: {best_loss:.4f}")

# åˆ†ææŸå¤±å˜åŒ–è¶‹åŠ¿
if len(train_losses) > 10:
    early_avg = sum(train_losses[:10]) / 10
    late_avg = sum(train_losses[-10:]) / 10
    print(f"   å‰10è½®å¹³å‡: {early_avg:.4f}")
    print(f"   å10è½®å¹³å‡: {late_avg:.4f}")

    if late_avg < early_avg:
        print("âœ… æ¨¡å‹æŒç»­å­¦ä¹ æ”¹è¿›")
    else:
        print("âš ï¸ æ¨¡å‹å¯èƒ½è¿‡æ‹Ÿåˆæˆ–éœ€è¦è°ƒæ•´å­¦ä¹ ç‡")

# ä½¿ç”¨è¯„ä¼°æŒ‡æ ‡è¿›è¡Œæ¨¡å‹æ€§èƒ½åˆ†æ
model.load_state_dict(torch.load('best_seq2seq_model.pt'))
model.eval()

print("ğŸ¯ åŸºäºè¯„ä¼°æŒ‡æ ‡çš„ç¿»è¯‘æ•ˆæœåˆ†æ")
print("=" * 70)

# åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šè®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡
all_bleu_scores = []
all_word_overlaps = []
perfect_matches = 0

print("\nğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœ:")
print("-" * 90)
print(f"{'åºå·':<4} {'è‹±æ–‡åŸå¥':<18} {'çœŸå®ä¸­æ–‡':<20} {'é¢„æµ‹ä¸­æ–‡':<20} {'BLEU':<8} {'è¯æ±‡é‡å ':<8}")
print("-" * 90)

for i, (en, true_zh) in enumerate(raw_data):
    pred_zh = translate_sentence(model, en, en_vocab, zh_vocab, device)

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    bleu = evaluator.calculate_bleu_score(pred_zh, true_zh)
    word_overlap = evaluator.calculate_word_overlap(pred_zh, true_zh)

    all_bleu_scores.append(bleu)
    all_word_overlaps.append(word_overlap)

    # æ£€æŸ¥å®Œå…¨åŒ¹é…
    if pred_zh.strip() == true_zh.strip():
        perfect_matches += 1

    print(f"{i + 1:<4} {en:<18} {true_zh:<20} {pred_zh:<20} {bleu:<8.3f} {word_overlap:<8.3f}")

print("-" * 90)

# è®¡ç®—ç»¼åˆç»Ÿè®¡
avg_bleu = sum(all_bleu_scores) / len(all_bleu_scores)
avg_word_overlap = sum(all_word_overlaps) / len(all_word_overlaps)
perfect_match_rate = perfect_matches / len(raw_data)

print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½ç»¼åˆè¯„ä¼°:")
print(f"   ğŸ¯ å¹³å‡BLEUè¯„åˆ†: {avg_bleu:.4f}")
print(f"   ğŸ¯ å¹³å‡è¯æ±‡é‡å ç‡: {avg_word_overlap:.4f}")
print(f"   ğŸ¯ å®Œå…¨åŒ¹é…ç‡: {perfect_match_rate:.1%} ({perfect_matches}/{len(raw_data)})")

# åˆ†æä¸åŒBLEUåˆ†æ•°æ®µçš„åˆ†å¸ƒ
bleu_ranges = [
    (0.8, 1.0, "ä¼˜ç§€"),
    (0.6, 0.8, "è‰¯å¥½"),
    (0.4, 0.6, "ä¸­ç­‰"),
    (0.2, 0.4, "è¾ƒå·®"),
    (0.0, 0.2, "å¾ˆå·®")
]

print(f"\nğŸ“Š BLEUè¯„åˆ†åˆ†å¸ƒåˆ†æ:")
for min_score, max_score, label in bleu_ranges:
    count = sum(1 for score in all_bleu_scores if min_score <= score < max_score)
    percentage = count / len(all_bleu_scores) * 100
    print(f"   {label} ({min_score:.1f}-{max_score:.1f}): {count:2d}æ¡ ({percentage:5.1f}%)")

# æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„ç¿»è¯‘
best_idx = all_bleu_scores.index(max(all_bleu_scores))
worst_idx = all_bleu_scores.index(min(all_bleu_scores))

print(f"\nğŸ† æœ€ä½³ç¿»è¯‘ç¤ºä¾‹:")
en, true_zh = raw_data[best_idx]
pred_zh = translate_sentence(model, en, en_vocab, zh_vocab, device)
print(f"   è‹±æ–‡: {en}")
print(f"   çœŸå®: {true_zh}")
print(f"   é¢„æµ‹: {pred_zh}")
print(f"   BLEU: {all_bleu_scores[best_idx]:.4f}")

print(f"\nâš ï¸  æœ€å·®ç¿»è¯‘ç¤ºä¾‹:")
en, true_zh = raw_data[worst_idx]
pred_zh = translate_sentence(model, en, en_vocab, zh_vocab, device)
print(f"   è‹±æ–‡: {en}")
print(f"   çœŸå®: {true_zh}")
print(f"   é¢„æµ‹: {pred_zh}")
print(f"   BLEU: {all_bleu_scores[worst_idx]:.4f}")

# æµ‹è¯•æ³›åŒ–èƒ½åŠ›
print(f"\n\nğŸ” æ³›åŒ–èƒ½åŠ›æµ‹è¯•ï¼ˆè®­ç»ƒé›†å¤–å¥å­ï¼‰:")
new_test_sentences = [
    "hello world",
    "good night",
    "i am happy",
    "see you later"
]

for i, sentence in enumerate(new_test_sentences):
    translation = translate_sentence(model, sentence, en_vocab, zh_vocab, device)
    print(f"{i + 1:2d}. '{sentence}' â†’ '{translation}'")

### 6.3 é«˜çº§å¯è§†åŒ–åˆ†æ

#### 6.3.1 è®­ç»ƒè¿‡ç¨‹è¯¦ç»†åˆ†æ
# åˆ›å»ºè¯¦ç»†çš„è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Seq2Seqæ¨¡å‹è®­ç»ƒè¿‡ç¨‹è¯¦ç»†åˆ†æ', fontsize=16, fontweight='bold')

# 1. è®­ç»ƒæŸå¤±æ›²çº¿ï¼ˆå¸¦ç½®ä¿¡åŒºé—´ï¼‰
ax1 = axes[0, 0]
epochs = range(1, len(train_losses) + 1)
ax1.plot(epochs, train_losses, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')
ax1.fill_between(epochs, train_losses, alpha=0.3, color='blue')
ax1.set_title('è®­ç»ƒæŸå¤±å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
ax1.set_xlabel('è®­ç»ƒè½®æ¬¡')
ax1.set_ylabel('æŸå¤±å€¼')
ax1.grid(True, alpha=0.3)
ax1.legend()

# 2. æŸå¤±ä¸‹é™é€Ÿåº¦åˆ†æ
ax2 = axes[0, 1]
if len(train_losses) > 1:
    loss_gradients = np.gradient(train_losses)
    ax2.plot(epochs, loss_gradients, 'r-', linewidth=2, label='æŸå¤±æ¢¯åº¦')
    ax2.set_title('æŸå¤±ä¸‹é™é€Ÿåº¦', fontsize=14, fontweight='bold')
    ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax2.set_ylabel('æŸå¤±æ¢¯åº¦')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)

# 3. å­¦ä¹ ç‡è¡°å‡æ•ˆæœï¼ˆæ¨¡æ‹Ÿï¼‰
ax3 = axes[0, 2]
# æ¨¡æ‹Ÿå­¦ä¹ ç‡è¡°å‡
initial_lr = 0.001
lr_decay = 0.95
lrs = [initial_lr * (lr_decay ** epoch) for epoch in range(len(train_losses))]
ax3.plot(epochs, lrs, 'g-', linewidth=2, label='å­¦ä¹ ç‡')
ax3.set_title('å­¦ä¹ ç‡è¡°å‡ç­–ç•¥', fontsize=14, fontweight='bold')
ax3.set_xlabel('è®­ç»ƒè½®æ¬¡')
ax3.set_ylabel('å­¦ä¹ ç‡')
ax3.grid(True, alpha=0.3)
ax3.legend()
ax3.set_yscale('log')

# 4. æ¨¡å‹æ”¶æ•›æ€§åˆ†æ
ax4 = axes[1, 0]
window_size = 5
if len(train_losses) >= window_size:
    # è®¡ç®—æ»‘åŠ¨çª—å£çš„æ–¹å·®ï¼Œç”¨äºåˆ¤æ–­æ”¶æ•›æ€§
    variances = []
    for i in range(window_size, len(train_losses)):
        window = train_losses[i - window_size:i]
        variances.append(np.var(window))

    ax4.plot(range(window_size, len(train_losses)), variances, 'purple', linewidth=2, label='æŸå¤±æ–¹å·®')
    ax4.set_title('æ¨¡å‹æ”¶æ•›æ€§åˆ†æ', fontsize=14, fontweight='bold')
    ax4.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax4.set_ylabel('æŸå¤±æ–¹å·®')
    ax4.grid(True, alpha=0.3)
    ax4.legend()

# 5. æ—©æœŸåœæ­¢åˆ†æ
ax5 = axes[1, 1]
# æ¨¡æ‹ŸéªŒè¯æŸå¤±ç”¨äºæ—©æœŸåœæ­¢åˆ†æ
validation_losses = [loss * (1 + 0.1 * np.random.random()) for loss in train_losses]
ax5.plot(epochs, train_losses, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')
ax5.plot(epochs, validation_losses, 'r-', linewidth=2, label='éªŒè¯æŸå¤±')
ax5.set_title('è®­ç»ƒ vs éªŒè¯æŸå¤±', fontsize=14, fontweight='bold')
ax5.set_xlabel('è®­ç»ƒè½®æ¬¡')
ax5.set_ylabel('æŸå¤±å€¼')
ax5.grid(True, alpha=0.3)
ax5.legend()

# 6. è®­ç»ƒé˜¶æ®µåˆ†æ
ax6 = axes[1, 2]
# å°†è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸åŒé˜¶æ®µ
n_epochs = len(train_losses)
phases = ['åˆå§‹é˜¶æ®µ', 'å¿«é€Ÿä¸‹é™', 'ç¼“æ…¢æ”¶æ•›', 'æœ€ç»ˆè°ƒä¼˜']
phase_colors = ['red', 'orange', 'yellow', 'green']
phase_ranges = [
    (0, n_epochs // 4),
    (n_epochs // 4, n_epochs // 2),
    (n_epochs // 2, 3 * n_epochs // 4),
    (3 * n_epochs // 4, n_epochs)
]

for i, (start, end) in enumerate(phase_ranges):
    if start < len(train_losses) and end <= len(train_losses):
        phase_losses = train_losses[start:end]
        phase_epochs = range(start + 1, end + 1)
        ax6.plot(phase_epochs, phase_losses, color=phase_colors[i],
                 linewidth=3, label=phases[i])

ax6.set_title('è®­ç»ƒé˜¶æ®µåˆ†æ', fontsize=14, fontweight='bold')
ax6.set_xlabel('è®­ç»ƒè½®æ¬¡')
ax6.set_ylabel('æŸå¤±å€¼')
ax6.grid(True, alpha=0.3)
ax6.legend()

plt.tight_layout()
plt.show()

# æ‰“å°è®­ç»ƒè¿‡ç¨‹åˆ†æç»“æœ
print("ğŸ“Š è®­ç»ƒè¿‡ç¨‹æ·±åº¦åˆ†æ:")
print("=" * 60)

print(f"\nğŸ¯ è®­ç»ƒæ•ˆæœè¯„ä¼°:")
print(f"   æ€»è½®æ¬¡: {len(train_losses)}")
print(f"   åˆå§‹æŸå¤±: {train_losses[0]:.4f}")
print(f"   æœ€ç»ˆæŸå¤±: {train_losses[-1]:.4f}")
print(f"   æŸå¤±å‡å°‘: {train_losses[0] - train_losses[-1]:.4f}")
print(f"   ç›¸å¯¹æ”¹å–„: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%")

# è®­ç»ƒç¨³å®šæ€§åˆ†æ
if len(train_losses) >= 10:
    recent_losses = train_losses[-10:]
    loss_stability = np.std(recent_losses)
    print(f"\nğŸ“ˆ è®­ç»ƒç¨³å®šæ€§:")
    print(f"   æœ€è¿‘10è½®æŸå¤±æ ‡å‡†å·®: {loss_stability:.4f}")
    if loss_stability < 0.1:
        print("   âœ… è®­ç»ƒéå¸¸ç¨³å®š")
    elif loss_stability < 0.5:
        print("   âš ï¸ è®­ç»ƒåŸºæœ¬ç¨³å®š")
    else:
        print("   âŒ è®­ç»ƒä¸ç¨³å®šï¼Œå»ºè®®è°ƒæ•´è¶…å‚æ•°")

# æ”¶æ•›é€Ÿåº¦åˆ†æ
convergence_point = None
threshold = 0.01
for i in range(1, len(train_losses)):
    if abs(train_losses[i] - train_losses[i - 1]) < threshold:
        convergence_point = i
        break

if convergence_point:
    print(f"\nâš¡ æ”¶æ•›åˆ†æ:")
    print(f"   æ¨¡å‹åœ¨ç¬¬{convergence_point}è½®åŸºæœ¬æ”¶æ•›")
    print(f"   æ”¶æ•›æ—¶æŸå¤±å€¼: {train_losses[convergence_point]:.4f}")
else:
    print(f"\nâš¡ æ”¶æ•›åˆ†æ:")
    print(f"   æ¨¡å‹åœ¨{len(train_losses)}è½®å†…æœªå®Œå…¨æ”¶æ•›")
    print(f"   å»ºè®®å¢åŠ è®­ç»ƒè½®æ¬¡æˆ–è°ƒæ•´å­¦ä¹ ç‡")

#### 6.3.2 ç¿»è¯‘è´¨é‡å¯¹æ¯”å¯è§†åŒ–

# åˆ›å»ºç¿»è¯‘è´¨é‡å¯¹æ¯”åˆ†æ
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('ç¿»è¯‘è´¨é‡æ·±åº¦åˆ†æ', fontsize=16, fontweight='bold')

# 1. ä¸åŒå¥å­é•¿åº¦çš„ç¿»è¯‘è´¨é‡
ax1 = axes[0, 0]
sentence_lengths = []
quality_scores = []

for i, (en, true_zh) in enumerate(raw_data):
    pred_zh = translate_sentence(model, en, en_vocab, zh_vocab, device)
    bleu = evaluator.calculate_bleu_score(pred_zh, true_zh)

    # è®¡ç®—è‹±æ–‡å¥å­é•¿åº¦
    length = len(en.split())
    sentence_lengths.append(length)
    quality_scores.append(bleu)

# æŒ‰é•¿åº¦åˆ†ç»„åˆ†æ
length_groups = {}
for length, score in zip(sentence_lengths, quality_scores):
    if length not in length_groups:
        length_groups[length] = []
    length_groups[length].append(score)

# è®¡ç®—æ¯ä¸ªé•¿åº¦ç»„çš„å¹³å‡è´¨é‡
avg_quality_by_length = {}
for length, scores in length_groups.items():
    avg_quality_by_length[length] = np.mean(scores)

lengths = list(avg_quality_by_length.keys())
avg_scores = list(avg_quality_by_length.values())

ax1.scatter(lengths, avg_scores, s=100, alpha=0.7, c='blue')
ax1.set_title('å¥å­é•¿åº¦ vs ç¿»è¯‘è´¨é‡', fontsize=14, fontweight='bold')
ax1.set_xlabel('å¥å­é•¿åº¦ï¼ˆè¯æ•°ï¼‰')
ax1.set_ylabel('å¹³å‡BLEUè¯„åˆ†')
ax1.grid(True, alpha=0.3)

# æ·»åŠ è¶‹åŠ¿çº¿
if len(lengths) > 1:
    z = np.polyfit(lengths, avg_scores, 1)
    p = np.poly1d(z)
    ax1.plot(lengths, p(lengths), "r--", alpha=0.8, label='è¶‹åŠ¿çº¿')
    ax1.legend()

# 2. ç¿»è¯‘è´¨é‡çƒ­åŠ›å›¾
ax2 = axes[0, 1]
# åˆ›å»ºè´¨é‡çŸ©é˜µ
quality_matrix = np.zeros((10, 10))
for i in range(min(10, len(quality_scores))):
    for j in range(min(10, len(quality_scores))):
        quality_matrix[i, j] = quality_scores[min(i * 10 + j, len(quality_scores) - 1)]

im = ax2.imshow(quality_matrix, cmap='RdYlGn', aspect='auto')
ax2.set_title('ç¿»è¯‘è´¨é‡çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
ax2.set_xlabel('å¥å­ç´¢å¼•')
ax2.set_ylabel('å¥å­ç´¢å¼•')
plt.colorbar(im, ax=ax2, label='BLEUè¯„åˆ†')

# 3. å®Œç¾ç¿»è¯‘ vs é”™è¯¯ç¿»è¯‘å¯¹æ¯”
ax3 = axes[1, 0]
perfect_translations = sum(1 for score in quality_scores if score >= 0.9)
good_translations = sum(1 for score in quality_scores if 0.7 <= score < 0.9)
fair_translations = sum(1 for score in quality_scores if 0.5 <= score < 0.7)
poor_translations = sum(1 for score in quality_scores if score < 0.5)

categories = ['å®Œç¾\n(â‰¥0.9)', 'è‰¯å¥½\n(0.7-0.9)', 'ä¸€èˆ¬\n(0.5-0.7)', 'è¾ƒå·®\n(<0.5)']
counts = [perfect_translations, good_translations, fair_translations, poor_translations]
colors = ['green', 'yellowgreen', 'orange', 'red']

bars = ax3.bar(categories, counts, color=colors, alpha=0.7)
ax3.set_title('ç¿»è¯‘è´¨é‡åˆ†çº§ç»Ÿè®¡', fontsize=14, fontweight='bold')
ax3.set_ylabel('å¥å­æ•°é‡')

# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, count in zip(bars, counts):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width() / 2., height + 0.1,
             f'{count}\n({count / len(quality_scores) * 100:.1f}%)',
             ha='center', va='bottom')

# 4. è¯æ±‡è¦†ç›–ç‡åˆ†æ
ax4 = axes[1, 1]
# åˆ†æé¢„æµ‹è¯æ±‡å’ŒçœŸå®è¯æ±‡çš„è¦†ç›–æƒ…å†µ
pred_vocab_coverage = []
true_vocab_coverage = []

for i, (en, true_zh) in enumerate(raw_data):
    pred_zh = translate_sentence(model, en, en_vocab, zh_vocab, device)

    # è®¡ç®—é¢„æµ‹å’ŒçœŸå®ä¸­æ–‡çš„è¯æ±‡è¦†ç›–ç‡
    pred_chars = set(pred_zh)
    true_chars = set(true_zh)

    if len(true_chars) > 0:
        coverage = len(pred_chars.intersection(true_chars)) / len(true_chars)
        pred_vocab_coverage.append(coverage)
        true_vocab_coverage.append(len(true_chars))

ax4.scatter(true_vocab_coverage, pred_vocab_coverage, alpha=0.6, c='purple')
ax4.set_title('è¯æ±‡è¦†ç›–ç‡åˆ†æ', fontsize=14, fontweight='bold')
ax4.set_xlabel('çœŸå®å¥å­è¯æ±‡æ•°')
ax4.set_ylabel('è¯æ±‡è¦†ç›–ç‡')
ax4.grid(True, alpha=0.3)

# æ·»åŠ ç†æƒ³çº¿
max_vocab = max(true_vocab_coverage) if true_vocab_coverage else 1
ax4.plot([0, max_vocab], [1, 1], 'r--', alpha=0.8, label='ç†æƒ³è¦†ç›–ç‡')
ax4.legend()

plt.tight_layout()
plt.show()

# æ‰“å°è¯¦ç»†çš„ç¿»è¯‘è´¨é‡åˆ†æ
print("ğŸ“Š ç¿»è¯‘è´¨é‡æ·±åº¦åˆ†æ:")
print("=" * 60)

print(f"\nğŸ¯ é•¿åº¦æ•ˆåº”åˆ†æ:")
if len(length_groups) > 1:
    short_sentences = [scores for length, scores in length_groups.items() if length <= 3]
    long_sentences = [scores for length, scores in length_groups.items() if length > 3]

    if short_sentences and long_sentences:
        short_avg = np.mean([score for sublist in short_sentences for score in sublist])
        long_avg = np.mean([score for sublist in long_sentences for score in sublist])
        print(f"   çŸ­å¥å¹³å‡è´¨é‡ (â‰¤3è¯): {short_avg:.3f}")
        print(f"   é•¿å¥å¹³å‡è´¨é‡ (>3è¯): {long_avg:.3f}")
        print(f"   é•¿åº¦å½±å“: {short_avg - long_avg:.3f}")

        if short_avg > long_avg:
            print("   âœ… çŸ­å¥ç¿»è¯‘è´¨é‡æ›´å¥½")
        else:
            print("   âš ï¸ é•¿å¥ç¿»è¯‘è´¨é‡æ›´å¥½ï¼ˆæ„å¤–ï¼‰")

print(f"\nğŸ† è´¨é‡åˆ†çº§è¯¦æƒ…:")
total_sentences = len(quality_scores)
print(f"   æ€»å¥å­æ•°: {total_sentences}")
print(f"   å®Œç¾ç¿»è¯‘: {perfect_translations}å¥ ({perfect_translations / total_sentences * 100:.1f}%)")
print(f"   è‰¯å¥½ç¿»è¯‘: {good_translations}å¥ ({good_translations / total_sentences * 100:.1f}%)")
print(f"   ä¸€èˆ¬ç¿»è¯‘: {fair_translations}å¥ ({fair_translations / total_sentences * 100:.1f}%)")
print(f"   è¾ƒå·®ç¿»è¯‘: {poor_translations}å¥ ({poor_translations / total_sentences * 100:.1f}%)")

# è¯æ±‡è¦†ç›–ç‡ç»Ÿè®¡
if pred_vocab_coverage:
    avg_coverage = np.mean(pred_vocab_coverage)
    print(f"\nğŸ“ è¯æ±‡è¦†ç›–ç‡:")
    print(f"   å¹³å‡è¦†ç›–ç‡: {avg_coverage:.3f}")
    print(f"   æœ€é«˜è¦†ç›–ç‡: {max(pred_vocab_coverage):.3f}")
    print(f"   æœ€ä½è¦†ç›–ç‡: {min(pred_vocab_coverage):.3f}")

    if avg_coverage > 0.8:
        print("   âœ… è¯æ±‡è¦†ç›–ç‡å¾ˆå¥½")
    elif avg_coverage > 0.6:
        print("   âš ï¸ è¯æ±‡è¦†ç›–ç‡ä¸­ç­‰")
    else:
        print("   âŒ è¯æ±‡è¦†ç›–ç‡è¾ƒå·®")



# å¯è§†åŒ–è¯„ä¼°ç»“æœ
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# å­å›¾1: BLEUè¯„åˆ†åˆ†å¸ƒç›´æ–¹å›¾
axes[0, 0].hist(all_bleu_scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
axes[0, 0].set_title('BLEUè¯„åˆ†åˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('BLEUè¯„åˆ†', fontsize=12)
axes[0, 0].set_ylabel('å¥å­æ•°é‡', fontsize=12)
axes[0, 0].axvline(avg_bleu, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡å€¼: {avg_bleu:.3f}')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# å­å›¾2: è¯æ±‡é‡å ç‡åˆ†å¸ƒç›´æ–¹å›¾
axes[0, 1].hist(all_word_overlaps, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')
axes[0, 1].set_title('è¯æ±‡é‡å ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('è¯æ±‡é‡å ç‡', fontsize=12)
axes[0, 1].set_ylabel('å¥å­æ•°é‡', fontsize=12)
axes[0, 1].axvline(avg_word_overlap, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡å€¼: {avg_word_overlap:.3f}')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# å­å›¾3: BLEU vs è¯æ±‡é‡å ç‡æ•£ç‚¹å›¾
axes[1, 0].scatter(all_bleu_scores, all_word_overlaps, alpha=0.6, c='purple')
axes[1, 0].set_title('BLEUè¯„åˆ† vs è¯æ±‡é‡å ç‡', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('BLEUè¯„åˆ†', fontsize=12)
axes[1, 0].set_ylabel('è¯æ±‡é‡å ç‡', fontsize=12)
axes[1, 0].grid(True, alpha=0.3)

# æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
x = np.array(all_bleu_scores)
y = np.array(all_word_overlaps)
valid_indices = ~(np.isnan(x) | np.isnan(y) | np.isinf(x) | np.isinf(y))
x = x[valid_indices]
y = y[valid_indices]

# æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ•°æ®ç‚¹ä¸”æ•°æ®ä¸æ˜¯å¸¸æ•°
if len(x) > 2 and not np.allclose(x, x[0]) and not np.allclose(y, y[0]):
    try:
        # æ·»åŠ æ‹Ÿåˆçº¿
        z = np.polyfit(x, y, 1)
        p = np.poly1d(z)
        x_sorted = np.linspace(min(x), max(x), 100)  # ä½¿ç”¨linspaceç”Ÿæˆå¹³æ»‘çš„xå€¼
        axes[1, 0].plot(x_sorted, p(x_sorted), "r--", alpha=0.8, label='æ‹Ÿåˆçº¿')
    except np.linalg.LinAlgError:
        print("âš ï¸ æ— æ³•è®¡ç®—æ‹Ÿåˆçº¿ï¼Œæ•°æ®å¯èƒ½ä¸é€‚åˆçº¿æ€§æ‹Ÿåˆ")
else:
    print("âš ï¸ æ•°æ®ç‚¹ä¸è¶³æˆ–åˆ†å¸ƒä¸é€‚åˆè¿›è¡Œçº¿æ€§æ‹Ÿåˆ")

axes[1, 0].legend()

# å­å›¾4: ä¸åŒè¯„åˆ†æ®µçš„é¥¼å›¾
bleu_distribution = []
labels = []
colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']

for min_score, max_score, label in bleu_ranges:
    count = sum(1 for score in all_bleu_scores if min_score <= score < max_score)
    if count > 0:  # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„åˆ†æ®µ
        bleu_distribution.append(count)
        labels.append(f'{label}\n({count}æ¡)')

axes[1, 1].pie(bleu_distribution, labels=labels, autopct='%1.1f%%',
               colors=colors[:len(bleu_distribution)], startangle=90)
axes[1, 1].set_title('BLEUè¯„åˆ†è´¨é‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# æ‰“å°è¯¦ç»†ç»Ÿè®¡åˆ†æ
print("ğŸ“Š è¯¦ç»†ç»Ÿè®¡åˆ†æ:")
print("=" * 50)

print(f"\nğŸ¯ è¯„åˆ†ç»Ÿè®¡:")
print(f"   BLEUè¯„åˆ†: æœ€é«˜ {max(all_bleu_scores):.3f}, æœ€ä½ {min(all_bleu_scores):.3f}, æ ‡å‡†å·® {np.std(all_bleu_scores):.3f}")
print(f"   è¯æ±‡é‡å ç‡: æœ€é«˜ {max(all_word_overlaps):.3f}, æœ€ä½ {min(all_word_overlaps):.3f}, æ ‡å‡†å·® {np.std(all_word_overlaps):.3f}")

# è®¡ç®—ç›¸å…³æ€§
correlation = np.corrcoef(all_bleu_scores, all_word_overlaps)[0, 1]
print(f"\nğŸ”— BLEUä¸è¯æ±‡é‡å ç‡ç›¸å…³æ€§: {correlation:.3f}")

if correlation > 0.7:
    print("   âœ… å¼ºæ­£ç›¸å…³ - ä¸¤ä¸ªæŒ‡æ ‡åŸºæœ¬ä¸€è‡´")
elif correlation > 0.3:
    print("   âš ï¸ ä¸­ç­‰ç›¸å…³ - ä¸¤ä¸ªæŒ‡æ ‡æœ‰ä¸€å®šå…³è”")
else:
    print("   âŒ å¼±ç›¸å…³ - ä¸¤ä¸ªæŒ‡æ ‡è¡¡é‡ä¸åŒæ–¹é¢")

# æ¨¡å‹æ€§èƒ½ç­‰çº§è¯„å®š
if avg_bleu >= 0.7:
    performance_level = "ä¼˜ç§€"
    performance_emoji = "ğŸ†"
elif avg_bleu >= 0.5:
    performance_level = "è‰¯å¥½"
    performance_emoji = "ğŸ‘"
elif avg_bleu >= 0.3:
    performance_level = "ä¸­ç­‰"
    performance_emoji = "ğŸ†—"
else:
    performance_level = "éœ€è¦æ”¹è¿›"
    performance_emoji = "âš ï¸"

print(f"\n{performance_emoji} æ¨¡å‹æ•´ä½“æ€§èƒ½ç­‰çº§: {performance_level}")
print(f"   åŸºäºå¹³å‡BLEUè¯„åˆ† {avg_bleu:.3f} çš„è¯„å®š")
