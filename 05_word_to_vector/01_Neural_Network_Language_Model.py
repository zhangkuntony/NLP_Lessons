# å¯¼å…¥å¿…è¦çš„åº“
import os
import glob
import chardet  # ç”¨äºæ£€æµ‹æ–‡ä»¶ç¼–ç 
import jieba    # ä¸­æ–‡åˆ†è¯
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']  # è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False

print("ğŸš€ æ‰€æœ‰å¿…è¦çš„åº“éƒ½å·²æˆåŠŸå¯¼å…¥ï¼")
print("æ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹å¤„ç†æ•°æ®...")

# å®šä¹‰æ•°æ®è·¯å¾„
data_path = "CN_Corpus/SogouC.reduced/Reduced/"
print(f"ğŸ“ æ•°æ®è·¯å¾„: {data_path}")

# æ£€æŸ¥æ•°æ®ç›®å½•
if os.path.exists(data_path):
    categories = os.listdir(data_path)
    print(f"ğŸ“Š æ‰¾åˆ° {len(categories)} ä¸ªæ•°æ®ç±»åˆ«: {categories}")
else:
    print("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")


# è§£å†³ä¸­æ–‡ç¼–ç é—®é¢˜
def detect_and_read_file(file_path, max_detect_bytes=10000):
    """
    æ™ºèƒ½æ£€æµ‹æ–‡ä»¶ç¼–ç å¹¶æ­£ç¡®è¯»å–æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        max_detect_bytes: ç”¨äºæ£€æµ‹ç¼–ç çš„æœ€å¤§å­—èŠ‚æ•°

    Returns:
        æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²ï¼Œå¦‚æœè¯»å–å¤±è´¥è¿”å›None
    """
    try:
        # 1. è¯»å–éƒ¨åˆ†æ–‡ä»¶å†…å®¹æ¥æ£€æµ‹ç¼–ç 
        with open(file_path, "rb") as f:
            raw_data = f.read(max_detect_bytes)

        # 2. ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
        encoding_result = chardet.detect(raw_data)
        detected_encoding = encoding_result["encoding"]
        confidence = encoding_result["confidence"]

        print(f"ğŸ” æ£€æµ‹åˆ°ç¼–ç : {detected_encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")

        # 3. å°è¯•ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è¯»å–æ–‡ä»¶
        try:
            with open(file_path, "r", encoding=detected_encoding) as f:
                content = f.read()
            return content
        except UnicodeDecodeError:
            print(f"âš ï¸  ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} è¯»å–å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")

        # 4. å¦‚æœæ£€æµ‹çš„ç¼–ç å¤±è´¥ï¼Œå°è¯•å¸¸è§çš„ä¸­æ–‡ç¼–ç 
        common_encodings = ['gbk', 'gb2312', 'utf-8', 'utf-16', 'big5']
        for encoding in common_encodings:
            try:
                with open(file_path, "r", encoding=encoding) as f:
                    content = f.read()
                print(f"âœ… æˆåŠŸç”¨ç¼–ç  {encoding} è¯»å–æ–‡ä»¶")
                return content

            except (UnicodeDecodeError, UnicodeError):
                continue

        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}")
        return None

    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

# æµ‹è¯•ç¼–ç æ£€æµ‹åŠŸèƒ½
print("ğŸ§ª æµ‹è¯•ç¼–ç æ£€æµ‹åŠŸèƒ½...")
test_file = os.path.join(data_path, "C000008", "10.txt")
if os.path.exists(test_file):
    content = detect_and_read_file(test_file)
    if content:
        print(f"ğŸ“ æ–‡ä»¶å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
        print(content[:200])
        print(f"ğŸ“Š æ–‡ä»¶æ€»é•¿åº¦: {len(content)} å­—ç¬¦")
    else:
        print("âŒ æ— æ³•è¯»å–æµ‹è¯•æ–‡ä»¶")
else:
    print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")


# æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯
import re
import string

class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""

    def __init__(self):
        # å°†è‹±æ–‡æ ‡ç‚¹å’Œä¸­æ–‡æ ‡ç‚¹è¿›è¡Œå­—ç¬¦ä¸²æ‹¼æ¥ï¼ˆstring.punctuationä¸ºè‹±æ–‡æ ‡ç‚¹ï¼‰
        self.punctuation = set(string.punctuation + 'ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€â€”â€¦')

    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ã€å¤šä½™ç©ºæ ¼ç­‰"""
        # 1. å»é™¤ç½‘å€ã€é‚®ç®±ç­‰
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        text = re.sub(r'\S+@\S+', '', text)

        # 2. å»é™¤æ•°å­—å’Œè‹±æ–‡ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
        text = re.sub(r'[a-zA-Z0-9]', '', text)

        # 3. å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', '', text)

        return text.strip()

    def segment_text(self, text):
        """ä¸­æ–‡åˆ†è¯"""
        # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
        words = jieba.cut(text)

        # è¿‡æ»¤æ‰å•å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
        filtered_words = []
        for word in words:
            word = word.strip()
            if len(word) > 1 and word not in self.punctuation:
                filtered_words.append(word)

        return filtered_words

    def process_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        content = detect_and_read_file(file_path)
        if content is None:
            return []

        # æ¸…ç†æ–‡æœ¬
        clean_content = self.clean_text(content)

        # åˆ†è¯
        words = self.segment_text(clean_content)

        return words

# åˆ›å»ºé¢„å¤„ç†å™¨
preprocessor = TextPreprocessor()

# æµ‹è¯•é¢„å¤„ç†åŠŸèƒ½
print("ğŸ§ª æµ‹è¯•æ–‡æœ¬é¢„å¤„ç†åŠŸèƒ½...")
test_text = "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼æˆ‘ä»¬å»å…¬å›­ç©å§ã€‚ç½‘å€ï¼šhttp://example.com é‚®ç®±ï¼štest@email.com"
print(f"åŸæ–‡: {test_text}")

clean_text = preprocessor.clean_text(test_text)
print(f"æ¸…ç†åï¼š{clean_text}")

words = preprocessor.segment_text(clean_text)
print(f"åˆ†è¯ç»“æœï¼š{words}")

# å¤„ç†ä¸€ä¸ªå®é™…æ–‡ä»¶
if os.path.exists(test_file):
    print(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {test_file}")
    processed_words = preprocessor.process_file(test_file)
    print(f"åˆ†è¯æ•°é‡ï¼š{len(processed_words)}")
    print(f"å‰20ä¸ªè¯ï¼š{processed_words[:20]}")
else:
    print("âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
    # åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿçš„ä¸­æ–‡æ–‡æœ¬ç”¨äºæµ‹è¯•
    sample_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚æ˜å¤©å¯èƒ½ä¼šä¸‹é›¨ã€‚æˆ‘å–œæ¬¢åœ¨æ™´å¤©çš„æ—¶å€™å»å…¬å›­æ•£æ­¥ã€‚"
    words = preprocessor.segment_text(sample_text)
    print(f"æ¨¡æ‹Ÿæ•°æ®åˆ†è¯ç»“æœ: {words}")


# æ„å»ºè¯æ±‡è¡¨å’Œæ•°æ®åŠ è½½å™¨
class Vocabulary:
    """è¯æ±‡è¡¨ç±»"""

    def __init__(self, min_freq=2):
        """
        Args:
            min_freq: è¯æ±‡æœ€å°‘å‡ºç°æ¬¡æ•°ï¼Œä½äºæ­¤é¢‘ç‡çš„è¯ä¼šè¢«æ ‡è®°ä¸ºæœªçŸ¥è¯
        """
        self.min_freq = min_freq
        self.word2idx = {}              # è¯ -> ç´¢å¼•
        self.idx2word = {}              # ç´¢å¼• -> è¯
        self.word_counts = Counter()    # è¯é¢‘ç»Ÿè®¡

        # ç‰¹æ®Šæ ‡è®°
        self.UNK_TOKEN = '<UNK>'        # æœªçŸ¥è¯
        self.PAD_TOKEN = '<PAD>'        # å¡«å……è¯ï¼ˆç”¨äºbatch paddingï¼‰

        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        self._add_word(self.PAD_TOKEN)  # ç´¢å¼• 0
        self._add_word(self.UNK_TOKEN)  # ç´¢å¼• 1

    def _add_word(self, word):
        """æ·»åŠ è¯åˆ°è¯æ±‡è¡¨"""
        if word not in self.word2idx:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word

    def build_vocab(self, word_lists):
        """
        ä»è¯åˆ—è¡¨æ„å»ºè¯æ±‡è¡¨

        Args:
            word_lists: è¯çš„åˆ—è¡¨çš„åˆ—è¡¨ï¼Œå¦‚ [['ä»Šå¤©', 'å¤©æ°”'], ['æ˜å¤©', 'ä¸‹é›¨']]
        """
        print("ğŸ“Š ç»Ÿè®¡è¯é¢‘...")

        # ç»Ÿè®¡æ‰€æœ‰è¯çš„é¢‘ç‡
        for words in word_lists:
            self.word_counts.update(words)

        print(f"ğŸ“ˆ æ€»å…±å‘ç° {len(self.word_counts)} ä¸ªå”¯ä¸€è¯æ±‡")
        print(f"ğŸ“‹ æœ€å¸¸è§çš„10ä¸ªè¯: {self.word_counts.most_common(10)}")

        # æ·»åŠ é¢‘ç‡é«˜äºé˜ˆå€¼çš„è¯
        added_count = 0
        for word, count in self.word_counts.items():
            if count >= self.min_freq:
                self._add_word(word)
                added_count += 1

        print(f"âœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {len(self.word2idx)} (å…¶ä¸­ {added_count} ä¸ªå¸¸ç”¨è¯)")
        print(f"ğŸ—‘ï¸  è¿‡æ»¤æ‰ {len(self.word_counts) - added_count} ä¸ªä½é¢‘è¯")

    def word_to_idx(self, word):
        """å°†è¯è½¬æ¢ä¸ºç´¢å¼•"""
        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])

    def idx_to_word(self, idx):
        """å°†ç´¢å¼•è½¬æ¢ä¸ºè¯"""
        return self.idx2word.get(idx, self.UNK_TOKEN)

    def words_to_indices(self, words):
        """å°†æ­¤åˆ—è¡¨è½¬æ¢ä¸ºç´¢å¼•åˆ—è¡¨"""
        return [self.word_to_idx(word) for word in words]

    def indices_to_words(self, indices):
        """å°†ç´¢å¼•åˆ—è¡¨è½¬æ¢ä¸ºè¯åˆ—è¡¨"""
        return [self.idx_to_word(idx) for idx in indices]

    def __len__(self):
        return len(self.word2idx)

# åˆ›å»ºä¸€äº›ç¤ºä¾‹æ•°æ®æ¥æµ‹è¯•è¯æ±‡è¡¨
print("ğŸ§ª åˆ›å»ºæµ‹è¯•æ•°æ®...")
sample_word_lists = [
    ['ä»Šå¤©', 'å¤©æ°”', 'å¾ˆ', 'å¥½'],
    ['æ˜å¤©', 'å¯èƒ½', 'ä¼š', 'ä¸‹é›¨'],
    ['æˆ‘', 'å–œæ¬¢', 'æ™´å¤©', 'çš„', 'æ—¶å€™'],
    ['ä»Šå¤©', 'å¤©æ°”', 'çœŸ', 'å¥½'],  # é‡å¤çš„è¯ç”¨æ¥æµ‹è¯•è¯é¢‘
    ['å¤©æ°”', 'é¢„æŠ¥', 'è¯´', 'æ˜å¤©', 'æ™´å¤©']
]

# æ„å»ºè¯æ±‡è¡¨
vocab = Vocabulary(min_freq=1)          # è®¾ç½®æœ€å°é¢‘ç‡ä¸º1ï¼Œ è¿™æ ·æ‰€æœ‰è¯éƒ½ä¼šè¢«ä¿ç•™
vocab.build_vocab(sample_word_lists)

# æµ‹è¯•è¯æ±‡è¡¨åŠŸèƒ½
print(f"\nğŸ§ª æµ‹è¯•è¯æ±‡è¡¨åŠŸèƒ½:")
test_words = ['ä»Šå¤©', 'å¤©æ°”', 'æœªçŸ¥è¯æ±‡']
for word in test_words:
    idx = vocab.word_to_idx(word)
    back_word = vocab.idx_to_word(idx)
    print(f"  '{word}' â†’ {idx} â†’ '{back_word}'")

# æµ‹è¯•å¥å­è½¬æ¢
test_sentence = ['ä»Šå¤©', 'å¤©æ°”', 'å¾ˆ', 'å¥½']
indices = vocab.words_to_indices(test_sentence)
back_words = vocab.indices_to_words(indices)
print(f"\nğŸ“ å¥å­è½¬æ¢æµ‹è¯•:")
print(f"  åŸå¥å­: {test_sentence}")
print(f"  ç´¢å¼•åºåˆ—: {indices}")
print(f"  è¿˜åŸå¥å­: {back_words}")


# NNLMæ•°æ®é›†ç±»
class NNLMDataset:
    """NNLMæ•°æ®é›†ç±»"""

    def __init__(self, word_lists, vocab, context_size=3):
        """
        Args:
            word_lists: è¯çš„åˆ—è¡¨çš„åˆ—è¡¨
            vocab: è¯æ±‡è¡¨å¯¹è±¡
            context_size: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆç”¨å‰å‡ ä¸ªè¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰
        """
        self.vocab = vocab
        self.context_size = context_size
        self.data = []

        print(f"ğŸ”¨ æ„å»ºè®­ç»ƒæ•°æ®ï¼Œä¸Šä¸‹æ–‡çª—å£å¤§å°: {context_size}")
        self._build_data(word_lists)

    def _build_data(self, word_lists):
        """æ„å»ºè®­ç»ƒæ•°æ®å¯¹"""
        for words in word_lists:
            # å°†è¯è½¬æ¢ä¸ºç´¢å¼•
            indices = self.vocab.words_to_indices(words)

            # æ„å»ºä¸Šä¸‹æ–‡-ç›®æ ‡å¯¹
            for i in range(len(indices) - self.context_size):
                context = indices[i:i + self.context_size]          # å‰nä¸ªè¯
                target = indices[i + self.context_size]             # ä¸‹ä¸€ä¸ªè¯
                self.data.append((context, target))

        print(f"âœ… æ•°æ®æ„å»ºå®Œæˆï¼æ€»å…± {len(self.data)} ä¸ªè®­ç»ƒæ ·æœ¬")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

    def get_batch(self, batch_size=32, shuffle=True):
        """è·å–æ‰¹æ¬¡æ•°æ®"""
        if shuffle:
            indices = np.random.choice(len(self.data), size=min(batch_size, len(self.data)), replace=False)
        else:
            indices = list(range(min(batch_size, len(self.data))))

        contexts = []
        targets = []

        for idx in indices:
            context, target = self.data[idx]
            contexts.append(context)
            targets.append(target)

        return torch.tensor(contexts), torch.tensor(targets)

# åˆ›å»ºæ•°æ®é›†
print("ğŸ“¦ åˆ›å»ºNNLMæ•°æ®é›†...")
dataset = NNLMDataset(sample_word_lists, vocab, context_size=3)

# æŸ¥çœ‹ä¸€äº›è®­ç»ƒæ ·æœ¬
print(f"\nğŸ“‹ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
# for i in range(min(5, len(dataset))):
for i in range(len(dataset)):
    context, target = dataset[i]
    context_words = vocab.indices_to_words(context)
    target_word = vocab.idx_to_word(target)
    print(f"  æ ·æœ¬ {i+1}: {context_words} â†’ {target_word}")

# æµ‹è¯•æ‰¹æ¬¡æ•°æ®è·å–
print(f"\nğŸ² æµ‹è¯•æ‰¹æ¬¡æ•°æ®è·å–:")
batch_contexts, batch_targets = dataset.get_batch(batch_size=3)
print(f"  æ‰¹æ¬¡ä¸Šä¸‹æ–‡å½¢çŠ¶: {batch_contexts.shape}")
print(f"  æ‰¹æ¬¡ç›®æ ‡å½¢çŠ¶: {batch_targets.shape}")
print(f"  æ‰¹æ¬¡ä¸Šä¸‹æ–‡å†…å®¹: {batch_contexts}")
print(f"  æ‰¹æ¬¡ç›®æ ‡å†…å®¹: {batch_targets}")

# è½¬æ¢å›è¯æ±‡æŸ¥çœ‹
print(f"\nğŸ“ æ‰¹æ¬¡å†…å®¹ï¼ˆè¯æ±‡å½¢å¼ï¼‰:")
for i in range(len(batch_contexts)):
    context_words = vocab.indices_to_words(batch_contexts[i].tolist())
    target_word = vocab.idx_to_word(batch_targets[i].item())
    print(f"  æ‰¹æ¬¡æ ·æœ¬ {i + 1}: {context_words} â†’ {target_word}")


# NNLM æ¨¡å‹å®ç°
class NNLM(nn.Module):
    """ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹"""

    def __init__(self, vocab_size, context_size, embedding_dim=50, hidden_dim=128):
        """
        Args:
            vocab_size: è¯æ±‡è¡¨å¤§å°
            context_size: ä¸Šä¸‹æ–‡çª—å£å¤§å°
            embedding_dim: è¯åµŒå…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(NNLM, self).__init__()

        self.vocab_size = vocab_size
        self.context_size = context_size
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # ğŸ§± ç»„ä»¶1ï¼šè¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # ğŸ§± ç»„ä»¶2ï¼šéšè—å±‚
        # è¾“å…¥ç»´åº¦ = context_size * embedding_dim (æ‹¼æ¥åçš„å‘é‡é•¿åº¦)
        self.hidden = nn.Linear(context_size * embedding_dim, hidden_dim)

        # ğŸ§± ç»„ä»¶3ï¼šè¾“å‡ºå±‚
        self.output = nn.Linear(hidden_dim, vocab_size)

        # æ¿€æ´»å‡½æ•°
        self.relu = nn.ReLU()
        self.softmax = nn.LogSoftmax(dim=-1)            # ä½¿ç”¨LogSoftmaxä¸NLLLossé…åˆ

        # åˆå§‹åŒ–å‚æ•°
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        # åµŒå…¥å±‚åˆå§‹åŒ–
        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)

        # çº¿æ€§å±‚åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.hidden.weight)
        nn.init.zeros_(self.hidden.bias)
        nn.init.xavier_uniform_(self.output.weight)
        nn.init.zeros_(self.output.bias)

    def forward(self, context):
        """
        å‰å‘ä¼ æ’­

        Args:
            context: ä¸Šä¸‹æ–‡è¯ç´¢å¼•ï¼Œå½¢çŠ¶ [batch_size, context_size]

        Returns:
            è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œå½¢çŠ¶ [batch_size, vocab_size]
        """
        batch_size = context.size(0)

        # æ­¥éª¤1ï¼šè¯åµŒå…¥
        # context: [batch_size, context_size] -> [batch_size, context_size, embedding_dim]
        embedded = self.embedding(context)

        # æ­¥éª¤2ï¼šæ‹¼æ¥
        # [batch_size, context_size, embedding_dim] -> [batch_size, context_size * embedding_dim]
        concatenated = embedded.view(batch_size, -1)

        # æ­¥éª¤3ï¼šéšè—å±‚
        # [batch_size, context_size * embedding_dim] -> [batch_size, hidden_dim]
        hidden_out = self.relu(self.hidden(concatenated))

        # æ­¥éª¤4ï¼šè¾“å‡ºå±‚
        # [batch_size, hidden_dim] -> [batch_size, vocab_size]
        output = self.output(hidden_out)

        # æ­¥éª¤5ï¼šæ¦‚ç‡åˆ†å¸ƒ
        log_probs = self.softmax(output)

        return log_probs

    def predict_next_word(self, context_words, vocab, top_k=5):
        """
        é¢„æµ‹ä¸‹ä¸€ä¸ªè¯

        Args:
            context_words: ä¸Šä¸‹æ–‡è¯åˆ—è¡¨
            vocab: è¯æ±‡è¡¨å¯¹è±¡
            top_k: è¿”å›æ¦‚ç‡æœ€é«˜çš„å‰kä¸ªè¯

        Returns:
            [(word, probability), ...] æŒ‰æ¦‚ç‡é™åºæ’åˆ—
        """
        self.eval()             # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        with torch.no_grad():
            # è½¬æ¢ä¸ºç´¢å¼•
            context_indices = vocab.words_to_indices(context_words)

            # ç¡®ä¿ä¸Šä¸‹æ–‡é•¿åº¦æ­£ç¡®
            if len(context_indices) != self.context_size:
                raise ValueError(f"ä¸Šä¸‹æ–‡é•¿åº¦åº”ä¸º {self.context_size}, ä½†å¾—åˆ°{len(context_indices)}")

            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
            context_tensor = torch.tensor([context_indices])

            # å‰å‘ä¼ æ’­
            log_probs = self.forward(context_tensor)
            probs = torch.exp(log_probs)            # ä»logæ¦‚ç‡è½¬æ¢ä¸ºæ¦‚ç‡

            # è·å–top_k
            top_probs, top_indices = torch.topk(probs[0], k=top_k)

            # è½¬æ¢å›è¯æ±‡
            predictions = []
            for prob, idx in zip(top_probs, top_indices):
                word = vocab.idx_to_word(idx.item())
                predictions.append((word, prob.item()))

            return predictions

# åˆ›å»ºæ¨¡å‹
print("ğŸ—ï¸  åˆ›å»ºNNLMæ¨¡å‹...")
model = NNLM(
    vocab_size=len(vocab),
    context_size=3,
    embedding_dim=20,       # è¾ƒå°çš„ç»´åº¦ç”¨äºæ¼”ç¤º
    hidden_dim=50
)

print(f"ğŸ“Š æ¨¡å‹å‚æ•°:")
print(f"  è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
print(f"  ä¸Šä¸‹æ–‡çª—å£: 3")
print(f"  åµŒå…¥ç»´åº¦: 20")
print(f"  éšè—å±‚ç»´åº¦: 50")

# ç»Ÿè®¡å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
print(f"  å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")

# æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
test_context = torch.tensor([[2, 5, 8]])        # æ‰¹æ¬¡å¤§å°ä¸º1çš„æµ‹è¯•è¾“å…¥ (æ³¨æ„æ·»åŠ äº†æ–¹æ‹¬å·)
test_output = model(test_context)
print(f"  è¾“å…¥å½¢çŠ¶: {test_context.shape}")
print(f"  è¾“å‡ºå½¢çŠ¶: {test_output.shape}")
print(f"  è¾“å‡ºæ¦‚ç‡å’Œ: {torch.exp(test_output).sum().item():.6f} (åº”è¯¥æ¥è¿‘1.0)")