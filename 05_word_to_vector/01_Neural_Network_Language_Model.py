# å¯¼å…¥å¿…è¦çš„åº“
import os
import glob
import chardet  # ç”¨äºæ£€æµ‹æ–‡ä»¶ç¼–ç 
import jieba    # ä¸­æ–‡åˆ†è¯
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']  # è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False

print("ğŸš€ æ‰€æœ‰å¿…è¦çš„åº“éƒ½å·²æˆåŠŸå¯¼å…¥ï¼")
print("æ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹å¤„ç†æ•°æ®...")

# å®šä¹‰æ•°æ®è·¯å¾„
data_path = "CN_Corpus/SogouC.reduced/Reduced/"
print(f"ğŸ“ æ•°æ®è·¯å¾„: {data_path}")

# æ£€æŸ¥æ•°æ®ç›®å½•
if os.path.exists(data_path):
    categories = os.listdir(data_path)
    print(f"ğŸ“Š æ‰¾åˆ° {len(categories)} ä¸ªæ•°æ®ç±»åˆ«: {categories}")
else:
    print("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")


# è§£å†³ä¸­æ–‡ç¼–ç é—®é¢˜
def detect_and_read_file(file_path, max_detect_bytes=10000):
    """
    æ™ºèƒ½æ£€æµ‹æ–‡ä»¶ç¼–ç å¹¶æ­£ç¡®è¯»å–æ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        max_detect_bytes: ç”¨äºæ£€æµ‹ç¼–ç çš„æœ€å¤§å­—èŠ‚æ•°

    Returns:
        æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²ï¼Œå¦‚æœè¯»å–å¤±è´¥è¿”å›None
    """
    try:
        # 1. è¯»å–éƒ¨åˆ†æ–‡ä»¶å†…å®¹æ¥æ£€æµ‹ç¼–ç 
        with open(file_path, "rb") as f:
            raw_data = f.read(max_detect_bytes)

        # 2. ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
        encoding_result = chardet.detect(raw_data)
        detected_encoding = encoding_result["encoding"]
        confidence = encoding_result["confidence"]

        print(f"ğŸ” æ£€æµ‹åˆ°ç¼–ç : {detected_encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")

        # 3. å°è¯•ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è¯»å–æ–‡ä»¶
        try:
            with open(file_path, "r", encoding=detected_encoding) as f:
                content = f.read()
            return content
        except UnicodeDecodeError:
            print(f"âš ï¸  ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {detected_encoding} è¯»å–å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")

        # 4. å¦‚æœæ£€æµ‹çš„ç¼–ç å¤±è´¥ï¼Œå°è¯•å¸¸è§çš„ä¸­æ–‡ç¼–ç 
        common_encodings = ['gbk', 'gb2312', 'utf-8', 'utf-16', 'big5']
        for encoding in common_encodings:
            try:
                with open(file_path, "r", encoding=encoding) as f:
                    content = f.read()
                print(f"âœ… æˆåŠŸç”¨ç¼–ç  {encoding} è¯»å–æ–‡ä»¶")
                return content

            except (UnicodeDecodeError, UnicodeError):
                continue

        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}")
        return None

    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

# æµ‹è¯•ç¼–ç æ£€æµ‹åŠŸèƒ½
print("ğŸ§ª æµ‹è¯•ç¼–ç æ£€æµ‹åŠŸèƒ½...")
test_file = os.path.join(data_path, "C000008", "10.txt")
if os.path.exists(test_file):
    content = detect_and_read_file(test_file)
    if content:
        print(f"ğŸ“ æ–‡ä»¶å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
        print(content[:200])
        print(f"ğŸ“Š æ–‡ä»¶æ€»é•¿åº¦: {len(content)} å­—ç¬¦")
    else:
        print("âŒ æ— æ³•è¯»å–æµ‹è¯•æ–‡ä»¶")
else:
    print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")


# æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯
import re
import string

class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""

    def __init__(self):
        # å°†è‹±æ–‡æ ‡ç‚¹å’Œä¸­æ–‡æ ‡ç‚¹è¿›è¡Œå­—ç¬¦ä¸²æ‹¼æ¥ï¼ˆstring.punctuationä¸ºè‹±æ–‡æ ‡ç‚¹ï¼‰
        self.punctuation = set(string.punctuation + 'ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€â€”â€¦')

    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ã€å¤šä½™ç©ºæ ¼ç­‰"""
        # 1. å»é™¤ç½‘å€ã€é‚®ç®±ç­‰
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        text = re.sub(r'\S+@\S+', '', text)

        # 2. å»é™¤æ•°å­—å’Œè‹±æ–‡ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
        text = re.sub(r'[a-zA-Z0-9]', '', text)

        # 3. å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', '', text)

        return text.strip()

    def segment_text(self, text):
        """ä¸­æ–‡åˆ†è¯"""
        # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
        words = jieba.cut(text)

        # è¿‡æ»¤æ‰å•å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
        filtered_words = []
        for word in words:
            word = word.strip()
            if len(word) > 1 and word not in self.punctuation:
                filtered_words.append(word)

        return filtered_words

    def process_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        content = detect_and_read_file(file_path)
        if content is None:
            return []

        # æ¸…ç†æ–‡æœ¬
        clean_content = self.clean_text(content)

        # åˆ†è¯
        words = self.segment_text(clean_content)

        return words

# åˆ›å»ºé¢„å¤„ç†å™¨
preprocessor = TextPreprocessor()

# æµ‹è¯•é¢„å¤„ç†åŠŸèƒ½
print("ğŸ§ª æµ‹è¯•æ–‡æœ¬é¢„å¤„ç†åŠŸèƒ½...")
test_text = "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼æˆ‘ä»¬å»å…¬å›­ç©å§ã€‚ç½‘å€ï¼šhttp://example.com é‚®ç®±ï¼štest@email.com"
print(f"åŸæ–‡: {test_text}")

clean_text = preprocessor.clean_text(test_text)
print(f"æ¸…ç†åï¼š{clean_text}")

words = preprocessor.segment_text(clean_text)
print(f"åˆ†è¯ç»“æœï¼š{words}")

# å¤„ç†ä¸€ä¸ªå®é™…æ–‡ä»¶
if os.path.exists(test_file):
    print(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {test_file}")
    processed_words = preprocessor.process_file(test_file)
    print(f"åˆ†è¯æ•°é‡ï¼š{len(processed_words)}")
    print(f"å‰20ä¸ªè¯ï¼š{processed_words[:20]}")
else:
    print("âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
    # åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿçš„ä¸­æ–‡æ–‡æœ¬ç”¨äºæµ‹è¯•
    sample_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚æ˜å¤©å¯èƒ½ä¼šä¸‹é›¨ã€‚æˆ‘å–œæ¬¢åœ¨æ™´å¤©çš„æ—¶å€™å»å…¬å›­æ•£æ­¥ã€‚"
    words = preprocessor.segment_text(sample_text)
    print(f"æ¨¡æ‹Ÿæ•°æ®åˆ†è¯ç»“æœ: {words}")


# æ„å»ºè¯æ±‡è¡¨å’Œæ•°æ®åŠ è½½å™¨
class Vocabulary:
    """è¯æ±‡è¡¨ç±»"""

    def __init__(self, min_freq=2):
        """
        Args:
            min_freq: è¯æ±‡æœ€å°‘å‡ºç°æ¬¡æ•°ï¼Œä½äºæ­¤é¢‘ç‡çš„è¯ä¼šè¢«æ ‡è®°ä¸ºæœªçŸ¥è¯
        """
        self.min_freq = min_freq
        self.word2idx = {}              # è¯ -> ç´¢å¼•
        self.idx2word = {}              # ç´¢å¼• -> è¯
        self.word_counts = Counter()    # è¯é¢‘ç»Ÿè®¡

        # ç‰¹æ®Šæ ‡è®°
        self.UNK_TOKEN = '<UNK>'        # æœªçŸ¥è¯
        self.PAD_TOKEN = '<PAD>'        # å¡«å……è¯ï¼ˆç”¨äºbatch paddingï¼‰

        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        self._add_word(self.PAD_TOKEN)  # ç´¢å¼• 0
        self._add_word(self.UNK_TOKEN)  # ç´¢å¼• 1

    def _add_word(self, word):
        """æ·»åŠ è¯åˆ°è¯æ±‡è¡¨"""
        if word not in self.word2idx:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word

    def build_vocab(self, word_lists):
        """
        ä»è¯åˆ—è¡¨æ„å»ºè¯æ±‡è¡¨

        Args:
            word_lists: è¯çš„åˆ—è¡¨çš„åˆ—è¡¨ï¼Œå¦‚ [['ä»Šå¤©', 'å¤©æ°”'], ['æ˜å¤©', 'ä¸‹é›¨']]
        """
        print("ğŸ“Š ç»Ÿè®¡è¯é¢‘...")

        # ç»Ÿè®¡æ‰€æœ‰è¯çš„é¢‘ç‡
        for words in word_lists:
            self.word_counts.update(words)

        print(f"ğŸ“ˆ æ€»å…±å‘ç° {len(self.word_counts)} ä¸ªå”¯ä¸€è¯æ±‡")
        print(f"ğŸ“‹ æœ€å¸¸è§çš„10ä¸ªè¯: {self.word_counts.most_common(10)}")

        # æ·»åŠ é¢‘ç‡é«˜äºé˜ˆå€¼çš„è¯
        added_count = 0
        for word, count in self.word_counts.items():
            if count >= self.min_freq:
                self._add_word(word)
                added_count += 1

        print(f"âœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {len(self.word2idx)} (å…¶ä¸­ {added_count} ä¸ªå¸¸ç”¨è¯)")
        print(f"ğŸ—‘ï¸  è¿‡æ»¤æ‰ {len(self.word_counts) - added_count} ä¸ªä½é¢‘è¯")

    def word_to_idx(self, word):
        """å°†è¯è½¬æ¢ä¸ºç´¢å¼•"""
        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])

    def idx_to_word(self, idx):
        """å°†ç´¢å¼•è½¬æ¢ä¸ºè¯"""
        return self.idx2word.get(idx, self.UNK_TOKEN)

    def words_to_indices(self, words):
        """å°†æ­¤åˆ—è¡¨è½¬æ¢ä¸ºç´¢å¼•åˆ—è¡¨"""
        return [self.word_to_idx(word) for word in words]

    def indices_to_words(self, indices):
        """å°†ç´¢å¼•åˆ—è¡¨è½¬æ¢ä¸ºè¯åˆ—è¡¨"""
        return [self.idx_to_word(idx) for idx in indices]

    def __len__(self):
        return len(self.word2idx)

# åˆ›å»ºä¸€äº›ç¤ºä¾‹æ•°æ®æ¥æµ‹è¯•è¯æ±‡è¡¨
print("ğŸ§ª åˆ›å»ºæµ‹è¯•æ•°æ®...")
sample_word_lists = [
    ['ä»Šå¤©', 'å¤©æ°”', 'å¾ˆ', 'å¥½'],
    ['æ˜å¤©', 'å¯èƒ½', 'ä¼š', 'ä¸‹é›¨'],
    ['æˆ‘', 'å–œæ¬¢', 'æ™´å¤©', 'çš„', 'æ—¶å€™'],
    ['ä»Šå¤©', 'å¤©æ°”', 'çœŸ', 'å¥½'],  # é‡å¤çš„è¯ç”¨æ¥æµ‹è¯•è¯é¢‘
    ['å¤©æ°”', 'é¢„æŠ¥', 'è¯´', 'æ˜å¤©', 'æ™´å¤©']
]

# æ„å»ºè¯æ±‡è¡¨
vocab = Vocabulary(min_freq=1)          # è®¾ç½®æœ€å°é¢‘ç‡ä¸º1ï¼Œ è¿™æ ·æ‰€æœ‰è¯éƒ½ä¼šè¢«ä¿ç•™
vocab.build_vocab(sample_word_lists)

# æµ‹è¯•è¯æ±‡è¡¨åŠŸèƒ½
print(f"\nğŸ§ª æµ‹è¯•è¯æ±‡è¡¨åŠŸèƒ½:")
test_words = ['ä»Šå¤©', 'å¤©æ°”', 'æœªçŸ¥è¯æ±‡']
for word in test_words:
    idx = vocab.word_to_idx(word)
    back_word = vocab.idx_to_word(idx)
    print(f"  '{word}' â†’ {idx} â†’ '{back_word}'")

# æµ‹è¯•å¥å­è½¬æ¢
test_sentence = ['ä»Šå¤©', 'å¤©æ°”', 'å¾ˆ', 'å¥½']
indices = vocab.words_to_indices(test_sentence)
back_words = vocab.indices_to_words(indices)
print(f"\nğŸ“ å¥å­è½¬æ¢æµ‹è¯•:")
print(f"  åŸå¥å­: {test_sentence}")
print(f"  ç´¢å¼•åºåˆ—: {indices}")
print(f"  è¿˜åŸå¥å­: {back_words}")