import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from collections import defaultdict, Counter
import re
import random
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sympy.printing.pretty.pretty_symbology import line_width
from torch.utils.hipify.hipify_python import preprocessor

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
random.seed(42)

print("âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼")

def generate_training_samples(sentence, window_size=2, model_type='skip-gram'):
    """
    ä»å¥å­ç”Ÿæˆè®­ç»ƒæ ·æœ¬
    :param sentence: åˆ†è¯åçš„å¥å­åˆ—è¡¨
    :param window_size: ä¸Šä¸‹æ–‡çª—å£å¤§å°
    :param model_type: 'skip-gram' æˆ– 'cbow'
    :return: è®­ç»ƒæ ·æœ¬åˆ—è¡¨
    """
    samples = []

    for i, center_word in enumerate(sentence):
        # è·å–ä¸Šä¸‹æ–‡çª—å£
        start_idx = max(0, i - window_size)
        end_idx = min(len(sentence), i + window_size + 1)

        context_words = []
        for j in range(start_idx, end_idx):
            if j != i:          # æ’é™¤ä¸­å¿ƒè¯æœ¬èº«
                context_words.append(sentence[j])

        if model_type == 'skip-gram':
            # skip-gram: ä¸­å¿ƒè¯ -> ä¸Šä¸‹æ–‡è¯
            for context_word in context_words:
                samples.append((center_word, context_word))

        elif model_type == 'cbow':
            # CBOW: ä¸Šä¸‹æ–‡è¯ -> ä¸­å¿ƒè¯
            if context_words:       # ç¡®ä¿æœ‰ä¸Šä¸‹æ–‡è¯
                samples.append((context_words, center_word))

    return samples

# æ¼”ç¤ºæ ·æœ¬ç”Ÿæˆ
sentence = ['æˆ‘', 'å–œæ¬¢', 'åƒ', 'è‹¹æœ', 'å’Œ', 'é¦™è•‰']
print("åŸå¥å­:", sentence)
print('\n' + '=' * 50)

# skip-gram æ ·æœ¬
skip_gram_samples = generate_training_samples(sentence, window_size=2, model_type='skip-gram')
print("ğŸ¯ Skip-gram è®­ç»ƒæ ·æœ¬ (ä¸­å¿ƒè¯ â†’ ä¸Šä¸‹æ–‡è¯):")
for i, (center, context) in enumerate(skip_gram_samples):          # åªæ˜¾ç¤ºå‰10ä¸ª
    print(f"  {i + 1}. '{center}' -> '{context}'")

print('\n' + '=' * 50)

# CBOWæ ·æœ¬
cbow_samples = generate_training_samples(sentence, window_size=2, model_type='cbow')
print("ğŸ“Š CBOW è®­ç»ƒæ ·æœ¬ (ä¸Šä¸‹æ–‡è¯ â†’ ä¸­å¿ƒè¯):")
for i, (context, center) in enumerate(cbow_samples):
    context_str = ", ".join(context)
    print(f"  {i + 1}. [{context_str}] -> '{center}'")



# ä»å¤´å¼€å§‹å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„Word2Vecæ¨¡å‹ï¼æˆ‘ä»¬å°†å®ç°Skip-gramæ¶æ„ï¼Œå¹¶ä½¿ç”¨è´Ÿé‡‡æ ·ä¼˜åŒ–ã€‚

# å‡†å¤‡è®­ç»ƒè¯­æ–™
corpus = [
    "æˆ‘ å–œæ¬¢ åƒ è‹¹æœ",
    "æˆ‘ å–œæ¬¢ åƒ é¦™è•‰",
    "è‹¹æœ å’Œ é¦™è•‰ éƒ½ å¾ˆ å¥½åƒ",
    "æ°´æœ å¾ˆ æœ‰è¥å…»",
    "æˆ‘ æ¯å¤© éƒ½ åƒ æ°´æœ",
    "è‹¹æœ æ˜¯ çº¢è‰² çš„",
    "é¦™è•‰ æ˜¯ é»„è‰² çš„",
    "æˆ‘ å–œæ¬¢ çº¢è‰² çš„ è‹¹æœ",
    "æ–°é²œ çš„ æ°´æœ å¾ˆ ç”œ",
    "è¿™ä¸ª è‹¹æœ å¾ˆ ç”œ å¾ˆ å¥½åƒ",
    "é‚£ä¸ª é¦™è•‰ å¾ˆ ç”œ",
    "æˆ‘ ä¹° äº† å¾ˆå¤š æ°´æœ",
    "æ°´æœ å¸‚åœº æœ‰ å¾ˆå¤š è‹¹æœ",
    "æˆ‘ åœ¨ æ°´æœ åº— ä¹° é¦™è•‰",
    "è¿™äº› æ°´æœ éƒ½ å¾ˆ æ–°é²œ"
]

class Word2VecDataPreprocessor:
    """Word2Vecæ•°æ®é¢„å¤„ç†å™¨
    æ³¨æ„:è¿™ä¸ªé¢„å¤„ç†å™¨åªç”Ÿæˆäº†æ­£æ ·æœ¬å¯¹(ä¸­å¿ƒè¯,ä¸Šä¸‹æ–‡è¯),æ²¡æœ‰è¿›è¡Œè´Ÿé‡‡æ ·
    """

    def __init__(self, min_count=1):
        self.min_count = min_count          # è¯é¢‘é˜ˆå€¼
        self.word2idx = {}                  # è¯åˆ°ç´¢å¼•çš„æ˜ å°„
        self.idx2word = {}                  # ç´¢å¼•åˆ°è¯çš„æ˜ å°„
        self.word_counts = Counter()        # è¯é¢‘ç»Ÿè®¡
        self.vocab_size = 0                 # è¯æ±‡è¡¨å¤§å°

    def build_vocab(self, corpus):
        """æ„å»ºè¯æ±‡è¡¨ï¼Œä½†ä¸æ¶‰åŠè´Ÿé‡‡æ ·"""
        # ç»Ÿè®¡è¯é¢‘
        for sentence in corpus:
            words = sentence.split()
            for word in words:
                self.word_counts[word] += 1

        # è¿‡æ»¤ä½é¢‘è¯ï¼Œä½†ä¸æ˜¯è´Ÿé‡‡æ ·
        filtered_words = [word for word, count in self.word_counts.items()
                          if count >= self.min_count]

        # æ„å»ºè¯æ±‡æ˜ å°„
        for i, word in enumerate(filtered_words):
            self.word2idx[word] = i
            self.idx2word[i] = word

        self.vocab_size = len(self.word2idx)
        print(f"è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        print(f"è¯æ±‡è¡¨: {list(self.word2idx.keys())}")

    def sentence_to_indices(self, sentence):
        """å°†å¥å­è½¬æ¢ä¸ºç´¢å¼•"""
        words = sentence.split()
        return [self.word2idx[word] for word in words if word in self.word2idx]

    def generate_skip_gram_samples(self, corpus, window_size=2):
        """ç”Ÿæˆskip-gramè®­ç»ƒæ ·æœ¬
        åªç”Ÿæˆæ­£æ ·æœ¬å¯¹,è´Ÿé‡‡æ ·åœ¨æ¨¡å‹è®­ç»ƒæ—¶è¿›è¡Œ
        """
        samples = []

        for sentence in corpus:
            indices = self.sentence_to_indices(sentence)

            for i, center_idx in enumerate(indices):
                # è·å–ä¸Šä¸‹æ–‡çª—å£
                start = max(0, i - window_size)
                end = min(len(indices), i + window_size + 1)

                # åªç”Ÿæˆæ­£æ ·æœ¬å¯¹
                for j in range(start, end):
                    if j != i:              # æ’é™¤ä¸­å¿ƒè¯
                        context_idx = indices[j]
                        samples.append((center_idx, context_idx))

        return samples

# åˆå§‹åŒ–é¢„å¤„ç†å™¨
preprocessor = Word2VecDataPreprocessor(min_count=1)
preprocessor.build_vocab(corpus)

# ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼ˆåªæœ‰æ­£æ ·æœ¬ï¼‰
training_samples = preprocessor.generate_skip_gram_samples(corpus, window_size=2)
print(f"\nè®­ç»ƒæ ·æœ¬æ•°é‡ï¼š{len(training_samples)}")
print("å‰10ä¸ªè®­ç»ƒæ ·æœ¬")
for i, (center, context) in enumerate(training_samples[:10]):
    center_word = preprocessor.idx2word[center]
    context_word = preprocessor.idx2word[context]
    print(f"  {i + 1}. {center_word}({center}) -> {context_word}({context})")


# Word2Vec æ¨¡å‹å®ç°ï¼Œå®ç°æ ¸å¿ƒçš„Skip-gramæ¨¡å‹
class Word2VecSkipGram:
    """Skip-gram Word2Vecæ¨¡å‹
    ä½¿ç”¨è´Ÿé‡‡æ ·ä¼˜åŒ–çš„Skip-gramæ¨¡å‹å®ç°ã€‚ä¸»è¦åŒ…å«:
    1. è¯å‘é‡çŸ©é˜µåˆå§‹åŒ–
    2. è´Ÿé‡‡æ ·
    3. å‰å‘ä¼ æ’­è®¡ç®—
    4. åå‘ä¼ æ’­æ›´æ–°
    """

    def __init__(self, vocab_size, embedding_dim=50, learning_rate=0.01, neg_samples=5):
        self.vocab_size = vocab_size                # è¯æ±‡è¡¨å¤§å°
        self.embedding_dim = embedding_dim          # è¯å‘é‡ç»´åº¦
        self.learning_rate = learning_rate          # å­¦ä¹ ç‡
        self.neg_samples = neg_samples              # æ¯ä¸ªæ­£æ ·æœ¬å¯¹åº”çš„è´Ÿæ ·æœ¬æ•°é‡

        # åˆå§‹åŒ–ä¸¤ä¸ªæƒé‡çŸ©é˜µï¼š
        # 1. W_in: è¾“å…¥è¯å‘é‡çŸ©é˜µ, shape=(vocab_size, embedding_dim)
        # 2. W_out: è¾“å‡ºä¸Šä¸‹æ–‡çŸ©é˜µ, shape=(embedding_dim, vocab_size)
        # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–, èŒƒå›´ä¸º[-0.5/dim, 0.5/dim]
        self.W_in = np.random.uniform(-0.5/embedding_dim, 0.5/embedding_dim, (vocab_size, embedding_dim))
        self.W_out = np.random.uniform(-0.5/embedding_dim, 0.5/embedding_dim, (embedding_dim, vocab_size))

    def sigmoid(self, x):
        """Sigmoidæ¿€æ´»å‡½æ•°
        ä¸ºé˜²æ­¢æ•°å€¼æº¢å‡º,å°†è¾“å…¥é™åˆ¶åœ¨[-500, 500]èŒƒå›´å†…
        """
        x = np.clip(x, -500, 500)
        return 1.0 / (1.0 + np.exp(-x))

    def negative_sampling(self, target_idx, num_samples):
        """
        è´Ÿé‡‡æ ·ï¼šéšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„è´Ÿæ ·æœ¬è¯
        :param target_idx: ç›®æ ‡è¯çš„ç´¢å¼•ï¼ˆæ­£æ ·æœ¬ï¼‰
        :param num_samples: éœ€è¦é‡‡æ ·çš„è´Ÿæ ·æœ¬çš„æ•°é‡
        :return: è´Ÿæ ·æœ¬ç´¢å¼•åˆ—è¡¨
        æ³¨æ„ï¼šç¡®ä¿ä¸ä¼šé‡‡æ ·åˆ°ç›®æ ‡è¯æœ¬èº«ä½œä¸ºè´Ÿæ ·æœ¬
        """
        negative_samples = []
        while len(negative_samples) < num_samples:
            neg_idx = np.random.randint(0, self.vocab_size)
            if neg_idx != target_idx:           # æ’é™¤ç›®æ ‡è¯
                negative_samples.append(neg_idx)
        return negative_samples

    def forward_pass(self, center_idx, context_idx, negative_indices):
        """
        å‰å‘ä¼ æ’­è®¡ç®—
        :param center_idx: ä¸­å¿ƒè¯ç´¢å¼•
        :param context_idx: ä¸Šä¸‹æ–‡è¯ç´¢å¼•ï¼ˆæ­£æ ·æœ¬ï¼‰
        :param negative_indices: è´Ÿæ ·æœ¬è¯ç´¢å¼•åˆ—è¡¨
        :return:
            center_embedding: ä¸­å¿ƒè¯å‘é‡
            pos_score: æ­£æ ·æœ¬å¾—åˆ†
            pos_prob: æ­£æ ·æœ¬æ¦‚ç‡
            neg_scores: è´Ÿæ ·æœ¬å¾—åˆ†åˆ—è¡¨
            neg_probs: è´Ÿæ ·æœ¬æ¦‚ç‡åˆ—è¡¨
        """

        # 1. è·å–ä¸­å¿ƒè¯å‘é‡
        center_embedding = self.W_in[center_idx]        # shape: (embedding_dim, )

        # 2. è®¡ç®—æ­£æ ·æœ¬å¾—åˆ†å’Œæ¦‚ç‡
        pos_score = np.dot(center_embedding, self.W_out[:, context_idx])
        pos_prob = self.sigmoid(pos_score)

        # 3. è®¡ç®—æ‰€æœ‰è´Ÿæ ·æœ¬çš„å¾—åˆ†å’Œæ¦‚ç‡
        neg_scores = []
        neg_probs = []
        for neg_idx in negative_indices:
            neg_score = np.dot(center_embedding, self.W_out[:, neg_idx])
            # å¯¹è´Ÿæ ·æœ¬ä½¿ç”¨-score, å› ä¸ºæˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–æ­£æ ·æœ¬æ¦‚ç‡ï¼Œæœ€å°åŒ–è´Ÿæ ·æœ¬æ¦‚ç‡
            neg_prob = self.sigmoid(-neg_score)
            neg_scores.append(neg_score)
            neg_probs.append(neg_prob)

        return center_embedding, pos_score, pos_prob, neg_scores, neg_probs

    def backward_pass(self, center_idx, context_idx, negative_indices,
                      center_embedding, pos_prob, neg_probs):
        """
        åå‘è½¬æ’­æ›´æ–°æƒé‡
        ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æ›´æ–°è¯å‘é‡
        :param center_idx: ä¸­å¿ƒè¯ç´¢å¼•
        :param context_idx: ä¸Šä¸‹æ–‡è¯ç´¢å¼•ï¼ˆæ­£æ ·æœ¬ï¼‰
        :param negative_indices: è´Ÿæ ·æœ¬è¯ç´¢å¼•åˆ—è¡¨
        :param center_embedding: ä¸­å¿ƒè¯å‘é‡
        :param pos_prob: æ­£æ ·æœ¬é¢„æµ‹æ¦‚ç‡
        :param neg_probs: è´Ÿæ ·æœ¬é¢„æµ‹æ¦‚ç‡åˆ—è¡¨
        """

        # 1. è®¡ç®—å¹¶æ›´æ–°æ­£æ ·æœ¬ç›¸å…³çš„æƒé‡
        pos_grad = (1 - pos_prob) * self.learning_rate
        self.W_out[:, context_idx] += pos_grad * center_embedding       # æ›´æ–°è¾“å‡ºçŸ©é˜µ
        center_grad = pos_grad * self.W_out[:, context_idx]             # ç´¯ç§¯ä¸­å¿ƒè¯æ¢¯åº¦

        # 2. è®¡ç®—å¹¶æ›´æ–°è´Ÿæ ·æœ¬ç›¸å…³çš„æƒé‡
        for i, neg_idx in enumerate(negative_indices):
            # è´Ÿæ ·æœ¬æ¢¯åº¦ï¼ˆæ³¨æ„ç¬¦å·ç›¸åï¼‰
            neg_grad = -(1 - neg_probs[i]) * self.learning_rate
            # æ›´æ–°è´Ÿæ ·æœ¬åœ¨è¾“å‡ºçŸ©é˜µä¸­çš„æƒé‡
            self.W_out[:, neg_idx] += neg_grad * center_embedding
            # ç´¯ç§¯è´Ÿæ ·æœ¬å¯¹ä¸­å¿ƒè¯çš„æ¢¯åº¦è´¡çŒ®
            center_grad += neg_grad * self.W_out[:, neg_idx]

        # 3. æœ€åæ›´æ–°ä¸­å¿ƒè¯çš„è¯å‘é‡
        self.W_in[center_idx] += center_grad

    def train_step(self, center_idx, context_idx):
        """å•æ­¥è®­ç»ƒ
        æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„å®Œæ•´è®­ç»ƒæ­¥éª¤
        """

        # 1. ä¸ºå½“å‰æ ·æœ¬è¿›è¡Œè´Ÿé‡‡æ ·
        negative_indices = self.negative_sampling(context_idx, self.neg_samples)

        # 2. å‰å‘ä¼ æ’­è®¡ç®—
        center_embedding, pos_score, pos_prob, neg_scores, neg_probs = self.forward_pass(center_idx, context_idx, negative_indices)

        # 3. è®¡ç®—æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
        pos_loss = -np.log(pos_prob + 1e-10)            # æ­£æ ·æœ¬æŸå¤±
        neg_loss = -np.sum([np.log(neg_prob + 1e-10) for neg_prob in neg_probs])       # è´Ÿæ ·æœ¬æŸå¤±
        total_loss = pos_loss + neg_loss

        # 4. åå‘ä¼ æ’­æ›´æ–°å‚æ•°
        self.backward_pass(center_idx, context_idx, negative_indices, center_embedding, pos_prob, neg_probs)

        return total_loss

    def train(self, training_samples, epochs=10):
        """è®­ç»ƒæ¨¡å‹
        Args:
            training_samples: è®­ç»ƒæ ·æœ¬åˆ—è¡¨,æ¯ä¸ªæ ·æœ¬æ˜¯(ä¸­å¿ƒè¯,ä¸Šä¸‹æ–‡è¯)å¯¹
            epochs: è®­ç»ƒè½®æ•°
        Returns:
            losses: æ¯ä¸ªepochçš„å¹³å‡æŸå¤±åˆ—è¡¨
        """
        print("å¼€å§‹è®­ç»ƒWord2Vecæ¨¡å‹...")
        losses = []

        for epoch in range(epochs):
            total_loss = 0
            np.random.shuffle(training_samples)             # æ‰“ä¹±è®­ç»ƒæ ·æœ¬é¡ºåº

            # éå†æ‰€æœ‰è®­ç»ƒæ ·æœ¬
            for center_idx, context_idx, in training_samples:
                loss = self.train_step(center_idx, context_idx)
                total_loss += loss

            # è®¡ç®—å¹¶è®°å½•å½“å‰epochçš„å¹³å‡æŸå¤±
            avg_loss = total_loss / len(training_samples)
            losses.append(avg_loss)

            if (epoch + 1) % 2 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, å¹³å‡æŸå¤±: {avg_loss:.4f}")

        print("è®­ç»ƒå®Œæˆï¼")
        return losses

    def get_word_vector(self, word_idx):
        """è·å–æŒ‡å®šè¯çš„è¯å‘é‡"""
        return self.W_in[word_idx]

    def similarity(self, word_idx1, word_idx2):
        """è®¡ç®—ä¸¤ä¸ªè¯çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        vec1 = self.get_word_vector(word_idx1)
        vec2 = self.get_word_vector(word_idx2)

        # å¯¹è¯å‘é‡è¿›è¡ŒL2å½’ä¸€åŒ–
        vec1_norm = vec1 / np.linalg.norm(vec1)
        vec2_norm = vec2 / np.linalg.norm(vec2)

        return np.dot(vec1_norm, vec2_norm)

# åˆå§‹åŒ–æ¨¡å‹
model = Word2VecSkipGram(
    vocab_size=preprocessor.vocab_size,
    embedding_dim=50,
    learning_rate=0.1,
    neg_samples=5
)

print("âœ… Word2Vecæ¨¡å‹åˆå§‹åŒ–å®Œæˆ!")
print(f"è¯æ±‡è¡¨å¤§å°: {model.vocab_size}")
print(f"åµŒå…¥ç»´åº¦: {model.embedding_dim}")
print(f"å­¦ä¹ ç‡: {model.learning_rate}")
print(f"è´Ÿé‡‡æ ·æ•°é‡: {model.neg_samples}")


# è®­ç»ƒä¸å¯è§†åŒ–
# è®­ç»ƒæ¨¡å‹
losses = model.train(training_samples, epochs=20)

# ç»˜åˆ¶æŸå¤±æ›²çº¿
plt.figure(figsize=(10, 6))
plt.plot(losses, 'b-', linewidth=2)
plt.title('Word2Vec è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=16)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('å¹³å‡æŸå¤±', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"æœ€ç»ˆæŸå¤±ï¼š{losses[-1]:.4f}")
print(f"æŸå¤±ä¸‹é™ï¼š{losses[0]:.4f} -> {losses[-1]:.4f}")


# åˆ†æè®­ç»ƒç»“æœ
def find_most_similar_words(model, preprocessor, target_word, top_k=5):
    """æ‰¾åˆ°ä¸ç›®æ ‡è¯æœ€ç›¸ä¼¼çš„è¯"""
    if target_word not in preprocessor.word2idx:
        return []

    target_idx = preprocessor.word2idx[target_word]
    similarities = []

    for word, idx in preprocessor.word2idx.items():
        if word != target_word:             # æ’é™¤è‡ªå·±
            sim = model.similarity(target_idx, idx)
            similarities.append((word, sim))

    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)

    return similarities[:top_k]

# æµ‹è¯•è¯æ±‡ç›¸ä¼¼æ€§
test_words = ["è‹¹æœ", "æˆ‘", "åƒ", "å¾ˆ"]

print("ğŸ” è¯æ±‡ç›¸ä¼¼æ€§åˆ†æ:")
print("="*50)

for word in test_words:
    if word in preprocessor.word2idx:
        similar_words = find_most_similar_words(model, preprocessor, word, top_k=5)
        print(f"\nä¸ '{word}' æœ€ç›¸ä¼¼çš„è¯ï¼š")
        for i, (sim_word, sim_score) in enumerate(similar_words):
            print(f"  {i+1}. {sim_word}: {sim_score:.4f}")
    else:
        print(f"\nè¯ '{word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")

# è®¡ç®—ä¸€äº›æœ‰è¶£çš„è¯å¯¹ç›¸ä¼¼åº¦
print("\n" + "="*50)
print("ğŸ“Š ç‰¹å®šè¯å¯¹ç›¸ä¼¼åº¦:")

word_pairs = [
    ("è‹¹æœ", "é¦™è•‰"),   # ä¸¤ç§æ°´æœ
    ("è‹¹æœ", "æ°´æœ"),   # å…·ä½“å’ŒæŠ½è±¡
    ("çº¢è‰²", "é»„è‰²"),   # ä¸¤ç§é¢œè‰²
    ("å–œæ¬¢", "åƒ"),     # åŠ¨ä½œè¯
    ("æˆ‘", "å¾ˆ"),       # ä¸ç›¸å…³çš„è¯
]

for word1, word2 in word_pairs:
    if word1 in preprocessor.word2idx and word2 in preprocessor.word2idx:
        idx1 = preprocessor.word2idx[word1]
        idx2 = preprocessor.word2idx[word2]
        sim = model.similarity(idx1, idx2)
        print(f"'{word1}' å’Œ '{word2}': {sim:.4f}")
    else:
        print(f"'{word1}' æˆ– '{word2}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")


# è¯å‘é‡å¯è§†åŒ–
def visualize_word_vectors(model, preprocessor, method='pca', figsize=(12, 8)):
    """å¯è§†åŒ–è¯å‘é‡"""
    word_vectors = []
    words = []

    for word, idx in preprocessor.word2idx.items():
        vector = model.get_word_vector(idx)
        word_vectors.append(vector)
        words.append(word)

    word_vectors = np.array(word_vectors)

    # é™ç»´
    if method == 'pca':
        reducer = PCA(n_components=2, random_state=42)
        vectors_2d = reducer.fit_transform(word_vectors)
        title = f"Word2Vec è¯å‘é‡å¯è§†åŒ–(PCAé™ç»´)"
    else:
        reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words) - 1))
        vectors_2d = reducer.fit_transform(word_vectors)
        title = f'Word2Vec è¯å‘é‡å¯è§†åŒ–(t-SNEé™ç»´)'

    # ç»˜å›¾
    plt.figure(figsize=figsize)

    # å®šä¹‰é¢œè‰²æ˜ å°„
    colors = plt.cm.Set3(np.linspace(0, 1, len(words)))

    # ç»˜åˆ¶æ•£ç‚¹å›¾
    scatter = plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1],
                          c=colors, s=100, alpha=0.7, edgecolors='black', linewidth=0.5)

    # æ·»åŠ è¯æ±‡æ ‡ç­¾
    for i, word in enumerate(words):
        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),
                     xytext=(5, 5), textcoords='offset points',
                     fontsize=12, fontweight='bold',
                     bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))

    plt.title(title, fontsize=16, fontweight='bold')
    plt.xlabel('ç»´åº¦1', fontsize=12)
    plt.ylabel('ç»´åº¦2', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return vectors_2d

# PCA å¯è§†åŒ–
print("ğŸ“Š ä½¿ç”¨PCAè¿›è¡Œ2Då¯è§†åŒ–:")
vectors_2d_pca = visualize_word_vectors(model, preprocessor, method='pca')


# å®é™…åº”ç”¨
# è¯æ±‡ç±»æ¯”ä»»åŠ¡
def word_analogy(model, preprocessor, word_a, word_b, word_c, top_k=3):
    """
    è¯æ±‡ç±»æ¯”: word_a - word_b + word_c = ?
    ä¾‹å¦‚: è‹¹æœ - çº¢è‰² + é»„è‰² = é¦™è•‰
    """
    # æ£€æŸ¥è¯æ±‡æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
    words = [word_a, word_b, word_c]
    for word in words:
        if word not in preprocessor.word2idx:
            print(f"è¯ '{word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")
            return[]

    # è·å–è¯å‘é‡
    vec_a = model.get_word_vector(preprocessor.word2idx[word_a])
    vec_b = model.get_word_vector(preprocessor.word2idx[word_b])
    vec_c = model.get_word_vector(preprocessor.word2idx[word_c])

    # è®¡ç®—ç›®æ ‡å‘é‡: a - b + c
    target_vector = vec_a - vec_b + vec_c

    # å½’ä¸€åŒ–
    target_vector = target_vector / np.linalg.norm(target_vector)

    # è®¡ç®—ä¸æ‰€æœ‰è¯çš„ç›¸ä¼¼åº¦
    similarities = []
    exclude_words = {word_a, word_b, word_c}            # æ’é™¤è¾“å…¥çš„ä¸‰ä¸ªè¯

    for word, idx in preprocessor.word2idx.items():
        if word not in exclude_words:
            word_vector = model.get_word_vector(idx)
            word_vector = word_vector / np.linalg.norm(word_vector)

            similarity = np.dot(target_vector, word_vector)
            similarities.append((word, similarity))

    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)

    return similarities[:top_k]

# æµ‹è¯•è¯æ±‡ç±»æ¯”
print("ğŸ¯ è¯æ±‡ç±»æ¯”æµ‹è¯•:")
print("="*50)

# è™½ç„¶æˆ‘ä»¬çš„å°è¯­æ–™åº“å¯èƒ½æ— æ³•å®Œç¾å±•ç¤ºå¤æ‚çš„è¯­ä¹‰å…³ç³»ï¼Œ
# ä½†æˆ‘ä»¬å¯ä»¥å°è¯•ä¸€äº›ç®€å•çš„ç±»æ¯”

analogy_tests = [
    ("è‹¹æœ", "çº¢è‰²", "é»„è‰²"),  # è‹¹æœ - çº¢è‰² + é»„è‰² = é¦™è•‰?
    ("æˆ‘", "å–œæ¬¢", "åƒ"),      # æˆ‘ - å–œæ¬¢ + åƒ = ?
    ("æ°´æœ", "è‹¹æœ", "é¦™è•‰"), # æ°´æœ - è‹¹æœ + é¦™è•‰ = ?
]

for word_a, word_b, word_c in analogy_tests:
    print(f"\nğŸ” {word_a} - {word_b} + {word_c} = ?")
    results = word_analogy(model, preprocessor, word_a, word_b, word_c, top_k=3)

    if results:
        print("å¯èƒ½çš„ç­”æ¡ˆï¼š")
        for i, (word, similarity) in enumerate(results):
            print(f"  {i+1}. {word} (ç›¸ä¼¼åº¦: {similarity:.4f})")
    else:
        print("æ— æ³•è®¡ç®—ç±»æ¯”")

# è¯å‘é‡è¿ç®—å¯è§†åŒ–
print("\n" + "="*50)
print("ğŸ“Š è¯å‘é‡è¿ç®—å¯è§†åŒ–")

def visualize_vector_arithmetic(model, preprocessor, word_a, word_b, word_c):
    """å¯è§†åŒ–è¯å‘é‡è¿ç®—"""
    # æ£€æŸ¥è¯æ±‡
    words = [word_a, word_b, word_c]
    for word in words:
        if word not in preprocessor.word2idx:
            print(f"è¯ '{word}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")
            return

    # è·å–è¯å‘é‡
    vec_a = model.get_word_vector(preprocessor.word2idx[word_a])
    vec_b = model.get_word_vector(preprocessor.word2idx[word_b])
    vec_c = model.get_word_vector(preprocessor.word2idx[word_c])
    target_vector = vec_a - vec_b + vec_c

    # æ”¶é›†æ‰€æœ‰å‘é‡ç”¨äºå¯è§†åŒ–
    vectors = np.array([vec_a, vec_b, vec_c, target_vector])
    labels = [word_a, word_b, word_c, f"{word_a} - {word_b} + {word_c}"]

    # PCAé™ç»´
    pca = PCA(n_components=2, random_state=42)
    vectors_2d = pca.fit_transform(vectors)

    # ç»˜å›¾
    plt.figure(figsize=(10, 8))

    colors = ['red', 'blue', 'green', 'purple']
    for i, (vec_2d, label, color) in enumerate(zip(vectors_2d, labels, colors)):
        plt.scatter(vec_2d[0], vec_2d[1], c=color, s=100, alpha=0.7, label=label)
        plt.annotate(label, (vec_2d[0], vec_2d[1]), xytext=(5, 5), textcoords='offset points',
                     fontsize=10, fontweight='bold')

    plt.title(f'è¯å‘é‡è¿ç®—: {word_a} - {word_b} + {word_c}', fontsize=14)
    plt.xlabel('PC1', fontsize=12)
    plt.ylabel('PC2', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# å¯è§†åŒ–ä¸€ä¸ªè¯å‘é‡è¿ç®—
if "è‹¹æœ" in preprocessor.word2idx and "çº¢è‰²" in preprocessor.word2idx and "é»„è‰²" in preprocessor.word2idx:
    visualize_vector_arithmetic(model, preprocessor, "è‹¹æœ", "çº¢è‰²", "é»„è‰²")


# ä½¿ç”¨ç°æˆçš„Word2Vecåº“
# ä½¿ç”¨Gensimè®­ç»ƒWord2Vec (éœ€è¦å®‰è£…: pip install gensim)
from gensim.models import Word2Vec

# å‡†å¤‡è¯­æ–™(Gensiméœ€è¦å¥å­åˆ—è¡¨ï¼Œæ¯ä¸ªå¥å­æ˜¯è¯çš„åˆ—è¡¨)
sentences = [sentence.split() for sentence in corpus]

# è®­ç»ƒWord2Vecæ¨¡å‹
gensim_model = Word2Vec(
    sentences=sentences,
    vector_size=50,                     # è¯å‘é‡ç»´åº¦
    window=2,                           # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    min_count=1,                        # æœ€å°è¯é¢‘
    workers=1,                          # çº¿ç¨‹æ•°
    sg=1,                               # 1è¡¨ç¤ºskip-gram, 0è¡¨ç¤ºCBOW
    epochs=20                           # è®­ç»ƒè½®æ•°
)

print("âœ… Gensim Word2Vecæ¨¡å‹è®­ç»ƒå®Œæˆ!")
print(f"è¯æ±‡è¡¨å¤§å°: {len(gensim_model.wv.key_to_index)}")

# æµ‹è¯•ç›¸ä¼¼åº¦
print("\nğŸ” Gensimæ¨¡å‹ç›¸ä¼¼è¯:")
test_word = "è‹¹æœ"
if test_word in gensim_model.wv:
    similar_words = gensim_model.wv.most_similar(test_word, topn=3)
    print(f"ä¸'{test_word}'æœ€ç›¸ä¼¼çš„è¯ï¼š")
    for word, similarity in similar_words:
        print(f"  {word}: {similarity:.4f}")

# è¯æ±‡ç±»æ¯”
print("\nğŸ¯ Gensimè¯æ±‡ç±»æ¯”:")
try:
    # positiveè¡¨ç¤ºè¦åŠ çš„è¯, negativeè¡¨ç¤ºè¦å‡çš„è¯
    result = gensim_model.wv.most_similar(
        positive=['é»„è‰²', 'è‹¹æœ'],
        negative=['çº¢è‰²'],
        topn=3
    )
    print("è‹¹æœ - çº¢è‰² + é»„è‰² = ")
    for word, similarity in result:
        print(f"  {word}: {similarity:.4f}")

except:
    print("ç±»æ¯”è®¡ç®—å¤±è´¥ï¼ˆå¯èƒ½æ˜¯è¯æ±‡è¡¨å¤ªå°ï¼‰")


# ä½¿ç”¨Gensimè®­ç»ƒè‡ªå®šä¹‰Word2Vecæ¨¡å‹
# å‡†å¤‡æ›´ä¸°å¯Œçš„è®­ç»ƒè¯­æ–™
extended_corpus = [
    "æˆ‘ å–œæ¬¢ åƒ è‹¹æœ",
    "æˆ‘ å–œæ¬¢ åƒ é¦™è•‰",
    "è‹¹æœ å’Œ é¦™è•‰ éƒ½ å¾ˆ å¥½åƒ",
    "æ°´æœ å¾ˆ æœ‰è¥å…»",
    "æˆ‘ æ¯å¤© éƒ½ åƒ æ°´æœ",
    "è‹¹æœ æ˜¯ çº¢è‰² çš„",
    "é¦™è•‰ æ˜¯ é»„è‰² çš„",
    "æˆ‘ å–œæ¬¢ çº¢è‰² çš„ è‹¹æœ",
    "æ–°é²œ çš„ æ°´æœ å¾ˆ ç”œ",
    "è¿™ä¸ª è‹¹æœ å¾ˆ ç”œ å¾ˆ å¥½åƒ",
    "é‚£ä¸ª é¦™è•‰ å¾ˆ ç”œ",
    "æˆ‘ ä¹° äº† å¾ˆå¤š æ°´æœ",
    "æ°´æœ å¸‚åœº æœ‰ å¾ˆå¤š è‹¹æœ",
    "æˆ‘ åœ¨ æ°´æœ åº— ä¹° é¦™è•‰",
    "è¿™äº› æ°´æœ éƒ½ å¾ˆ æ–°é²œ",
    "å¦ˆå¦ˆ ç»™ æˆ‘ ä¹° äº† è‹¹æœ",
    "çˆ¸çˆ¸ å–œæ¬¢ åƒ é¦™è•‰",
    "å°æœ‹å‹ éƒ½ å–œæ¬¢ æ°´æœ",
    "å¥åº· çš„ ç”Ÿæ´» éœ€è¦ å¤š åƒ æ°´æœ",
    "è‹¹æœ å«æœ‰ ä¸°å¯Œ çš„ ç»´ç”Ÿç´ ",
    "é¦™è•‰ å«æœ‰ é’¾ å…ƒç´ ",
    "æ°´æœ æ²™æ‹‰ å¾ˆ å¥½åƒ",
    "æˆ‘ æœ€ å–œæ¬¢ çš„ æ°´æœ æ˜¯ è‹¹æœ",
    "çº¢è‰² è‹¹æœ æ¯” ç»¿è‰² è‹¹æœ ç”œ",
    "æˆç†Ÿ çš„ é¦™è•‰ æ˜¯ é»„è‰² çš„",
    "æ–°é²œ æ°´æœ è¥å…» ä»·å€¼ é«˜",
    "æ¯å¤© åƒ æ°´æœ æœ‰ç›Š å¥åº·",
    "æ°´æœ åº— é‡Œ æœ‰ å„ç§ æ°´æœ",
    "è‹¹æœ æ ‘ ç»“å‡º çº¢è‰² æœå®",
    "é¦™è•‰ æ ‘ ç”Ÿé•¿ åœ¨ çƒ­å¸¦ åœ°åŒº"
]

from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«ä»¥æŸ¥çœ‹è®­ç»ƒè¿›åº¦
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

print("âœ… Gensimåº“å¯¼å…¥æˆåŠŸ!")
print(f"æ‰©å±•è¯­æ–™åº“å¤§å°: {len(extended_corpus)} å¥")

# å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆGensiméœ€è¦å¥å­åˆ—è¡¨ï¼Œæ¯ä¸ªå¥å­æ˜¯è¯çš„åˆ—è¡¨ï¼‰
sentences = [sentence.split() for sentence in extended_corpus]
print(f"é¢„å¤„ç†åçš„å¥å­æ•°é‡: {len(sentences)}")
print(f"ç¤ºä¾‹å¥å­: {sentences[0]}")

if sentences is not None:
    # è‡ªå®šä¹‰è®­ç»ƒç›‘ç£å™¨
    class TrainingMonitor(CallbackAny2Vec):
        """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å™¨"""

        def __init__(self):
            self.epoch = 0
            self.losses = []

        def on_epoch_end(self, model):
            loss = model.get_latest_training_loss()
            if self.epoch == 0:
                self.current_loss = loss
            else:
                self.current_loss = loss - self.previous_loss
            self.losses.append(self.current_loss)
            self.previous_loss = loss
            self.epoch += 1
            print(f'Epoch {self.epoch}: Loss = {self.current_loss:.4f}')

    # åˆ›å»ºç›‘æ§å™¨
    monitor = TrainingMonitor()

    print("ğŸš€ å¼€å§‹ä½¿ç”¨Gensimè®­ç»ƒWord2Vecæ¨¡å‹...")
    print("=" * 50)

    # é…ç½®1: Skip-gram + è´Ÿé‡‡æ ·
    print("\nğŸ“ é…ç½®1: Skip-gram + è´Ÿé‡‡æ ·")
    gensim_skipgram = Word2Vec(
        sentences=sentences,
        vector_size=50,  # è¯å‘é‡ç»´åº¦
        window=2,  # ä¸Šä¸‹æ–‡çª—å£å¤§å°
        min_count=1,  # æœ€å°è¯é¢‘ï¼ˆä¿ç•™æ‰€æœ‰è¯ï¼‰
        workers=1,  # çº¿ç¨‹æ•°
        sg=1,  # Skip-gram
        hs=0,  # ä¸ä½¿ç”¨å±‚æ¬¡softmax
        negative=5,  # è´Ÿé‡‡æ ·æ•°é‡
        epochs=20,  # è®­ç»ƒè½®æ•°
        alpha=0.025,  # åˆå§‹å­¦ä¹ ç‡
        min_alpha=0.0001,  # æœ€å°å­¦ä¹ ç‡
        seed=42,  # éšæœºç§å­
        compute_loss=True,  # è®¡ç®—æŸå¤±
        callbacks=[monitor]  # è®­ç»ƒç›‘æ§
    )

    print(f"âœ… Skip-gramæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print(f"   è¯æ±‡è¡¨å¤§å°: {len(gensim_skipgram.wv.key_to_index)}")
    print(f"   å‘é‡ç»´åº¦: {gensim_skipgram.wv.vector_size}")

else:
    print("âŒ æ— æ³•è¿›è¡ŒGensimè®­ç»ƒï¼ˆç¼ºå°‘Gensimåº“ï¼‰")


if sentences is not None:
    # è®­ç»ƒå¤šç§é…ç½®è¿›è¡Œå¯¹æ¯”
    print("\n" + "=" * 50)
    print("ğŸ”„ è®­ç»ƒä¸åŒé…ç½®çš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”")

    # é…ç½®2: CBOW + è´Ÿé‡‡æ ·
    print("\nğŸ“ é…ç½®2: CBOW + è´Ÿé‡‡æ ·")
    monitor_cbow = TrainingMonitor()
    gensim_cbow = Word2Vec(
        sentences=sentences,
        vector_size=50,
        window=2,
        min_count=1,
        workers=1,
        sg=0,  # CBOW
        hs=0,
        negative=5,
        epochs=20,
        alpha=0.025,
        seed=42,
        compute_loss=True,
        callbacks=[monitor_cbow]
    )

    print(f"âœ… CBOWæ¨¡å‹è®­ç»ƒå®Œæˆ!")

    # é…ç½®3: Skip-gram + å±‚æ¬¡Softmax
    print("\nğŸ“ é…ç½®3: Skip-gram + å±‚æ¬¡Softmax")
    monitor_hs = TrainingMonitor()
    gensim_hs = Word2Vec(
        sentences=sentences,
        vector_size=50,
        window=2,
        min_count=1,
        workers=1,
        sg=1,  # Skip-gram
        hs=1,  # å±‚æ¬¡softmax
        negative=0,  # ä¸ä½¿ç”¨è´Ÿé‡‡æ ·
        epochs=20,
        alpha=0.025,
        seed=42,
        compute_loss=True,
        callbacks=[monitor_hs]
    )

    print(f"âœ… å±‚æ¬¡Softmaxæ¨¡å‹è®­ç»ƒå®Œæˆ!")

    # ç»˜åˆ¶æŸå¤±å¯¹æ¯”
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    plt.plot(monitor.losses, 'b-', linewidth=2, label='Skip-gram + è´Ÿé‡‡æ ·')
    plt.title('Skip-gram + è´Ÿé‡‡æ · æŸå¤±æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.legend()

    plt.subplot(2, 2, 2)
    plt.plot(monitor_cbow.losses, 'r-', linewidth=2, label='CBOW + è´Ÿé‡‡æ ·')
    plt.title('CBOW + è´Ÿé‡‡æ · æŸå¤±æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.legend()

    plt.subplot(2, 2, 3)
    plt.plot(monitor_hs.losses, 'g-', linewidth=2, label='Skip-gram + å±‚æ¬¡Softmax')
    plt.title('Skip-gram + å±‚æ¬¡Softmax æŸå¤±æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.legend()

    plt.subplot(2, 2, 4)
    plt.plot(monitor.losses, 'b-', linewidth=2, label='Skip-gram + è´Ÿé‡‡æ ·')
    plt.plot(monitor_cbow.losses, 'r-', linewidth=2, label='CBOW + è´Ÿé‡‡æ ·')
    plt.plot(monitor_hs.losses, 'g-', linewidth=2, label='Skip-gram + å±‚æ¬¡Softmax')
    plt.title('æ‰€æœ‰é…ç½®æŸå¤±å¯¹æ¯”')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.legend()

    plt.tight_layout()
    plt.show()

    print("\nğŸ“Š è®­ç»ƒç»“æœæ€»ç»“:")
    print(f"Skip-gram + è´Ÿé‡‡æ · æœ€ç»ˆæŸå¤±: {monitor.losses[-1]:.4f}")
    print(f"CBOW + è´Ÿé‡‡æ · æœ€ç»ˆæŸå¤±: {monitor_cbow.losses[-1]:.4f}")
    print(f"Skip-gram + å±‚æ¬¡Softmax æœ€ç»ˆæŸå¤±: {monitor_hs.losses[-1]:.4f}")


if sentences is not None:
    # æ¨¡å‹æ•ˆæœå¯¹æ¯”è¯„ä¼°
    print("\n" + "="*60)
    print("ğŸ§ª ä¸åŒæ¨¡å‹æ•ˆæœå¯¹æ¯”æµ‹è¯•")

    models = {
        'Skip-gram + è´Ÿé‡‡æ ·': gensim_skipgram,
        'CBOW + è´Ÿé‡‡æ ·': gensim_cbow,
        'Skip-gram + å±‚æ¬¡Softmax': gensim_hs
    }

    # 1. ç›¸ä¼¼è¯æµ‹è¯•
    print("\nğŸ” ç›¸ä¼¼è¯æµ‹è¯•å¯¹æ¯”:")
    test_words = ['è‹¹æœ', 'æˆ‘', 'æ°´æœ']

    for word in test_words:
        if word in gensim_skipgram.wv:              # æ£€æŸ¥è¯æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
            print(f"\nğŸ“ ä¸ '{word}' æœ€ç›¸ä¼¼çš„è¯:")

            for model_name, model in models.items():
                try:
                    similar_words = model.wv.most_similar(word, topn=3)
                    similar_str = ", ".join([f"{w}({s:.3f})" for w, s in similar_words])
                    print(f"  {model_name:<25}: {similar_str}")
                except:
                    print(f"  {model_name:<25}: è®¡ç®—å¤±è´¥")

    # 2. è¯æ±‡ç±»æ¯”æµ‹è¯•
    print("\nğŸ¯ è¯æ±‡ç±»æ¯”ä»»åŠ¡å¯¹æ¯”:")
    analogy_tests = [
        ('è‹¹æœ', 'çº¢è‰²', 'é»„è‰²'),  # è‹¹æœ - çº¢è‰² + é»„è‰² = é¦™è•‰?
        ('æˆ‘', 'å–œæ¬¢', 'åƒ'),  # æˆ‘ - å–œæ¬¢ + åƒ = ?
    ]

    for word_a, word_b, word_c in analogy_tests:
        print(f"\nğŸ” {word_a} - {word_b} + {word_c} = ?")

        for model_name, model in models.items():
            try:
                if all(word in model.wv for word in [word_a, word_b, word_c]):
                    result = model.wv.most_similar(
                        positive=[word_a, word_c],
                        negative=[word_b],
                        topn=2
                    )
                    result_str = ", ".join([f"{w}({s:.3f})" for w, s in result])
                    print(f"  {model_name:<25}: {result_str}")
                else:
                    print(f"  {model_name:<25}: ç¼ºå°‘è¯æ±‡")
            except Exception as e:
                print(f"  {model_name:<25}: è®¡ç®—å¤±è´¥")

    # 3. è¯å¯¹ç›¸ä¼¼åº¦å¯¹æ¯”
    print("\nğŸ“Š è¯å¯¹ç›¸ä¼¼åº¦å¯¹æ¯”:")
    word_pairs = [
        ('è‹¹æœ', 'é¦™è•‰'),
        ('è‹¹æœ', 'æ°´æœ'),
        ('çº¢è‰²', 'é»„è‰²'),
        ('æˆ‘', 'å–œæ¬¢')
    ]

    print(f"{'è¯å¯¹':<15} | {'Skip-gram+è´Ÿé‡‡æ ·':<15} | {'CBOW+è´Ÿé‡‡æ ·':<15} | {'Skip-gram+å±‚æ¬¡':<15}")
    print("-" * 70)

    for word1, word2 in word_pairs:
        row = f"{word1}-{word2}"

        for model_name, model in models.items():
            try:
                if word1 in model.wv and word2 in model.wv:
                    sim = model.wv.similarity(word1, word2)
                    row += f" | {sim:<15.4f}"
                else:
                    row += f" | {'ç¼ºå°‘è¯æ±‡':<15}"
            except:
                row += f" | {'é”™è¯¯':<15}"

        print(row)


# æ¨¡å‹ä¿å­˜ä¸åŠ è½½
if sentences is not None:
    import os

    print("ğŸ’¾ æ¨¡å‹ä¿å­˜ä¸åŠ è½½æ¼”ç¤º")
    print("="*50)

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = "word2vec_models"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # æ–¹æ³•1. ä¿å­˜å®Œæ•´æ¨¡å‹(æ¨èç”¨äºç»§ç»­è®­ç»ƒ)
    print("\nğŸ“ æ–¹æ³•1: ä¿å­˜å®Œæ•´æ¨¡å‹")
    model_path = os.path.join(save_dir, "gensim_skipgram.model")
    gensim_skipgram.save(model_path)
    print(f"âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

    # æ–¹æ³•2. ä»…ä¿å­˜è¯å‘é‡(æ¨èç”¨äºæ¨ç†)
    print("\nğŸ“ æ–¹æ³•2: ä»…ä¿å­˜è¯å‘é‡")
    vectors_path = os.path.join(save_dir, "word_vectors.kv")
    gensim_skipgram.wv.save(vectors_path)
    print(f"âœ… è¯å‘é‡å·²ä¿å­˜åˆ°: {vectors_path}")

    # æ–¹æ³•3. ä¿å­˜ä¸ºWord2Vecæ ¼å¼(å…¼å®¹æ€§å¥½)
    print("\nğŸ“ æ–¹æ³•3: ä¿å­˜ä¸ºWord2Vecæ ¼å¼")
    w2v_path = os.path.join(save_dir, "vectors.txt")
    gensim_skipgram.wv.save_word2vec_format(w2v_path, binary=False)
    print(f"âœ… Word2Vecæ ¼å¼å·²ä¿å­˜åˆ°: {w2v_path}")

    print("\nğŸ”„ æ¨¡å‹åŠ è½½æ¼”ç¤º:")

    # åŠ è½½å®Œæ•´æ¨¡å‹
    print("\nğŸ“‚ åŠ è½½å®Œæ•´æ¨¡å‹:")
    try:
        loaded_model = Word2Vec.load(model_path)
        print(f"âœ… å®Œæ•´æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(loaded_model.wv.key_to_index)}")

        # å¯ä»¥ç»§ç»­è®­ç»ƒ
        print("   ğŸ“š å¯ä»¥ç»§ç»­è®­ç»ƒ...")
        loaded_model.train(sentences, total_examples=len(sentences), epochs=2)
        print("   âœ… é¢å¤–è®­ç»ƒå®Œæˆ")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    # åŠ è½½è¯å‘é‡
    print("\nğŸ“‚ åŠ è½½è¯å‘é‡:")
    try:
        from gensim.models import KeyedVectors
        loaded_vectors = KeyedVectors.load(vectors_path)
        print(f"âœ… è¯å‘é‡åŠ è½½æˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(loaded_vectors.key_to_index)}")

        # æµ‹è¯•åŠŸèƒ½
        if 'è‹¹æœ' in loaded_vectors:
            similar = loaded_vectors.most_similar('è‹¹æœ', topn=2)
            print(f"   ä¸'è‹¹æœ'æœ€ç›¸ä¼¼çš„è¯: {similar}")

    except Exception as e:
        print(f"âŒ è¯å‘é‡åŠ è½½å¤±è´¥: {e}")

    # åŠ è½½Word2Vecæ ¼å¼
    print("\nğŸ“‚ åŠ è½½Word2Vecæ ¼å¼:")
    try:
        loaded_w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=False)
        print(f"âœ… Word2Vecæ ¼å¼åŠ è½½æˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(loaded_w2v.key_to_index)}")

    except Exception as e:
        print(f"âŒ Word2Vecæ ¼å¼åŠ è½½å¤±è´¥: {e}")

    print("\\nğŸ“Š æ–‡ä»¶å¤§å°å¯¹æ¯”:")
    try:
        for filename in os.listdir(save_dir):
            filepath = os.path.join(save_dir, filename)
            if os.path.isfile(filepath):
                size_mb = os.path.getsize(filepath) / (1024 * 1024)
                print(f"  {filename:<20}: {size_mb:.2f}MB")
    except:
        pass

    print("\\nğŸ’¡ ä¿å­˜æ ¼å¼é€‰æ‹©å»ºè®®:")
    print("""
        1. ğŸ”„ ç»§ç»­è®­ç»ƒåœºæ™¯ï¼š
           - ä½¿ç”¨ model.save() ä¿å­˜å®Œæ•´æ¨¡å‹
           - åŒ…å«è®­ç»ƒçŠ¶æ€ï¼Œå¯ç»§ç»­è®­ç»ƒ

        2. ğŸš€ ç”Ÿäº§éƒ¨ç½²åœºæ™¯ï¼š
           - ä½¿ç”¨ model.wv.save() ä»…ä¿å­˜è¯å‘é‡
           - æ–‡ä»¶æ›´å°ï¼ŒåŠ è½½æ›´å¿«

        3. ğŸ”— è·¨å¹³å°å…¼å®¹ï¼š
           - ä½¿ç”¨ save_word2vec_format() 
           - æ ‡å‡†æ ¼å¼ï¼Œå„ç§å·¥å…·éƒ½èƒ½è¯»å–
        """)


# Gensim vs ä»é›¶å®ç° å¯¹æ¯”
if sentences is not None:
    print("âš”ï¸ Gensim vs ä»é›¶å®ç° æ¨¡å‹å¯¹æ¯”")
    print("=" * 60)
    model = Word2VecSkipGram(
        vocab_size=preprocessor.vocab_size,
        embedding_dim=50,
        learning_rate=0.1,
        neg_samples=5
    )

    # å¯¹æ¯”ç›¸ä¼¼è¯ç»“æœ
    print("\nğŸ” ç›¸ä¼¼è¯å¯¹æ¯”æµ‹è¯•:")
    comparison_words = ['è‹¹æœ', 'æˆ‘', 'æ°´æœ']

    for word in comparison_words:
        if word in gensim_skipgram.wv and word in preprocessor.word2idx:
            print(f"\nğŸ“ ä¸ '{word}' æœ€ç›¸ä¼¼çš„è¯:")

            # Gensimæ¨¡å‹ç»“æœ
            try:
                gensim_similar = gensim_skipgram.wv.most_similar(word, topn=3)
                gensim_str = ", ".join([f"{w}({s:.3f})" for w, s in gensim_similar])
                print(f"  Gensim Skip-gram      : {gensim_str}")
            except:
                print(f"  Gensim Skip-gram      : è®¡ç®—å¤±è´¥")

            # æˆ‘ä»¬çš„æ¨¡å‹ç»“æœ
            try:
                word_idx = preprocessor.word2idx[word]
                our_similar = find_most_similar_words(model, preprocessor, word, top_k=3)
                our_str = ", ".join([f"{w}({s:.3f})" for w, s in our_similar])
                print(f"  æˆ‘ä»¬çš„Skip-gramå®ç°   : {our_str}")
            except Exception as e:
                print(f"  æˆ‘ä»¬çš„Skip-gramå®ç°   : è®¡ç®—å¤±è´¥")
                print(e)

    # å¯¹æ¯”è¯å¯¹ç›¸ä¼¼åº¦
    print("\nğŸ“Š è¯å¯¹ç›¸ä¼¼åº¦å¯¹æ¯”:")
    test_pairs = [('è‹¹æœ', 'é¦™è•‰'), ('è‹¹æœ', 'æ°´æœ'), ('æˆ‘', 'å–œæ¬¢')]

    print(f"{'è¯å¯¹':<12} | {'Gensim':<10} | {'æˆ‘ä»¬çš„å®ç°':<10} | {'å·®å¼‚':<10}")
    print("-" * 50)

    for word1, word2 in test_pairs:
        try:
            # Gensimç›¸ä¼¼åº¦
            if word1 in gensim_skipgram.wv and word2 in gensim_skipgram.wv:
                gensim_sim = gensim_skipgram.wv.similarity(word1, word2)
            else:
                gensim_sim = None

            # æˆ‘ä»¬çš„å®ç°ç›¸ä¼¼åº¦
            if word1 in preprocessor.word2idx and word2 in preprocessor.word2idx:
                idx1 = preprocessor.word2idx[word1]
                idx2 = preprocessor.word2idx[word2]
                our_sim = model.similarity(idx1, idx2)
            else:
                our_sim = None

            # è®¡ç®—å·®å¼‚
            if gensim_sim is not None and our_sim is not None:
                diff = abs(gensim_sim - our_sim)
                print(f"{word1} - {word2:<6} | {gensim_sim:<10.4f} | {our_sim:<10.4f} | {diff:<10.4f}")
            else:
                print(f"{word1} - {word2:<6} | {'N/A':<10} | {'N/A':<10} | {'N/A':<10}")

        except Exception as e:
            print(f"{word1}-{word2:<6} | {'é”™è¯¯':<10} | {'é”™è¯¯':<10} | {'é”™è¯¯':<10}")

    print("\\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æè§ä¸‹æ–¹æ€»ç»“")